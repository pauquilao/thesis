{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!usr/bin/env python3\n",
    "# Author: Paulo Quilao\n",
    "# MS Geomatics Engineering Thesis\n",
    "# Module for prepartion of datasets for neural network\n",
    "\n",
    "\n",
    "# Import necessary packages\n",
    "import os\n",
    "import glob\n",
    "import gdal\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def convert_layer(filepath) -> np.ndarray:\n",
    "    \"\"\"Converts image file to an array.\"\"\"\n",
    "    try:\n",
    "        # Read params using gdal and convert band 1 to np array\n",
    "        image = gdal.Open(filepath)\n",
    "        if image is None:\n",
    "            print(\"Cannot locate image.\")\n",
    "            return\n",
    "        # Convert raster to numpy array\n",
    "        layer = image.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(repr(e))\n",
    "\n",
    "    finally:\n",
    "        # Close dataset\n",
    "        image = None\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "def get_path_ds(filepath, extension):\n",
    "    \"\"\"Returns path of tif image.\"\"\"\n",
    "    ds_path = glob.glob(os.path.join(filepath, extension))\n",
    "    # assert len(ds_path) == 9, \"Path should contain 9 files\"\n",
    "    return ds_path\n",
    "\n",
    "\n",
    "def get_relevant_val(array):\n",
    "    \"\"\"Returns an array with nontrivial scalars.\"\"\"\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "    array = array[array > -999]\n",
    "    return array\n",
    "\n",
    "# end of helper functions\n",
    "\n",
    "\n",
    "# Class for preparation of datasets\n",
    "class Dataset:\n",
    "    def __init__(self, prone, notprone):\n",
    "        self.prone = prone\n",
    "        self.notprone = notprone\n",
    "        self.file_ext = \"*.tif\"\n",
    "\n",
    "    def set_file_ext(self, extension):\n",
    "        \"\"\"Alters the default image file extension (i.e. .tif).\"\"\"\n",
    "        self.file_ext = extension\n",
    "\n",
    "    def load_layers(self):\n",
    "        \"\"\"Prepare layers for neural network model.\"\"\"\n",
    "\n",
    "        # Read all landslide drivers\n",
    "        print(\"Loading layers for the creation of input vector...\")\n",
    "        print()\n",
    "\n",
    "        # Path to datasets\n",
    "        ds_prone = get_path_ds(self.prone, self.file_ext)\n",
    "        ds_np = get_path_ds(self.notprone, self.file_ext)\n",
    "\n",
    "        # Prone datasets\n",
    "        X_p = []\n",
    "        for image in ds_prone:\n",
    "            X_p.append(convert_layer(image))\n",
    "\n",
    "        # Not prone datasets\n",
    "        X_np = []\n",
    "        for image in ds_np:\n",
    "            X_np.append(convert_layer(image))\n",
    "\n",
    "        # Check shape of each array if read correctly\n",
    "        # Expected output:(6334, 3877)\n",
    "        # If the list is not empty\n",
    "        if X_p and X_np:\n",
    "            print(\"Shape of converted layers to numpy array:\")\n",
    "            flag = True\n",
    "            for factor1, factor2 in zip(X_p, X_np):\n",
    "                assert factor1.shape == factor2.shape, \"Vectorized images should have the same shape.\"\n",
    "                flag = True\n",
    "        if flag:\n",
    "            print(\"prone factor: {}, not prone factor: {}\".format(X_p[0].shape, X_np[0].shape))\n",
    "            print()\n",
    "\n",
    "        else:\n",
    "            print(\"Error in input images.\")\n",
    "\n",
    "        # Convert arrays to row vector\n",
    "        # Landslide prone samples\n",
    "        X_p_flat = []\n",
    "        for factor in X_p:\n",
    "            X_p_flat.append(factor.flatten().astype(\"float32\"))\n",
    "\n",
    "        # Not prone to landslide samples\n",
    "        X_np_flat = []\n",
    "        for factor in X_np:\n",
    "            X_np_flat.append(factor.flatten().astype(\"float32\"))\n",
    "\n",
    "        # Get flattened shape and dtype\n",
    "        if X_p_flat and X_np_flat:\n",
    "            print(\"Converting arrays to 1D arrays...\")\n",
    "            flag = True\n",
    "            for vector1, vector2 in zip(X_p_flat, X_np_flat):\n",
    "                assert vector1.shape == vector2.shape, \"Row vectors should have the same shape.\"\n",
    "                flag = True\n",
    "\n",
    "        if flag:\n",
    "            print(\"Shape of 1D arrays:\")\n",
    "            print(f\"prone factor: {X_p_flat[0].shape}, dtype: {X_p_flat[0].dtype} | not prone factor: {X_np_flat[0].shape}, dtype: {X_np_flat[0].dtype}\")\n",
    "            print()\n",
    "\n",
    "        # Boolean filtering\n",
    "        # Get \"non-nodata\" values for each factor\n",
    "        print(\"Applying boolean filtering to 1D arrays...\")\n",
    "        for i in range(len(X_p_flat)):\n",
    "            X_p_flat[i] = get_relevant_val(X_p_flat[i])\n",
    "            X_np_flat[i] = get_relevant_val(X_np_flat[i])\n",
    "\n",
    "        # Add columns to those factors with less than columns of relevant dataset\n",
    "        for i in range(len(X_np_flat)):\n",
    "            ref = X_p_flat[0].shape[0]\n",
    "            col = X_np_flat[i].shape[0]\n",
    "            if col != ref:\n",
    "                X_np_flat[i] = np.concatenate((X_np_flat[i], [np.mean(X_np_flat[i])] * (ref - col)))\n",
    "\n",
    "        # Get filtered shape and dtype\n",
    "        if X_p_flat and X_np_flat:\n",
    "            flag = True\n",
    "            for vector1, vector2 in zip(X_p_flat, X_np_flat):\n",
    "                assert vector1.shape == vector2.shape, \"Row vectors should have the same shape.\"\n",
    "                flag = True\n",
    "\n",
    "        if flag:\n",
    "            print(\"Shape of filtered 1D arrays:\")\n",
    "            print(f\"prone factor: {X_p_flat[0].shape}, dtype: {X_p_flat[0].dtype} | not prone factor: {X_np_flat[0].shape}, dtype: {X_np_flat[0].dtype}\")\n",
    "            print()\n",
    "\n",
    "        # Scale values of each row vector\n",
    "        print(\"Scaling all input vectors using min-max scaling...\")\n",
    "        X_p_norm = []\n",
    "        X_np_norm = []\n",
    "        for i in range(len(X_p)):\n",
    "            norm_p = (X_p_flat[i] - min(X_p_flat[i])) / (max(X_p_flat[i]) - min(X_p_flat[i]))\n",
    "            norm_np = (X_np_flat[i] - min(X_np_flat[i])) / (max(X_np_flat[i]) - min(X_np_flat[i]))\n",
    "\n",
    "            X_p_norm.append(norm_p)\n",
    "            X_np_norm.append(norm_np)\n",
    "        print(\"All vectors are scaled in the interval [0, 1].\")\n",
    "        print()\n",
    "\n",
    "        X_p = np.vstack(X_p_norm).T.astype(\"float32\")\n",
    "        X_np = np.vstack(X_np_norm).T.astype(\"float32\")\n",
    "\n",
    "        print(f\"Landslide prone input vector: {X_p.shape} and data type: {X_p.dtype} \\nNot prone to landslide input vector: {X_np.shape} and data type {X_np.dtype}\")\n",
    "        print()\n",
    "\n",
    "        # Split dataset to training set and testing set\n",
    "        print(\"Splitting samples to training, validation, and testing sets...\")\n",
    "        train_split = int(0.8 * X_p.shape[0])  # 80% of total dataset\n",
    "\n",
    "        X_p_split = np.split(X_p, [train_split])\n",
    "        X_p_train = X_p_split[0]  # get the 80% of the dataset\n",
    "        X_p_test = X_p_split[1]  # get the remaining 80% of the dataset\n",
    "\n",
    "        X_np_split = np.split(X_np, [train_split])\n",
    "        X_np_train = X_np_split[0]\n",
    "        X_np_test = X_np_split[1]\n",
    "\n",
    "        X_train = np.vstack([X_p_train, X_np_train])\n",
    "        X_test = np.vstack([X_p_test, X_np_test])\n",
    "\n",
    "        # These target values resulted to continuous lsi (probability between 0.1 to 0.9)\n",
    "        # For one output node with 0.1 and 0.9 target values\n",
    "        # 0.1 -> absence of landslide; 0.9 -> presence of landslide\n",
    "        y_p_train = np.full(X_p_train.shape[0], 0.9, dtype=\"float32\")\n",
    "        y_p_test = np.full(X_p_test.shape[0], 0.9, dtype=\"float32\")\n",
    "\n",
    "        y_np_train = np.full(X_np_train.shape[0], 0.1, dtype=\"float32\")\n",
    "        y_np_test = np.full(X_np_test.shape[0], 0.1, dtype=\"float32\")\n",
    "\n",
    "        y_train = np.concatenate([y_p_train, y_np_train])\n",
    "        y_train = np.vstack(y_train)\n",
    "        y_test = np.concatenate([y_p_test, y_np_test])\n",
    "        y_test = np.vstack(y_test)\n",
    "\n",
    "        # Check if the total number of dataset matched with training and testing sets\n",
    "        assert (X_train.shape[0] + X_test.shape[0]) == (X_p.shape[0] + X_np.shape[0]), \"Number of rows should match the total number of datasets.\"\n",
    "        assert (X_train.shape[1] == X_p.shape[1]) and (X_test.shape[1] == X_p.shape[1]), \"Number of columns should match the total number of datasets.\"\n",
    "\n",
    "        print(\"Finalizing samples...\")\n",
    "        # Create a union of X and y vectors\n",
    "        dataset_train = np.concatenate((X_train, y_train), axis=1)\n",
    "        dataset_test = np.concatenate((X_test, y_test), axis=1)\n",
    "\n",
    "        # Shuffle datasets\n",
    "        np.random.shuffle(dataset_train)\n",
    "        np.random.shuffle(dataset_test)\n",
    "\n",
    "        # For 1 output node\n",
    "        X_train_1o = dataset_train[..., :10]\n",
    "        y_train_1o = dataset_train[..., [-1]]\n",
    "\n",
    "        # Split train into train and validation sets\n",
    "        # 80:20 ratio\n",
    "        train_val_split = int(0.8 * X_train_1o.shape[0])\n",
    "        X_train_split = np.split(X_train_1o, [train_val_split])\n",
    "        y_train_split = np.split(y_train_1o, [train_val_split])\n",
    "\n",
    "        # X vector\n",
    "        X_train_1o = X_train_split[0]\n",
    "        X_val_1o = X_train_split[1]\n",
    "\n",
    "        # y vector\n",
    "        y_train_1o = y_train_split[0]\n",
    "        y_val_1o = y_train_split[1]\n",
    "\n",
    "        # X and y test sets\n",
    "        X_test_1o = dataset_test[..., :10]\n",
    "        y_test_1o = dataset_test[..., [-1]]\n",
    "\n",
    "        if X_train_1o.shape[0] == y_train_1o.shape[0] and X_test_1o.shape[0] == y_test_1o.shape[0] and X_val_1o.shape[0] == y_val_1o.shape[0]:\n",
    "            print(\"Datasets created with shape: train={}, val={}, test={}\".format(X_train_1o.shape, X_val_1o.shape, y_test_1o.shape))\n",
    "            print()\n",
    "\n",
    "        else:\n",
    "            print(\"Error in dataset shapes.\")\n",
    "\n",
    "        # For one output node\n",
    "        # Merge final train, test, and validation datasets (excluding validation set)\n",
    "        # For shuffling in training regimen\n",
    "        train_xy = np.concatenate([X_train_1o, y_train_1o], axis=1)\n",
    "        val_xy = np.concatenate([X_val_1o, y_val_1o], axis=1)\n",
    "        test_xy = np.concatenate([X_test_1o, y_test_1o], axis=1)\n",
    "\n",
    "        return [train_xy, val_xy, test_xy]\n",
    "\n",
    "    def load_fuzzified_layers(self, path):\n",
    "        \"\"\"Prepare arrays for lsi generation\"\"\"\n",
    "        lsi_path = get_path_ds(path, self.file_ext)\n",
    "\n",
    "        # Read all landslide drivers\n",
    "        X_raw = []\n",
    "        for factor in lsi_path:\n",
    "            X_raw.append(convert_layer(factor))\n",
    "\n",
    "        # Convert to 1D array\n",
    "        X_lsi_flat = []\n",
    "        for array in X_raw:\n",
    "            X_lsi_flat.append(array.flatten())\n",
    "\n",
    "        # Create input vector X from all contributing factors\n",
    "        x_lsi = np.vstack(X_lsi_flat).T.astype(\"float32\")\n",
    "        # Finalize datasets\n",
    "        y_lsi = x_lsi[..., [-1]]\n",
    "        x_lsi = x_lsi[..., :-1]\n",
    "\n",
    "        return [x_lsi, y_lsi]\n",
    "\n",
    "# end of Dataset class\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Filepath for landslide prone and nonprone samples\n",
    "    fp_prone = r\"D:\\ms gme\\thesis\\final parameters\\samples\\Final\\landslide\"\n",
    "    fp_notprone = r\"D:\\ms gme\\thesis\\final parameters\\Samples\\Final\\no_landslide\"\n",
    "\n",
    "    my_thesis = Dataset(fp_prone, fp_notprone)\n",
    "    datasets = my_thesis.load_layers()\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass\n",
    "    # main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Author: Paulo Quilao\n",
    "# MS Geomatics Engineering Thesis\n",
    "# Simulation of landslides using a two-layer neural network\n",
    "\n",
    "\n",
    "# Import necessary packages\n",
    "import os\n",
    "import sys\n",
    "import gdal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import time\n",
    "import copy\n",
    "# from dataset import *  # user-defined module\n",
    "\n",
    "\n",
    "# Neural network parent class\n",
    "class Neural_Network(object):\n",
    "    def __init__(self, *args):\n",
    "        ''' Initialization of the perceptron with given sizes.  '''\n",
    "\n",
    "        self.shape = args\n",
    "        n = len(args)\n",
    "\n",
    "        # Build layers\n",
    "        self.layers = []\n",
    "        # Input layer (+1 unit for bias)\n",
    "        self.layers.append(np.ones(self.shape[0]+1))\n",
    "        # Hidden layer(s) + output layer\n",
    "        for i in range(1,n):\n",
    "            self.layers.append(np.ones(self.shape[i]))\n",
    "\n",
    "        # Build weights matrix (randomly between -0.25 and +0.25)\n",
    "        self.weights = []\n",
    "        for i in range(n-1):\n",
    "            self.weights.append(np.zeros((self.layers[i].size,\n",
    "                                         self.layers[i+1].size)))\n",
    "\n",
    "        # dw will hold last change in weights (for momentum)\n",
    "        self.dw = [0,]*len(self.weights)\n",
    "\n",
    "        # Reset weights\n",
    "        self.reset()\n",
    "        \n",
    "        # Date/time information for exporting\n",
    "        dt = datetime.datetime.now()\n",
    "        self.date = dt.strftime(\"%x\")  # local date\n",
    "        self.time = dt.strftime(\"%X\")  # local time\n",
    "\n",
    "    def reset(self):\n",
    "        ''' Reset weights '''\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            Z = np.random.random((self.layers[i].size,self.layers[i+1].size))\n",
    "            self.weights[i][...] = (2*Z-1)*0.25\n",
    "            \n",
    "    # Training methods\n",
    "    # Activation function in output layer\n",
    "    def sigmoid(self, s):\n",
    "        \"\"\"Logistic activation function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "\n",
    "    def sigmoid_prime(self, s):\n",
    "        \"\"\"Derivative of logistic function.\"\"\"\n",
    "        return self.sigmoid(s) * (1 - self.sigmoid(s))\n",
    "\n",
    "    # Activation function in hidden layer\n",
    "    def lrelu(self, x):\n",
    "        \"\"\"Leaky rectified linear unit.\"\"\"\n",
    "        return np.where(x > 0, x, x * 0.01)\n",
    "\n",
    "    def lrelu_prime(self, x):\n",
    "        \"\"\"Derivative of leaky rectified linear unit.\"\"\"\n",
    "        dx = np.ones_like(x)\n",
    "        dx[x < 0] = 0.01\n",
    "        return dx\n",
    "    \n",
    "    def forward(self, data):\n",
    "        ''' Propagate data from input layer to output layer. '''\n",
    "\n",
    "        # Set input layer\n",
    "        self.layers[0][0:-1] = data\n",
    "\n",
    "        # Propagate from layer 0 to layer n-1 using lrelu as activation function\n",
    "        for i in range(1,len(self.shape)):\n",
    "            # Propagate activity\n",
    "            self.layers[i][...] = self.lrelu(np.dot(self.layers[i-1],self.weights[i-1]))\n",
    "\n",
    "        # Return output\n",
    "        return self.layers[-1]\n",
    "\n",
    "    def backward(self, target, lrate=0.01, momentum=0.9):\n",
    "        ''' Back propagate error related to target using lrate. '''\n",
    "\n",
    "        deltas = []\n",
    "\n",
    "        # Compute error on output layer\n",
    "        error = target - self.layers[-1]\n",
    "        delta = error*self.sigmoid_prime(self.layers[-1])\n",
    "        deltas.append(delta)\n",
    "\n",
    "        # Compute error on hidden layers\n",
    "        for i in range(len(self.shape)-2,0,-1):\n",
    "            delta = np.dot(deltas[0],self.weights[i].T)*self.lrelu_prime(self.layers[i])\n",
    "            deltas.insert(0,delta)\n",
    "\n",
    "        # Update weights\n",
    "        for i in range(len(self.weights)):\n",
    "            layer = np.atleast_2d(self.layers[i])\n",
    "            delta = np.atleast_2d(deltas[i])\n",
    "            dw = np.dot(layer.T, delta)\n",
    "            self.weights[i] += lrate*dw + momentum*self.dw[i]\n",
    "            self.dw[i] = dw\n",
    "\n",
    "        # Return error\n",
    "        return np.sqrt(np.mean(error**2))\n",
    "\n",
    "    def compute_error(self, X, y):\n",
    "        \"\"\"Computes RMSE of feeded dataset after a full pass.\"\"\"\n",
    "        feed = self.forward(X)\n",
    "        error = np.sqrt(np.mean((y - feed)**2))\n",
    "        return error\n",
    "\n",
    "    def predict(self, test_x, test_y):\n",
    "        \"\"\"Simulates y based on feeded testing data.\"\"\"\n",
    "        print(\"Predicted data based on trained weights:\")\n",
    "        print(\"Predicted values: {}\\n\".format(self.forward(test_x)))\n",
    "        print(\"Target values: {}\\n\".format(test_y))\n",
    "        rmse = self.compute_error(test_x, test_y)\n",
    "        print(\"Testing loss: {}\\n\".format(rmse))\n",
    "\n",
    "    # end of training methods\n",
    "\n",
    "    # Plot method\n",
    "    def plot_loss_curve(self, xlabel, ylabel, name, save=True):\n",
    "        \"\"\"Creates a loss curve plot.\"\"\"\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.loss_train, label=\"Training Loss\")\n",
    "        plt.plot(self.loss_val, label=\"Validation Loss\")\n",
    "        plt.title(f\"h={self.hidden_size}, lrate={self.lrate}, momentum={self.momentum}\")\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.legend(loc=1)\n",
    "\n",
    "        if save:\n",
    "            # Save plot\n",
    "            folder = \"plots\"\n",
    "            op = os.path.join(os.getcwd(), folder)\n",
    "\n",
    "            if not os.path.exists(op):\n",
    "                os.makedirs(folder)\n",
    "\n",
    "            suffix = f\"{str(self.hidden_size)}_{str(self.lrate)}_{str(self.momentum)}\"\n",
    "\n",
    "            if not os.path.isfile(os.path.join(op, name)):\n",
    "                plt.savefig(os.path.join(op, f\"{name}_{suffix}.jpg\"))\n",
    "            else:\n",
    "                print(\"Name already exists.\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # Write methods\n",
    "    def save_predict(self, test_x, test_y):\n",
    "        \"\"\"Saves a summary of results based on simulated testing data.\"\"\"\n",
    "        folder = \"runs\"\n",
    "        op = os.path.join(os.getcwd(), folder)\n",
    "\n",
    "        if not os.path.exists(op):\n",
    "            os.makedirs(folder)\n",
    "\n",
    "        # Predicted\n",
    "        with open(os.path.join(op, \"predict.txt\"), \"a\") as pred:\n",
    "            pred.write(\"Date and time: {}, {}\\n\".format(self.date, self.time))\n",
    "            pred.write(\"Optimizer: {}\\n\".format(type(self).__name__))\n",
    "            pred.write(\"NN structure: {}\\n\".format(self.layers))\n",
    "            pred.write(\"lrate={}, \".format(self.lrate))\n",
    "            pred.write(\"momentum={}, \".format(self.momentum))\n",
    "            rmse = self.compute_error(test_x, test_y)\n",
    "            pred.write(\"error: {}\\n\\n\".format(rmse))\n",
    "\n",
    "    def save_summary(self, filename, layers: list):\n",
    "        \"\"\"Exports the training summary.\"\"\"\n",
    "        folder = \"runs\"\n",
    "        op = os.path.join(os.getcwd(), folder)\n",
    "\n",
    "        if not os.path.exists(op):\n",
    "            os.makedirs(folder)\n",
    "\n",
    "        # Compute weight of each factor\n",
    "        m_ave = []\n",
    "        m_max = []\n",
    "        weights = self.W1\n",
    "\n",
    "        for w in weights:\n",
    "            m_ave.append(np.mean(w))\n",
    "            m_max.append(np.amax(w))\n",
    "\n",
    "        # Map weights to factors\n",
    "        f = layers\n",
    "        d_ave = dict(zip(f, m_ave[:-1]))    # exclude bias, average weight\n",
    "        d_max = dict(zip(f, m_max[:-1]))    # exclude bias, max weight\n",
    "\n",
    "        # Save weights if error in testing data is reasonable\n",
    "        with open(os.path.join(op, filename), \"a\") as data:\n",
    "            data.write(\"Date and time: {}, {}\\n\".format(self.date, self.time))\n",
    "            data.write(\"NN structure: {}\\n\".format(self.layers[:-1]))\n",
    "            data.write(\"Hyperparameters: \")\n",
    "            data.write(\"lrate={}, \".format(self.lrate))\n",
    "            data.write(\"mu={}, \".format(self.momentum))\n",
    "\n",
    "            try:\n",
    "                # For MGD\n",
    "                data.write(\"batch size={}\".format(self.batch_size))\n",
    "                data.write(\"\\nCycle: {} for 1 epoch (max of {}), \".format(self.total_batch, epoch))\n",
    "                data.write(\"terminated at epoch={}, iteration={}\".format(self.term_epoch, self.term_iter))\n",
    "\n",
    "            except AttributeError:\n",
    "                # For SGD and BGD\n",
    "                data.write(\"\\nEpoch={}, terminated at:{}\".format(self.epoch, self.term_epoch))\n",
    "\n",
    "            data.write(\"\\nSet and actual errors: \")\n",
    "            data.write(\"RMSE: {}, \".format(self.rmse))\n",
    "            data.write(\"val error: {}\\n\".format(np.amin(self.loss_val)))\n",
    "            data.write(\"Input to hidden weights:\\n {}\\n\".format(self.W1))\n",
    "            data.write(\"Hidden to output weights:\\n {}\\n\".format(self.W2))\n",
    "            data.write(\"Weight of each layer (ave):\\n {}\\n\".format(d_ave))\n",
    "            data.write(\"Weight of each layer (max):\\n {}\\n\\n\".format(d_max))\n",
    "\n",
    "        # Check if the file exists\n",
    "        assert os.path.isfile(os.path.join(op, filename)), \"The file was not created.\"\n",
    "        print(\"\\nSummary of training was exported.\")\n",
    "\n",
    "    def export_to_image(self, ref_path, out_path, array):\n",
    "        \"\"\"Exports array to image (default GeoTiff).\"\"\"\n",
    "        print(\"\\nExporting array to image...\")\n",
    "\n",
    "        # Reference data\n",
    "        data0 = gdal.Open(ref_path)\n",
    "\n",
    "        if data0 is None:\n",
    "            print(\"No reference data.\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        # get rows and columns of array\n",
    "        row = array.shape[0]  # number of pixels in y\n",
    "        col = array.shape[1]  # number of pixels in x\n",
    "\n",
    "        driver = gdal.GetDriverByName('GTiff')\n",
    "        dataset = driver.Create(out_path, col, row, 1, gdal.GDT_Float32)\n",
    "\n",
    "        if dataset is None:\n",
    "            print(\"Could not create lsi.tif\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        dataset.GetRasterBand(1).WriteArray(lsi)\n",
    "        dataset.GetRasterBand(1).SetNodataValue(-9999)\n",
    "\n",
    "        # Add GeoTranform and Projection\n",
    "        geotrans = data0.GetGeoTransform()  # get GeoTranform from ref 'data0'\n",
    "        proj = data0.GetProjection()  # get GetProjection from ref 'data0'\n",
    "        dataset.SetGeoTransform(geotrans)\n",
    "        dataset.SetProjection(proj)\n",
    "        dataset.FlushCache()\n",
    "        dataset = None\n",
    "\n",
    "        # Check if exported correctly\n",
    "        if os.path.isfile(out_path):\n",
    "            print(\"\\nLandslide susceptility index was exported.\")\n",
    "\n",
    "    # Getters\n",
    "    def get_structure(self):\n",
    "        \"\"\"Returns the initialized NN structure.\"\"\"\n",
    "        return self.layers\n",
    "\n",
    "    def get_weights(self):\n",
    "        \"\"\"Returns the weights of the model.\"\"\"\n",
    "        return [self.W1, self.W2]\n",
    "\n",
    "    def get_lrate(self):\n",
    "        \"\"\"Returns the set learning rate.\"\"\"\n",
    "        return self.lrate\n",
    "\n",
    "    def get_momentum(self):\n",
    "        \"\"\"Returns the set momentum factor.\"\"\"\n",
    "        return self.momentum\n",
    "\n",
    "    # Setters\n",
    "    def set_weights(self, weight):\n",
    "        \"\"\"Set the weights for final forward pass (excluding bias weights).\"\"\"\n",
    "        self.W1 = weight[0][0:-1]  # exclude the bias for lsi prediction\n",
    "        self.W2 = weight[1]\n",
    "\n",
    "# end of Neural_Network class\n",
    "\n",
    "\n",
    "# Class optimizers\n",
    "class BGD(Neural_Network):\n",
    "    \"\"\"Batch Gradient Descent.\"\"\"\n",
    "\n",
    "    def train_BGD(self, *args):\n",
    "        \"\"\"Performs simulation using batch gradien descent.\"\"\"\n",
    "\n",
    "        print(f\"\\nStarting BGD Training with NN structure:{self.layers}, lrate={self.lrate}, mu={self.momentum}:\")\n",
    "        training = args[0][0]\n",
    "        validation = args[0][1]\n",
    "\n",
    "        self.rmse = args[1]\n",
    "        self.epoch = args[2]\n",
    "\n",
    "        # X and y training sets\n",
    "        X_train_1o = training[..., :10]\n",
    "        y_train_1o = training[..., [-1]]\n",
    "\n",
    "        # X and y validation sets\n",
    "        X_val_1o = validation[..., :10]\n",
    "        y_val_1o = validation[..., [-1]]\n",
    "\n",
    "        self.loss_train = []\n",
    "        self.loss_val = []\n",
    "\n",
    "        for i in range(epoch):\n",
    "            # Shuffle training set every epoch\n",
    "#             np.random.shuffle(training)\n",
    "\n",
    "            # Training loss\n",
    "            rmse_train = self.train(X_train_1o, y_train_1o)\n",
    "\n",
    "            # Validation loss\n",
    "            rmse_val = self.compute_error(X_val_1o, y_val_1o)\n",
    "\n",
    "            # Store errors\n",
    "            self.loss_val.append(rmse_val)\n",
    "            self.loss_train.append(rmse_train)\n",
    "\n",
    "            # Print result per epoch\n",
    "            print(\"epoch: {}\".format(i))\n",
    "            print(\"Training Loss: {}\".format(rmse_train))\n",
    "            print(\"Validation Loss: {}\".format(rmse_val))\n",
    "            print()\n",
    "\n",
    "            # Threshold error\n",
    "            if rmse_val <= self.rmse:\n",
    "                break\n",
    "\n",
    "        # Termination information\n",
    "        self.term_epoch = i\n",
    "\n",
    "        # Training statistics\n",
    "        min_loss_train = np.amin(self.loss_train)\n",
    "        max_loss_train = np.amax(self.loss_train)\n",
    "        mean_loss_train = np.mean(self.loss_train)\n",
    "        min_val_train = np.amin(self.loss_val)\n",
    "        max_val_train = np.amax(self.loss_val)\n",
    "        mean_val_train = np.mean(self.loss_val)\n",
    "\n",
    "        print(\"\\nTraining statistics:\")\n",
    "        print(\"Minimum train error:\", min_loss_train)\n",
    "        print(\"Maximum train error:\", max_loss_train)\n",
    "        print(\"Mean train error:\", mean_loss_train)\n",
    "        print()\n",
    "        print(\"Minimum val error:\", min_val_train)\n",
    "        print(\"Maximum val error:\", max_val_train)\n",
    "        print(\"Mean val error:\", mean_val_train)\n",
    "\n",
    "\n",
    "class SGD(Neural_Network):\n",
    "    \"\"\"Stochastic Gradient Descent.\"\"\"\n",
    "\n",
    "    def train_SGD(self, *args):\n",
    "        \"\"\"Performs simulation using stochastic gradien descent.\"\"\"\n",
    "        print(f\"\\nStarting SGD Training with NN structure:{self.layers}, lrate={self.lrate}, mu={self.momentum}:\")\n",
    "        training = args[0][0]\n",
    "        validation = args[0][1]\n",
    "\n",
    "        self.rmse = args[1]\n",
    "        self.epoch = args[2]\n",
    "\n",
    "        # X and y training sets\n",
    "        X_train_1o = training[..., :10]\n",
    "        y_train_1o = training[..., [-1]]\n",
    "\n",
    "        # X and y validation sets\n",
    "        X_val_1o = validation[..., :10]\n",
    "        y_val_1o = validation[..., [-1]]\n",
    "\n",
    "        self.loss_train = []\n",
    "        self.loss_val = []\n",
    "\n",
    "        for i in range(epoch):\n",
    "            # Stochastic sampling\n",
    "            n = np.random.choice(X_train_1o.shape[0])\n",
    "\n",
    "            # Training loss\n",
    "            rmse_train = self.train(X_train_1o[n], y_train_1o[n])\n",
    "\n",
    "            # Validation loss\n",
    "            rmse_val = self.compute_error(X_val_1o, y_val_1o)\n",
    "\n",
    "            # Store errors\n",
    "            self.loss_val.append(rmse_val)\n",
    "            self.loss_train.append(rmse_train)\n",
    "\n",
    "            # Print result per epoch\n",
    "            print(\"epoch: {}\".format(i))\n",
    "            print(\"Training Loss: {}\".format(rmse_train))\n",
    "            print(\"Validation Loss: {}\".format(rmse_val))\n",
    "            print()\n",
    "\n",
    "            # Threshold error\n",
    "            if rmse_val <= self.rmse:\n",
    "                break\n",
    "\n",
    "        # Termination information\n",
    "        self.term_epoch = i\n",
    "\n",
    "        # Training statistics\n",
    "        min_loss_train = np.amin(self.loss_train)\n",
    "        max_loss_train = np.amax(self.loss_train)\n",
    "        mean_loss_train = np.mean(self.loss_train)\n",
    "        min_val_train = np.amin(self.loss_val)\n",
    "        max_val_train = np.amax(self.loss_val)\n",
    "        mean_val_train = np.mean(self.loss_val)\n",
    "\n",
    "        print(\"\\nTraining statistics:\")\n",
    "        print(\"Minimum train error:\", min_loss_train)\n",
    "        print(\"Maximum train error:\", max_loss_train)\n",
    "        print(\"Mean train error:\", mean_loss_train)\n",
    "        print()\n",
    "        print(\"Minimum val error:\", min_val_train)\n",
    "        print(\"Maximum val error:\", max_val_train)\n",
    "        print(\"Mean val error:\", mean_val_train)\n",
    "\n",
    "\n",
    "class MGD(Neural_Network):\n",
    "    \"\"\"Mini-batch Gradient Descent.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        Neural_Network.__init__(self, *args)\n",
    "        self.batch_size = kwargs[\"bs\"]\n",
    "\n",
    "    def train_MGD(self, *args):\n",
    "        \"\"\"Performs simulation using mini-batch gradien descent.\"\"\"\n",
    "#         print(f\"\\nStarting MGD Training with NN structure:{self.layers}, lrate={self.lrate}, mu={self.momentum}:\")\n",
    "\n",
    "        training = args[0][0]\n",
    "        validation = args[0][1]\n",
    "\n",
    "        self.rmse = args[1]\n",
    "        self.epoch = args[2]\n",
    "\n",
    "        # X and y training sets\n",
    "        X_train_1o = training[..., :9]\n",
    "        y_train_1o = training[..., [-1]]\n",
    "\n",
    "        # X and y validation sets\n",
    "        X_val_1o = validation[..., :9]\n",
    "        y_val_1o = validation[..., [-1]]\n",
    "\n",
    "        # Initialize number of iterations\n",
    "        self.total_batch = X_train_1o.shape[0] // (self.batch_size - 1)  # exlude the last index\n",
    "        val_batch = X_val_1o.shape[0] // (self.batch_size - 1)\n",
    "\n",
    "        self.loss_train = []\n",
    "        self.loss_val = []\n",
    "        for i in range(epoch):\n",
    "\n",
    "            # Shuffle datasets each epoch (after completely seeing the entire dataset)\n",
    "            np.random.shuffle(training)\n",
    "\n",
    "            # Reassign X and y training vectors\n",
    "            X_train_1o = training[..., :9]\n",
    "            y_train_1o = training[..., [-1]]\n",
    "\n",
    "            X_batches = np.array_split(X_train_1o, self.total_batch)\n",
    "            y_batches = np.array_split(y_train_1o, self.total_batch)\n",
    "\n",
    "            # Initialize rmse\n",
    "            # Will be reset after completing 1 epoch\n",
    "            ave_cost_train = 0\n",
    "\n",
    "            # Train the neural network per mini-batch\n",
    "            for j in range(self.total_batch):\n",
    "                minibatch_x = X_batches[j]\n",
    "                minibatch_y = y_batches[j]\n",
    "\n",
    "                # Train per mini-batch\n",
    "                self.forward(minibatch_x)\n",
    "                rmse_train = self.backward(minibatch_y)\n",
    "\n",
    "                # Get average training cost for each epoch\n",
    "                ave_cost_train += rmse_train / self.total_batch\n",
    "\n",
    "            self.loss_train.append(ave_cost_train)\n",
    "\n",
    "#             # Validation using the whole validation set\n",
    "#             rmse_val = self.compute_error(X_val_1o, y_val_1o)\n",
    "#             self.loss_val.append(rmse_val)\n",
    "\n",
    "#             # Print result per epoch\n",
    "#             print(\"epoch: {}\".format(i))\n",
    "#             print(\"Training loss: {}\".format(ave_cost_train))\n",
    "#             print(\"Validation loss: {}\".format(rmse_val))\n",
    "#             print()\n",
    "\n",
    "#             if rmse_val <= RMSE:\n",
    "#                 break\n",
    "\n",
    "            # Per mini-batch validation\n",
    "            ave_cost_val = 0\n",
    "        \n",
    "            x_val = np.array_split(X_val_1o, val_batch)\n",
    "            y_val = np.array_split(y_val_1o, val_batch)\n",
    "            \n",
    "            for v in range(val_batch):\n",
    "                mb_val_x = x_val[v]\n",
    "                mb_val_y = y_val[v]\n",
    "\n",
    "                # Validation loss\n",
    "                rmse_val = self.compute_error(mb_val_x, mb_val_y)\n",
    "\n",
    "                # Get average val cost for each epoch\n",
    "                ave_cost_val += rmse_val / val_batch\n",
    "\n",
    "            self.loss_val.append(ave_cost_val)\n",
    "\n",
    "            # Print result per epoch\n",
    "            print(\"epoch: {}\".format(i))\n",
    "            print(\"Training loss: {}\".format(ave_cost_train))\n",
    "            print(\"Validation loss: {}\".format(ave_cost_val))\n",
    "            print()\n",
    "\n",
    "            # Stoping criterion using validation set\n",
    "            if ave_cost_val <= self.rmse:\n",
    "                print(\"Training done.\")\n",
    "                break\n",
    "\n",
    "        # Termination information\n",
    "        self.term_epoch = i\n",
    "        self.term_iter = j\n",
    "\n",
    "        # Training statistics\n",
    "        min_loss_train = np.amin(self.loss_train)\n",
    "        max_loss_train = np.amax(self.loss_train)\n",
    "        mean_loss_train = np.mean(self.loss_train)\n",
    "        min_val_train = np.amin(self.loss_val)\n",
    "        max_val_train = np.amax(self.loss_val)\n",
    "        mean_val_train = np.mean(self.loss_val)\n",
    "\n",
    "        print(\"\\nTraining statistics:\")\n",
    "        print(\"Minimum train error:\", min_loss_train)\n",
    "        print(\"Maximum train error:\", max_loss_train)\n",
    "        print(\"Mean train error:\", mean_loss_train)\n",
    "        print()\n",
    "        print(\"Minimum val error:\", min_val_train)\n",
    "        print(\"Maximum val error:\", max_val_train)\n",
    "        print(\"Mean val error:\", mean_val_train)\n",
    "\n",
    "# end of class optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading layers for the creation of input vector...\n",
      "\n",
      "Shape of converted layers to numpy array:\n",
      "prone factor: (6334, 3877), not prone factor: (6334, 3877)\n",
      "\n",
      "Converting arrays to 1D arrays...\n",
      "Shape of 1D arrays:\n",
      "prone factor: (24556918,), dtype: float32 | not prone factor: (24556918,), dtype: float32\n",
      "\n",
      "Applying boolean filtering to 1D arrays...\n",
      "Shape of filtered 1D arrays:\n",
      "prone factor: (18862,), dtype: float32 | not prone factor: (18862,), dtype: float32\n",
      "\n",
      "Scaling all input vectors using min-max scaling...\n",
      "All vectors are scaled in the interval [0, 1].\n",
      "\n",
      "Landslide prone input vector: (18862, 9) and data type: float32 \n",
      "Not prone to landslide input vector: (18862, 9) and data type float32\n",
      "\n",
      "Splitting samples to training, validation, and testing sets...\n",
      "Finalizing samples...\n",
      "Datasets created with shape: train=(24142, 10), val=(6036, 10), test=(7546, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Start time\n",
    "    start_time = time.process_time()\n",
    "\n",
    "    # Filepath for landslide prone and nonprone samples\n",
    "    fp_prone = r\"D:\\ms gme\\thesis\\final parameters\\samples\\Final\\landslide\\fixed_nd\"\n",
    "    fp_notprone = r\"D:\\MS Gme\\Thesis\\Final Parameters\\Samples\\Final\\no_ls_5deg_slope\\fixed_nd\"\n",
    "\n",
    "    my_thesis = Dataset(fp_prone, fp_notprone)\n",
    "\n",
    "    # Load landslide factors\n",
    "    datasets = my_thesis.load_layers()\n",
    "    train_val_sets = datasets[0:2]\n",
    "    test_set = datasets[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Training loss: 0.1072677957727713\n",
      "Validation loss: 0.05613573408365431\n",
      "\n",
      "epoch: 1\n",
      "Training loss: 0.07257898523610608\n",
      "Validation loss: 0.04746135105933431\n",
      "\n",
      "epoch: 2\n",
      "Training loss: 0.06856133267821064\n",
      "Validation loss: 0.05667740553630302\n",
      "\n",
      "epoch: 3\n",
      "Training loss: 0.06621271523406787\n",
      "Validation loss: 0.0925043114166798\n",
      "\n",
      "epoch: 4\n",
      "Training loss: 0.06461782097119437\n",
      "Validation loss: 0.08383637248086047\n",
      "\n",
      "epoch: 5\n",
      "Training loss: 0.06467282046547838\n",
      "Validation loss: 0.06407582343294611\n",
      "\n",
      "epoch: 6\n",
      "Training loss: 0.0631208803546886\n",
      "Validation loss: 0.04622215569689785\n",
      "\n",
      "epoch: 7\n",
      "Training loss: 0.06225106733287074\n",
      "Validation loss: 0.045468055814292606\n",
      "\n",
      "epoch: 8\n",
      "Training loss: 0.05951900441199851\n",
      "Validation loss: 0.15674076367298626\n",
      "\n",
      "epoch: 9\n",
      "Training loss: 0.05828708377015309\n",
      "Validation loss: 0.04412351239328937\n",
      "\n",
      "epoch: 10\n",
      "Training loss: 0.05812221200667808\n",
      "Validation loss: 0.04331011532902586\n",
      "\n",
      "epoch: 11\n",
      "Training loss: 0.05684239232609368\n",
      "Validation loss: 0.0393398589389531\n",
      "\n",
      "epoch: 12\n",
      "Training loss: 0.05329244507795715\n",
      "Validation loss: 0.0499825435856655\n",
      "\n",
      "epoch: 13\n",
      "Training loss: 0.05059159946461089\n",
      "Validation loss: 0.030959342765428173\n",
      "\n",
      "epoch: 14\n",
      "Training loss: 0.04839728993172502\n",
      "Validation loss: 0.04706196262643778\n",
      "\n",
      "epoch: 15\n",
      "Training loss: 0.04638213189524794\n",
      "Validation loss: 0.031092224255548008\n",
      "\n",
      "epoch: 16\n",
      "Training loss: 0.04518006754161517\n",
      "Validation loss: 0.0381687295484892\n",
      "\n",
      "epoch: 17\n",
      "Training loss: 0.04419042916247144\n",
      "Validation loss: 0.0783727284647449\n",
      "\n",
      "epoch: 18\n",
      "Training loss: 0.045739338931707854\n",
      "Validation loss: 0.04467928879697854\n",
      "\n",
      "epoch: 19\n",
      "Training loss: 0.0432810801415622\n",
      "Validation loss: 0.03499074816955465\n",
      "\n",
      "epoch: 20\n",
      "Training loss: 0.041794180736501804\n",
      "Validation loss: 0.034942170728316777\n",
      "\n",
      "epoch: 21\n",
      "Training loss: 0.042269810591322426\n",
      "Validation loss: 0.035493889316134355\n",
      "\n",
      "epoch: 22\n",
      "Training loss: 0.04167048085616618\n",
      "Validation loss: 0.06955306757519965\n",
      "\n",
      "epoch: 23\n",
      "Training loss: 0.04171124315648155\n",
      "Validation loss: 0.11422731326155203\n",
      "\n",
      "epoch: 24\n",
      "Training loss: 0.04376238593810313\n",
      "Validation loss: 0.04319160348700077\n",
      "\n",
      "epoch: 25\n",
      "Training loss: 0.04908485551228826\n",
      "Validation loss: 0.07021103395970915\n",
      "\n",
      "epoch: 26\n",
      "Training loss: 0.045537000448830814\n",
      "Validation loss: 0.036181458702909\n",
      "\n",
      "epoch: 27\n",
      "Training loss: 0.04274764039460448\n",
      "Validation loss: 0.0321950128860465\n",
      "\n",
      "epoch: 28\n",
      "Training loss: 0.04212216944597137\n",
      "Validation loss: 0.05484173594260137\n",
      "\n",
      "epoch: 29\n",
      "Training loss: 0.04167954142626923\n",
      "Validation loss: 0.03734857163831726\n",
      "\n",
      "epoch: 30\n",
      "Training loss: 0.04195884735203782\n",
      "Validation loss: 0.03884123060219909\n",
      "\n",
      "epoch: 31\n",
      "Training loss: 0.042031880558915026\n",
      "Validation loss: 0.06991560082317207\n",
      "\n",
      "epoch: 32\n",
      "Training loss: 0.04040137906471076\n",
      "Validation loss: 0.03501413995359891\n",
      "\n",
      "epoch: 33\n",
      "Training loss: 0.039716827289985684\n",
      "Validation loss: 0.027335067527502067\n",
      "\n",
      "epoch: 34\n",
      "Training loss: 0.040321499638820875\n",
      "Validation loss: 0.03075833681726846\n",
      "\n",
      "epoch: 35\n",
      "Training loss: 0.03974826487068722\n",
      "Validation loss: 0.028995702269835298\n",
      "\n",
      "epoch: 36\n",
      "Training loss: 0.03922571999168918\n",
      "Validation loss: 0.030769094262026644\n",
      "\n",
      "epoch: 37\n",
      "Training loss: 0.03939740083585592\n",
      "Validation loss: 0.040589498340749576\n",
      "\n",
      "epoch: 38\n",
      "Training loss: 0.03943065454364281\n",
      "Validation loss: 0.034551617685533706\n",
      "\n",
      "epoch: 39\n",
      "Training loss: 0.03865865111110355\n",
      "Validation loss: 0.03745236567804591\n",
      "\n",
      "epoch: 40\n",
      "Training loss: 0.03789503941257685\n",
      "Validation loss: 0.03345854684489083\n",
      "\n",
      "epoch: 41\n",
      "Training loss: 0.03849744794288931\n",
      "Validation loss: 0.025069916733639074\n",
      "\n",
      "epoch: 42\n",
      "Training loss: 0.03798630983490847\n",
      "Validation loss: 0.024679849452798625\n",
      "\n",
      "epoch: 43\n",
      "Training loss: 0.03762930520548601\n",
      "Validation loss: 0.03042045588290689\n",
      "\n",
      "epoch: 44\n",
      "Training loss: 0.03841104687562458\n",
      "Validation loss: 0.03372342078599052\n",
      "\n",
      "epoch: 45\n",
      "Training loss: 0.03840207396391937\n",
      "Validation loss: 0.0424942120481971\n",
      "\n",
      "epoch: 46\n",
      "Training loss: 0.03704990926661554\n",
      "Validation loss: 0.023347538283904564\n",
      "\n",
      "epoch: 47\n",
      "Training loss: 0.038713006105398906\n",
      "Validation loss: 0.028580188645167207\n",
      "\n",
      "epoch: 48\n",
      "Training loss: 0.0368734566886493\n",
      "Validation loss: 0.030435926077190555\n",
      "\n",
      "epoch: 49\n",
      "Training loss: 0.03753457472714802\n",
      "Validation loss: 0.06789195688205775\n",
      "\n",
      "epoch: 50\n",
      "Training loss: 0.036304871645060154\n",
      "Validation loss: 0.025100738308074366\n",
      "\n",
      "epoch: 51\n",
      "Training loss: 0.0366421696368761\n",
      "Validation loss: 0.025394945714974006\n",
      "\n",
      "epoch: 52\n",
      "Training loss: 0.03666753513475785\n",
      "Validation loss: 0.029587621592468336\n",
      "\n",
      "epoch: 53\n",
      "Training loss: 0.03660764783848874\n",
      "Validation loss: 0.04996631993167881\n",
      "\n",
      "epoch: 54\n",
      "Training loss: 0.036371015005386316\n",
      "Validation loss: 0.03871270799474063\n",
      "\n",
      "epoch: 55\n",
      "Training loss: 0.03686503715571197\n",
      "Validation loss: 0.06178134933735821\n",
      "\n",
      "epoch: 56\n",
      "Training loss: 0.03625152556742662\n",
      "Validation loss: 0.03603454861194704\n",
      "\n",
      "epoch: 57\n",
      "Training loss: 0.03599230067866954\n",
      "Validation loss: 0.041136429772007474\n",
      "\n",
      "epoch: 58\n",
      "Training loss: 0.036011691759074614\n",
      "Validation loss: 0.03138128498411102\n",
      "\n",
      "epoch: 59\n",
      "Training loss: 0.03611474463095771\n",
      "Validation loss: 0.029234183828144668\n",
      "\n",
      "epoch: 60\n",
      "Training loss: 0.04000822884833241\n",
      "Validation loss: 0.03109232051271645\n",
      "\n",
      "epoch: 61\n",
      "Training loss: 0.03799172216714981\n",
      "Validation loss: 0.028460375965736966\n",
      "\n",
      "epoch: 62\n",
      "Training loss: 0.036080838950054546\n",
      "Validation loss: 0.03489610022971399\n",
      "\n",
      "epoch: 63\n",
      "Training loss: 0.03600568816013452\n",
      "Validation loss: 0.03325193978809616\n",
      "\n",
      "epoch: 64\n",
      "Training loss: 0.036041992079017544\n",
      "Validation loss: 0.0332915509047689\n",
      "\n",
      "epoch: 65\n",
      "Training loss: 0.03551095045261963\n",
      "Validation loss: 0.028155500946356125\n",
      "\n",
      "epoch: 66\n",
      "Training loss: 0.03501633336452719\n",
      "Validation loss: 0.033666521720473803\n",
      "\n",
      "epoch: 67\n",
      "Training loss: 0.03483459299774551\n",
      "Validation loss: 0.034430244988761294\n",
      "\n",
      "epoch: 68\n",
      "Training loss: 0.03514349710221326\n",
      "Validation loss: 0.04059492116799709\n",
      "\n",
      "epoch: 69\n",
      "Training loss: 0.03493891560563995\n",
      "Validation loss: 0.029382299444069556\n",
      "\n",
      "epoch: 70\n",
      "Training loss: 0.034227042736139086\n",
      "Validation loss: 0.048790801349791364\n",
      "\n",
      "epoch: 71\n",
      "Training loss: 0.034187389464698015\n",
      "Validation loss: 0.03517589307296919\n",
      "\n",
      "epoch: 72\n",
      "Training loss: 0.03434204403431282\n",
      "Validation loss: 0.026857720023635057\n",
      "\n",
      "epoch: 73\n",
      "Training loss: 0.03405197561002041\n",
      "Validation loss: 0.030380438340079145\n",
      "\n",
      "epoch: 74\n",
      "Training loss: 0.03371370085843756\n",
      "Validation loss: 0.027149752258600165\n",
      "\n",
      "epoch: 75\n",
      "Training loss: 0.034039805047800666\n",
      "Validation loss: 0.02716878872673256\n",
      "\n",
      "epoch: 76\n",
      "Training loss: 0.033839305152236446\n",
      "Validation loss: 0.031206361279384885\n",
      "\n",
      "epoch: 77\n",
      "Training loss: 0.033440398753263274\n",
      "Validation loss: 0.028445137901875146\n",
      "\n",
      "epoch: 78\n",
      "Training loss: 0.033504142355876475\n",
      "Validation loss: 0.04134394500843289\n",
      "\n",
      "epoch: 79\n",
      "Training loss: 0.03404692036705319\n",
      "Validation loss: 0.02959218778099628\n",
      "\n",
      "epoch: 80\n",
      "Training loss: 0.03342216416652887\n",
      "Validation loss: 0.03097976998131484\n",
      "\n",
      "epoch: 81\n",
      "Training loss: 0.03347836258069099\n",
      "Validation loss: 0.02969682349117282\n",
      "\n",
      "epoch: 82\n",
      "Training loss: 0.03364171019667286\n",
      "Validation loss: 0.03011252479603155\n",
      "\n",
      "epoch: 83\n",
      "Training loss: 0.033345843986136305\n",
      "Validation loss: 0.05128454436220488\n",
      "\n",
      "epoch: 84\n",
      "Training loss: 0.03336974067731518\n",
      "Validation loss: 0.029559674165438896\n",
      "\n",
      "epoch: 85\n",
      "Training loss: 0.03332272987087428\n",
      "Validation loss: 0.027000337158811182\n",
      "\n",
      "epoch: 86\n",
      "Training loss: 0.03372045742264411\n",
      "Validation loss: 0.02915747039566499\n",
      "\n",
      "epoch: 87\n",
      "Training loss: 0.03339955425519285\n",
      "Validation loss: 0.03775034017836438\n",
      "\n",
      "epoch: 88\n",
      "Training loss: 0.03352905046112735\n",
      "Validation loss: 0.02792823472889709\n",
      "\n",
      "epoch: 89\n",
      "Training loss: 0.033129154083554185\n",
      "Validation loss: 0.03154638138661036\n",
      "\n",
      "epoch: 90\n",
      "Training loss: 0.03352196505793845\n",
      "Validation loss: 0.031017840865574943\n",
      "\n",
      "epoch: 91\n",
      "Training loss: 0.0336056896966788\n",
      "Validation loss: 0.02989200693569595\n",
      "\n",
      "epoch: 92\n",
      "Training loss: 0.03378455714446936\n",
      "Validation loss: 0.029565938246363586\n",
      "\n",
      "epoch: 93\n",
      "Training loss: 0.03369234254609807\n",
      "Validation loss: 0.028164025221691442\n",
      "\n",
      "epoch: 94\n",
      "Training loss: 0.03314703321611598\n",
      "Validation loss: 0.12491254539995046\n",
      "\n",
      "epoch: 95\n",
      "Training loss: 0.033145224044449695\n",
      "Validation loss: 0.030454600185970803\n",
      "\n",
      "epoch: 96\n",
      "Training loss: 0.033675452989111364\n",
      "Validation loss: 0.0289661874293442\n",
      "\n",
      "epoch: 97\n",
      "Training loss: 0.03304122099637465\n",
      "Validation loss: 0.040275221204158114\n",
      "\n",
      "epoch: 98\n",
      "Training loss: 0.032876831203291206\n",
      "Validation loss: 0.028704189236327275\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 99\n",
      "Training loss: 0.03340521558291334\n",
      "Validation loss: 0.03188666694272669\n",
      "\n",
      "epoch: 100\n",
      "Training loss: 0.03326512729774236\n",
      "Validation loss: 0.03198005175417944\n",
      "\n",
      "epoch: 101\n",
      "Training loss: 0.03304753380308821\n",
      "Validation loss: 0.029370582993031455\n",
      "\n",
      "epoch: 102\n",
      "Training loss: 0.03333004142229438\n",
      "Validation loss: 0.036488378795959085\n",
      "\n",
      "epoch: 103\n",
      "Training loss: 0.03329681583248204\n",
      "Validation loss: 0.029136197510397793\n",
      "\n",
      "epoch: 104\n",
      "Training loss: 0.0334218159228924\n",
      "Validation loss: 0.039550350748277244\n",
      "\n",
      "epoch: 105\n",
      "Training loss: 0.033266098699600434\n",
      "Validation loss: 0.029052376287487486\n",
      "\n",
      "epoch: 106\n",
      "Training loss: 0.03306139443220867\n",
      "Validation loss: 0.027009661130654186\n",
      "\n",
      "epoch: 107\n",
      "Training loss: 0.03304329201570473\n",
      "Validation loss: 0.032919583258188174\n",
      "\n",
      "epoch: 108\n",
      "Training loss: 0.03283810718231069\n",
      "Validation loss: 0.028936527139783244\n",
      "\n",
      "epoch: 109\n",
      "Training loss: 0.03268211018735009\n",
      "Validation loss: 0.028002154264016393\n",
      "\n",
      "epoch: 110\n",
      "Training loss: 0.03176110058980071\n",
      "Validation loss: 0.042287647426388804\n",
      "\n",
      "epoch: 111\n",
      "Training loss: 0.031586913949994855\n",
      "Validation loss: 0.028890418010196095\n",
      "\n",
      "epoch: 112\n",
      "Training loss: 0.0312844762843475\n",
      "Validation loss: 0.02560050342303887\n",
      "\n",
      "epoch: 113\n",
      "Training loss: 0.031056367898229433\n",
      "Validation loss: 0.02839122499214103\n",
      "\n",
      "epoch: 114\n",
      "Training loss: 0.03128523739462597\n",
      "Validation loss: 0.027153981129957996\n",
      "\n",
      "epoch: 115\n",
      "Training loss: 0.031463901110438924\n",
      "Validation loss: 0.027633264738082558\n",
      "\n",
      "epoch: 116\n",
      "Training loss: 0.031006538046590173\n",
      "Validation loss: 0.03031834906108317\n",
      "\n",
      "epoch: 117\n",
      "Training loss: 0.031230213854447957\n",
      "Validation loss: 0.02444798711597955\n",
      "\n",
      "epoch: 118\n",
      "Training loss: 0.031263638099713255\n",
      "Validation loss: 0.025872214867224384\n",
      "\n",
      "epoch: 119\n",
      "Training loss: 0.03122422462477205\n",
      "Validation loss: 0.03588658190826677\n",
      "\n",
      "epoch: 120\n",
      "Training loss: 0.03152770252013577\n",
      "Validation loss: 0.027608305285102393\n",
      "\n",
      "epoch: 121\n",
      "Training loss: 0.03158480177551095\n",
      "Validation loss: 0.09559443366748353\n",
      "\n",
      "epoch: 122\n",
      "Training loss: 0.030660948442769863\n",
      "Validation loss: 0.025667529292559332\n",
      "\n",
      "epoch: 123\n",
      "Training loss: 0.030840930692185152\n",
      "Validation loss: 0.023095937832508656\n",
      "\n",
      "epoch: 124\n",
      "Training loss: 0.030925529298029348\n",
      "Validation loss: 0.02590705928127742\n",
      "\n",
      "epoch: 125\n",
      "Training loss: 0.0316779539434152\n",
      "Validation loss: 0.03491494352672786\n",
      "\n",
      "epoch: 126\n",
      "Training loss: 0.03304698178707067\n",
      "Validation loss: 0.03148233499835593\n",
      "\n",
      "epoch: 127\n",
      "Training loss: 0.032598898641659414\n",
      "Validation loss: 0.037545694003438976\n",
      "\n",
      "epoch: 128\n",
      "Training loss: 0.03276327952341528\n",
      "Validation loss: 0.026403448258849507\n",
      "\n",
      "epoch: 129\n",
      "Training loss: 0.033093942390824166\n",
      "Validation loss: 0.045401804271634774\n",
      "\n",
      "epoch: 130\n",
      "Training loss: 0.03295015668766593\n",
      "Validation loss: 0.027533793438384316\n",
      "\n",
      "epoch: 131\n",
      "Training loss: 0.033251063090434727\n",
      "Validation loss: 0.02775823769249863\n",
      "\n",
      "epoch: 132\n",
      "Training loss: 0.0332178063166439\n",
      "Validation loss: 0.022882372279155447\n",
      "\n",
      "epoch: 133\n",
      "Training loss: 0.03300465960748144\n",
      "Validation loss: 0.10392571163914295\n",
      "\n",
      "epoch: 134\n",
      "Training loss: 0.03371033758393737\n",
      "Validation loss: 0.03233602558774651\n",
      "\n",
      "epoch: 135\n",
      "Training loss: 0.03344817933575505\n",
      "Validation loss: 0.03195698793496284\n",
      "\n",
      "epoch: 136\n",
      "Training loss: 0.033285047299115994\n",
      "Validation loss: 0.030904508465696368\n",
      "\n",
      "epoch: 137\n",
      "Training loss: 0.0370879855159024\n",
      "Validation loss: 0.030540340937665122\n",
      "\n",
      "epoch: 138\n",
      "Training loss: 0.035880836595011685\n",
      "Validation loss: 0.059416548663735004\n",
      "\n",
      "epoch: 139\n",
      "Training loss: 0.03582432956613461\n",
      "Validation loss: 0.03079975760769368\n",
      "\n",
      "epoch: 140\n",
      "Training loss: 0.03490628689347134\n",
      "Validation loss: 0.029267207085579387\n",
      "\n",
      "epoch: 141\n",
      "Training loss: 0.03510147146199442\n",
      "Validation loss: 0.03441224851426359\n",
      "\n",
      "epoch: 142\n",
      "Training loss: 0.03465104566442276\n",
      "Validation loss: 0.03818652809051557\n",
      "\n",
      "epoch: 143\n",
      "Training loss: 0.0343945751181008\n",
      "Validation loss: 0.037162256715367464\n",
      "\n",
      "epoch: 144\n",
      "Training loss: 0.03429575632133738\n",
      "Validation loss: 0.028592487626447567\n",
      "\n",
      "epoch: 145\n",
      "Training loss: 0.034436046782548226\n",
      "Validation loss: 0.028441169511832797\n",
      "\n",
      "epoch: 146\n",
      "Training loss: 0.034411941753728074\n",
      "Validation loss: 0.032446441929358215\n",
      "\n",
      "epoch: 147\n",
      "Training loss: 0.033774697147878216\n",
      "Validation loss: 0.03135107900927907\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-ef0a140495dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mmgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_MGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_val_sets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRMSE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;31m# end of training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-a18ac3143a7b>\u001b[0m in \u001b[0;36mtrain_MGD\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m                 \u001b[1;31m# Train per mini-batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m                 \u001b[0mrmse_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-a18ac3143a7b>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[1;31m# Propagate activity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;31m# Return output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # MGD training\n",
    "    # Initiate seed for easier debugging\n",
    "    seed = 99\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Instantiate NN class\n",
    "    in_size = 9  # input layer\n",
    "    h_size = 25  # hidden layer\n",
    "    out_size = 1  # output layer\n",
    "\n",
    "    # Hyperparameters\n",
    "    lrate = 0.01\n",
    "    momentum = 0.9\n",
    "    batch_size = 2\n",
    "    \n",
    "    mgd = MGD(in_size, h_size, out_size, bs=batch_size, lr=lrate, mu=momentum)\n",
    "\n",
    "    # Initialize Root Mean Square Error\n",
    "    RMSE = 0.01\n",
    "    # Initialize number of epochs\n",
    "    epoch = 3000\n",
    "\n",
    "    mgd.train_MGD(train_val_sets, RMSE, epoch)\n",
    "    # end of training\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # BGD training\n",
    "    # Initiate seed for easier debugging\n",
    "    seed = 99\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Instantiate NN class\n",
    "    in_size = 9  # input layer\n",
    "    h_size = 25  # hidden layer\n",
    "    out_size = 1  # output layer\n",
    "\n",
    "    # Hyperparameters\n",
    "    lrate = 0.001\n",
    "    momentum = 0.9\n",
    "    \n",
    "    bgd = BGD(in_size, h_size, out_size, lr=lrate, mu=momentum)\n",
    "\n",
    "    # Initialize Root Mean Square Error\n",
    "    RMSE = 0.01\n",
    "    # Initialize number of epochs\n",
    "    epoch = 3000\n",
    "\n",
    "    bgd.train_BGD(train_val_sets, RMSE, epoch)\n",
    "    # end of training\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MGD' object has no attribute 'hidden_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-0134f16c8cda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mylabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Average Cost\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"MGD_leaky_newsample_mlp\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mmgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_loss_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mylabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Predict using testing samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-a18ac3143a7b>\u001b[0m in \u001b[0;36mplot_loss_curve\u001b[1;34m(self, xlabel, ylabel, name, save)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Training Loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Validation Loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"h={self.hidden_size}, lrate={self.lrate}, momentum={self.momentum}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MGD' object has no attribute 'hidden_size'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Plot loss curve\n",
    "    xlabel = \"Epoch\"\n",
    "    ylabel = \"Average Cost\"\n",
    "    name = \"MGD_leaky_newsample_mlp\"\n",
    "    mgd.plot_loss_curve(xlabel, ylabel, name, save=True)\n",
    "\n",
    "    # Predict using testing samples\n",
    "    # Forward pass using the whole dataset\n",
    "    X_test_1o = test_set[..., :9]\n",
    "    y_test_1o = test_set[..., [-1]]\n",
    "\n",
    "    mgd.predict(X_test_1o, y_test_1o)\n",
    "\n",
    "    # Write results to disk\n",
    "    # Predicted\n",
    "    mgd.save_predict(X_test_1o, y_test_1o)\n",
    "\n",
    "    # Summary of results\n",
    "    # Get layers name with the same index as input vector\n",
    "    layers = [file for file in os.listdir(fp_prone) if os.path.isfile(os.path.join(fp_prone, file))][:-1]  #exclude \"readme.txt\" file\n",
    "    filename = \"runs_MGD_leaky_mlp.txt\"\n",
    "    mgd.save_summary(filename, layers)\n",
    "\n",
    "    # Plot target and predicted values\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.xlabel(\"Test samples\")\n",
    "    plt.ylabel(\"Cell Value\")\n",
    "\n",
    "    x, y = copy.deepcopy(X_test_1o), copy.deepcopy(y_test_1o)\n",
    "\n",
    "    # Draw target values\n",
    "    plt.plot(y, \"o\", color=\"b\", label=\"Target value\")\n",
    "\n",
    "    # Draw network output values\n",
    "    loss_testing = []\n",
    "    \n",
    "    for i in range(X_test_1o.shape[0]):\n",
    "        y[i] = mgd.forward(x[i])\n",
    "        rmse = np.sqrt(np.mean((y_test_1o[i] - y[i])**2)) \n",
    "        loss_testing.append(rmse)\n",
    "    \n",
    "    mean_error = np.mean(loss_testing)\n",
    "        \n",
    "    plt.plot(y, '.', color='r', alpha=0.6, label=\"Predicted value\")\n",
    "\n",
    "    plt.legend(loc=2)\n",
    "    # Save plot\n",
    "    op = r\"D:\\MS Gme\\Thesis\\Final Parameters\\ANN\\sublime_run\\jupyter_notebook\\plots\"\n",
    "    fn = f\"{h_size}_{mgd.get_lrate()}_{mgd.get_momentum()}_{mgd.term_iter}_{mean_error}\"\n",
    "\n",
    "    if not os.path.isfile(os.path.join(op, fn)):\n",
    "        plt.savefig(os.path.join(op, f\"MGD_leaky_test_newsample_mlp{fn}.jpg\"))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"\\nThe script finished its execution after %.2f seconds\" % (time.process_time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Generate LSI using the optimized weights\n",
    "    print(\"\\nGenerating lsi using the best fit model...\")\n",
    "    fuzzy_path = r\"D:\\MS Gme\\Thesis\\Final Parameters\\Samples\\for_lsi\\Fuzzy\\fuzzy3\"\n",
    "\n",
    "    lsi_ds = my_thesis.load_fuzzified_layers(fuzzy_path)\n",
    "    X_lsi = lsi_ds[0]\n",
    "    y_lsi = lsi_ds[1]\n",
    "\n",
    "    mgd.set_weights(mgd.get_weights())  # exxlude the bias weights\n",
    "    # Execute forward pass to the whole area per sample\n",
    "    for i in range(X_lsi.shape[0]):\n",
    "        y_lsi[i] = mgd.forward(X_lsi[i])\n",
    "\n",
    "    # Reshape computed lsi to 2D array\n",
    "    lsi = y_lsi.reshape(6334, 3877)\n",
    "\n",
    "    # Export generated lsi\n",
    "    folder = \"lsi\"\n",
    "    out_path = os.getcwd()\n",
    "    op = os.path.join(out_path, folder)\n",
    "    if not os.path.exists(op):\n",
    "        os.mkdir(folder)\n",
    "\n",
    "    fn = \"lsi_MGD_leaky_fuzzy3\"\n",
    "    ref_data = os.path.join(fuzzy_path, \"itogon_grid.tif\")\n",
    "    mgd.export_to_image(ref_data, os.path.join(op, fn), lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

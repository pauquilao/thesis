{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!usr/bin/env python3\n",
    "# Author: Paulo Quilao\n",
    "# MS Geomatics Engineering Thesis\n",
    "# Module for prepartion of datasets for neural network\n",
    "\n",
    "\n",
    "# Import necessary packages\n",
    "import os\n",
    "import glob\n",
    "import gdal\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def convert_layer(filepath) -> np.ndarray:\n",
    "    \"\"\"Converts image file to an array.\"\"\"\n",
    "    try:\n",
    "        # Read params using gdal and convert band 1 to np array\n",
    "        image = gdal.Open(filepath)\n",
    "        if image is None:\n",
    "            print(\"Cannot locate image.\")\n",
    "            return\n",
    "        # Convert raster to numpy array\n",
    "        layer = image.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(repr(e))\n",
    "\n",
    "    finally:\n",
    "        # Close dataset\n",
    "        image = None\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "def get_path_ds(filepath, extension):\n",
    "    \"\"\"Returns path of tif image.\"\"\"\n",
    "    ds_path = glob.glob(os.path.join(filepath, extension))\n",
    "    # assert len(ds_path) == 9, \"Path should contain 9 files\"\n",
    "    return ds_path\n",
    "\n",
    "\n",
    "def get_relevant_val(array):\n",
    "    \"\"\"Returns an array with nontrivial scalars.\"\"\"\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "    array = array[array > -999]\n",
    "    return array\n",
    "\n",
    "# end of helper functions\n",
    "\n",
    "\n",
    "# Class for preparation of datasets\n",
    "class Dataset:\n",
    "    def __init__(self, prone, notprone):\n",
    "        self.prone = prone\n",
    "        self.notprone = notprone\n",
    "        self.file_ext = \"*.tif\"\n",
    "\n",
    "    def set_file_ext(self, extension):\n",
    "        \"\"\"Alters the default image file extension (i.e. .tif).\"\"\"\n",
    "        self.file_ext = extension\n",
    "\n",
    "    def load_layers(self):\n",
    "        \"\"\"Prepare layers for neural network model.\"\"\"\n",
    "\n",
    "        # Read all landslide drivers\n",
    "        print(\"Loading layers for the creation of input vector...\")\n",
    "        print()\n",
    "\n",
    "        # Path to datasets\n",
    "        ds_prone = get_path_ds(self.prone, self.file_ext)\n",
    "        ds_np = get_path_ds(self.notprone, self.file_ext)\n",
    "\n",
    "        # Prone datasets\n",
    "        X_p = []\n",
    "        for image in ds_prone:\n",
    "            X_p.append(convert_layer(image))\n",
    "\n",
    "        # Not prone datasets\n",
    "        X_np = []\n",
    "        for image in ds_np:\n",
    "            X_np.append(convert_layer(image))\n",
    "\n",
    "        # Check shape of each array if read correctly\n",
    "        # Expected output:(6334, 3877)\n",
    "        # If the list is not empty\n",
    "        if X_p and X_np:\n",
    "            print(\"Shape of converted layers to numpy array:\")\n",
    "            flag = True\n",
    "            for factor1, factor2 in zip(X_p, X_np):\n",
    "                assert factor1.shape == factor2.shape, \"Vectorized images should have the same shape.\"\n",
    "                flag = True\n",
    "        if flag:\n",
    "            print(\"prone factor: {}, not prone factor: {}\".format(X_p[0].shape, X_np[0].shape))\n",
    "            print()\n",
    "\n",
    "        else:\n",
    "            print(\"Error in input images.\")\n",
    "\n",
    "        # Convert arrays to row vector\n",
    "        # Landslide prone samples\n",
    "        X_p_flat = []\n",
    "        for factor in X_p:\n",
    "            X_p_flat.append(factor.flatten().astype(\"float32\"))\n",
    "\n",
    "        # Not prone to landslide samples\n",
    "        X_np_flat = []\n",
    "        for factor in X_np:\n",
    "            X_np_flat.append(factor.flatten().astype(\"float32\"))\n",
    "\n",
    "        # Get flattened shape and dtype\n",
    "        if X_p_flat and X_np_flat:\n",
    "            print(\"Converting arrays to 1D arrays...\")\n",
    "            flag = True\n",
    "            for vector1, vector2 in zip(X_p_flat, X_np_flat):\n",
    "                assert vector1.shape == vector2.shape, \"Row vectors should have the same shape.\"\n",
    "                flag = True\n",
    "\n",
    "        if flag:\n",
    "            print(\"Shape of 1D arrays:\")\n",
    "            print(f\"prone factor: {X_p_flat[0].shape}, dtype: {X_p_flat[0].dtype} | not prone factor: {X_np_flat[0].shape}, dtype: {X_np_flat[0].dtype}\")\n",
    "            print()\n",
    "\n",
    "        # Boolean filtering\n",
    "        # Get \"non-nodata\" values for each factor\n",
    "        print(\"Applying boolean filtering to 1D arrays...\")\n",
    "        for i in range(len(X_p_flat)):\n",
    "            X_p_flat[i] = get_relevant_val(X_p_flat[i])\n",
    "            X_np_flat[i] = get_relevant_val(X_np_flat[i])\n",
    "\n",
    "        # Add columns to those factors with less than columns of relevant dataset\n",
    "        for i in range(len(X_np_flat)):\n",
    "            ref = X_p_flat[0].shape[0]\n",
    "            col = X_np_flat[i].shape[0]\n",
    "            if col != ref:\n",
    "                X_np_flat[i] = np.concatenate((X_np_flat[i], [np.mean(X_np_flat[i])] * (ref - col)))\n",
    "\n",
    "        # Get filtered shape and dtype\n",
    "        if X_p_flat and X_np_flat:\n",
    "            flag = True\n",
    "            for vector1, vector2 in zip(X_p_flat, X_np_flat):\n",
    "                assert vector1.shape == vector2.shape, \"Row vectors should have the same shape.\"\n",
    "                flag = True\n",
    "\n",
    "        if flag:\n",
    "            print(\"Shape of filtered 1D arrays:\")\n",
    "            print(f\"prone factor: {X_p_flat[0].shape}, dtype: {X_p_flat[0].dtype} | not prone factor: {X_np_flat[0].shape}, dtype: {X_np_flat[0].dtype}\")\n",
    "            print()\n",
    "\n",
    "        # Scale values of each row vector\n",
    "        print(\"Scaling all input vectors using min-max scaling...\")\n",
    "        X_p_norm = []\n",
    "        X_np_norm = []\n",
    "        for i in range(len(X_p)):\n",
    "            norm_p = (X_p_flat[i] - min(X_p_flat[i])) / (max(X_p_flat[i]) - min(X_p_flat[i]))\n",
    "            norm_np = (X_np_flat[i] - min(X_np_flat[i])) / (max(X_np_flat[i]) - min(X_np_flat[i]))\n",
    "\n",
    "            X_p_norm.append(norm_p)\n",
    "            X_np_norm.append(norm_np)\n",
    "        print(\"All vectors are scaled in the interval [0, 1].\")\n",
    "        print()\n",
    "\n",
    "        # Create input vector X from all parameters including bias\n",
    "        print(\"Creating X vector with bias...\")\n",
    "        bias_p = np.ones(X_p_norm[0].shape[0])\n",
    "        bias_np = np.ones(X_np_norm[0].shape[0])\n",
    "\n",
    "        X_p = np.vstack([X_p_norm, bias_p]).T.astype(\"float32\")\n",
    "        X_np = np.vstack([X_np_norm, bias_np]).T.astype(\"float32\")\n",
    "\n",
    "        print(f\"Landslide prone input vector: {X_p.shape} and data type: {X_p.dtype} \\nNot prone to landslide input vector: {X_np.shape} and data type {X_np.dtype}\")\n",
    "        print()\n",
    "\n",
    "        # Split dataset to training set and testing set\n",
    "        print(\"Splitting samples to training, validation, and testing sets...\")\n",
    "        train_split = int(0.8 * X_p.shape[0])  # 80% of total dataset\n",
    "\n",
    "        X_p_split = np.split(X_p, [train_split])\n",
    "        X_p_train = X_p_split[0]  # get the 80% of the dataset\n",
    "        X_p_test = X_p_split[1]  # get the remaining 80% of the dataset\n",
    "\n",
    "        X_np_split = np.split(X_np, [train_split])\n",
    "        X_np_train = X_np_split[0]\n",
    "        X_np_test = X_np_split[1]\n",
    "\n",
    "        X_train = np.vstack([X_p_train, X_np_train])\n",
    "        X_test = np.vstack([X_p_test, X_np_test])\n",
    "\n",
    "        # These target values resulted to continuous lsi (probability between 0.1 to 0.9)\n",
    "        # For one output node with 0.1 and 0.9 target values\n",
    "        # 0.1 -> absence of landslide; 0.9 -> presence of landslide\n",
    "        y_p_train = np.full(X_p_train.shape[0], 0.9, dtype=\"float32\")\n",
    "        y_p_test = np.full(X_p_test.shape[0], 0.9, dtype=\"float32\")\n",
    "\n",
    "        y_np_train = np.full(X_np_train.shape[0], 0.1, dtype=\"float32\")\n",
    "        y_np_test = np.full(X_np_test.shape[0], 0.1, dtype=\"float32\")\n",
    "\n",
    "        y_train = np.concatenate([y_p_train, y_np_train])\n",
    "        y_train = np.vstack(y_train)\n",
    "        y_test = np.concatenate([y_p_test, y_np_test])\n",
    "        y_test = np.vstack(y_test)\n",
    "\n",
    "        # Check if the total number of dataset matched with training and testing sets\n",
    "        assert (X_train.shape[0] + X_test.shape[0]) == (X_p.shape[0] + X_np.shape[0]), \"Number of rows should match the total number of datasets.\"\n",
    "        assert (X_train.shape[1] == X_p.shape[1]) and (X_test.shape[1] == X_p.shape[1]), \"Number of columns should match the total number of datasets.\"\n",
    "\n",
    "        print(\"Finalizing samples...\")\n",
    "        # Create a union of X and y vectors\n",
    "        dataset_train = np.concatenate((X_train, y_train), axis=1)\n",
    "        dataset_test = np.concatenate((X_test, y_test), axis=1)\n",
    "\n",
    "        # Shuffle datasets\n",
    "        np.random.shuffle(dataset_train)\n",
    "        np.random.shuffle(dataset_test)\n",
    "\n",
    "        # For 1 output node\n",
    "        X_train_1o = dataset_train[..., :10]\n",
    "        y_train_1o = dataset_train[..., [-1]]\n",
    "\n",
    "        # Split train into train and validation sets\n",
    "        # 80:20 ratio\n",
    "        train_val_split = int(0.8 * X_train_1o.shape[0])\n",
    "        X_train_split = np.split(X_train_1o, [train_val_split])\n",
    "        y_train_split = np.split(y_train_1o, [train_val_split])\n",
    "\n",
    "        # X vector\n",
    "        X_train_1o = X_train_split[0]\n",
    "        X_val_1o = X_train_split[1]\n",
    "\n",
    "        # y vector\n",
    "        y_train_1o = y_train_split[0]\n",
    "        y_val_1o = y_train_split[1]\n",
    "\n",
    "        # X and y test sets\n",
    "        X_test_1o = dataset_test[..., :10]\n",
    "        y_test_1o = dataset_test[..., [-1]]\n",
    "\n",
    "        if X_train_1o.shape[0] == y_train_1o.shape[0] and X_test_1o.shape[0] == y_test_1o.shape[0] and X_val_1o.shape[0] == y_val_1o.shape[0]:\n",
    "            print(\"Datasets created with shape: train={}, val={}, test={}\".format(X_train_1o.shape, X_val_1o.shape, y_test_1o.shape))\n",
    "            print()\n",
    "\n",
    "        else:\n",
    "            print(\"Error in dataset shapes.\")\n",
    "\n",
    "        # For one output node\n",
    "        # Merge final train, test, and validation datasets (excluding validation set)\n",
    "        # For shuffling in training regimen\n",
    "        train_xy = np.concatenate([X_train_1o, y_train_1o], axis=1)\n",
    "        val_xy = np.concatenate([X_val_1o, y_val_1o], axis=1)\n",
    "        test_xy = np.concatenate([X_test_1o, y_test_1o], axis=1)\n",
    "\n",
    "        return [train_xy, val_xy, test_xy]\n",
    "\n",
    "    def load_fuzzified_layers(self, path):\n",
    "        \"\"\"Prepare arrays for lsi generation\"\"\"\n",
    "        lsi_path = get_path_ds(path, self.file_ext)\n",
    "\n",
    "        # Read all landslide drivers\n",
    "        X_raw = []\n",
    "        for factor in lsi_path:\n",
    "            X_raw.append(convert_layer(factor))\n",
    "\n",
    "        # Convert to 1D array\n",
    "        X_lsi_flat = []\n",
    "        for array in X_raw:\n",
    "            X_lsi_flat.append(array.flatten())\n",
    "\n",
    "        # Create input vector X from all contributing factors\n",
    "        x_lsi = np.vstack(X_lsi_flat).T.astype(\"float32\")\n",
    "        # Finalize datasets\n",
    "        y_lsi = x_lsi[..., [-1]]\n",
    "        x_lsi = x_lsi[..., :-1]\n",
    "\n",
    "        return [x_lsi, y_lsi]\n",
    "\n",
    "# end of Dataset class\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Filepath for landslide prone and nonprone samples\n",
    "    fp_prone = r\"D:\\ms gme\\thesis\\final parameters\\samples\\Final\\landslide\"\n",
    "    fp_notprone = r\"D:\\ms gme\\thesis\\final parameters\\Samples\\Final\\no_landslide\"\n",
    "\n",
    "    my_thesis = Dataset(fp_prone, fp_notprone)\n",
    "    datasets = my_thesis.load_layers()\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass\n",
    "    # main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Author: Paulo Quilao\n",
    "# MS Geomatics Engineering Thesis\n",
    "# Simulation of landslides using a two-layer neural network\n",
    "\n",
    "\n",
    "# Import necessary packages\n",
    "import os\n",
    "import sys\n",
    "import gdal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import time\n",
    "import copy\n",
    "# from dataset import *  # user-defined module\n",
    "\n",
    "\n",
    "# Neural network parent class\n",
    "class Neural_Network(object):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # Layers\n",
    "        self.layers = args\n",
    "\n",
    "        self.input_size = self.layers[0] + 1  # + 1 for bias\n",
    "        self.hidden_size = self.layers[1]  # number of hidden neurons\n",
    "        self.output_size = self.layers[2]  # number of output neurons\n",
    "\n",
    "        # Parameters\n",
    "        self.W1 = np.random.uniform(0.1, 0.3, size=(self.input_size, self.hidden_size))\n",
    "        self.W2 = np.random.uniform(0.1, 0.3, size=(self.hidden_size, self.output_size))\n",
    "\n",
    "        # Hyperparameters\n",
    "        if len(kwargs.values()) == 1:\n",
    "            if \"lr\" in kwargs:\n",
    "                self.lrate = kwargs[\"lr\"]\n",
    "            else:\n",
    "                # Default value for lrate\n",
    "                self.lrate = 0.01\n",
    "                self.momentum = kwargs[\"mu\"]\n",
    "\n",
    "        elif len(kwargs.values()) > 1:\n",
    "            self.lrate = kwargs[\"lr\"]\n",
    "            self.momentum = kwargs[\"mu\"]\n",
    "\n",
    "        self.cache_W1 = np.zeros(self.W1.shape)  # holds input-to-hidden weights for momentum\n",
    "        self.cache_W2 = np.zeros(self.W2.shape)  # holds hidden-to-output weights for momentum\n",
    "\n",
    "        # Date/time information for exporting\n",
    "        dt = datetime.datetime.now()\n",
    "        self.date = dt.strftime(\"%x\")  # local date\n",
    "        self.time = dt.strftime(\"%X\")  # local time\n",
    "        \n",
    "    # Training methods\n",
    "    # Activation function in output layer\n",
    "    def sigmoid(self, s):\n",
    "        \"\"\"Logistic activation function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "\n",
    "    def sigmoid_prime(self, s):\n",
    "        \"\"\"Derivative of logistic function.\"\"\"\n",
    "        return self.sigmoid(s) * (1 - self.sigmoid(s))\n",
    "\n",
    "    # Activation function in hidden layer\n",
    "    def lrelu(self, x):\n",
    "        \"\"\"Leaky rectified linear unit.\"\"\"\n",
    "        return np.where(x > 0, x, x * 0.01)\n",
    "\n",
    "    def lrelu_prime(self, x):\n",
    "        \"\"\"Derivative of leaky rectified linear unit.\"\"\"\n",
    "        dx = np.ones_like(x)\n",
    "        dx[x < 0] = 0.01\n",
    "        return dx\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward propagation through the network.\"\"\"\n",
    "        self.z = np.dot(X, self.W1)\n",
    "        self.z2 = self.lrelu(self.z)  # activation on hidden layer\n",
    "        self.z3 = np.dot(self.z2, self.W2)\n",
    "        o = self.sigmoid(self.z3)  # final activation on output layer\n",
    "        return o\n",
    "\n",
    "    def backward(self, X, y, o):\n",
    "        \"\"\"Backward propagation to opmtimize weights.\"\"\"\n",
    "        self.o_error = y - o  # loss function\n",
    "        self.o_delta = self.o_error * self.sigmoid_prime(o)  # applying derivative of sigmoid to error\n",
    "\n",
    "        self.z2_error = self.o_delta.dot(self.W2.T)\n",
    "        self.z2_delta = self.z2_error * self.lrelu_prime(self.z2)  # applying derivative of sigmoid to z2 error\n",
    "\n",
    "        # Convert to 2d array to handle 1x1 vectors\n",
    "        X = np.atleast_2d(X)\n",
    "        self.z2_delta = np.atleast_2d(self.z2_delta)\n",
    "\n",
    "        # Momentum update to input-to-hidden weights\n",
    "        dw1 = (X.T.dot(self.z2_delta) * self.lrate) + (self.momentum * self.cache_W1)  # delta input to hidden weights for momentum\n",
    "        self.W1 += dw1\n",
    "\n",
    "        # Convert to 2d array to handle 1x1 vectors\n",
    "        self.z2 = np.atleast_2d(self.z2)\n",
    "        self.o_delta = np.atleast_2d(self.o_delta)\n",
    "\n",
    "        # Momentum update to hidden-to-output weights\n",
    "        dw2 = (self.z2.T.dot(self.o_delta) * self.lrate) + (self.momentum * self.cache_W2)  # delta hidden to output weights for momentum\n",
    "        self.W2 += dw2\n",
    "\n",
    "        # Store previous weights\n",
    "        self.cache_W1 = dw1\n",
    "        self.cache_W2 = dw2\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"Main learning method of the model.\"\"\"\n",
    "        o = self.forward(X)\n",
    "        self.backward(X, y, o)\n",
    "\n",
    "    def compute_error(self, X, y):\n",
    "        \"\"\"Computes RMSE of feeded dataset after a full pass.\"\"\"\n",
    "        feed = self.forward(X)\n",
    "        error = np.sqrt(np.mean((y - feed)**2))\n",
    "        return error\n",
    "\n",
    "    def predict(self, test_x, test_y):\n",
    "        \"\"\"Simulates y based on feeded testing data.\"\"\"\n",
    "        print(\"Predicted data based on trained weights:\")\n",
    "        print(\"Predicted values: {}\\n\".format(self.forward(test_x)))\n",
    "        print(\"Target values: {}\\n\".format(test_y))\n",
    "        rmse = self.compute_error(test_x, test_y)\n",
    "        print(\"Testing loss: {}\\n\".format(rmse))\n",
    "\n",
    "    # end of training methods\n",
    "\n",
    "    # Plot method\n",
    "    def plot_loss_curve(self, xlabel, ylabel, name, save=True):\n",
    "        \"\"\"Creates a loss curve plot.\"\"\"\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.loss_train, label=\"Training Loss\")\n",
    "        plt.plot(self.loss_val, label=\"Validation Loss\")\n",
    "        plt.title(f\"h={self.hidden_size}, lrate={self.lrate}, momentum={self.momentum}\")\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.legend(loc=1)\n",
    "\n",
    "        if save:\n",
    "            # Save plot\n",
    "            folder = \"plots\"\n",
    "            op = os.path.join(os.getcwd(), folder)\n",
    "\n",
    "            if not os.path.exists(op):\n",
    "                os.makedirs(folder)\n",
    "\n",
    "            suffix = f\"{str(self.hidden_size)}_{str(self.lrate)}_{str(self.momentum)}\"\n",
    "\n",
    "            if not os.path.isfile(os.path.join(op, name)):\n",
    "                plt.savefig(os.path.join(op, f\"{name}_{suffix}.jpg\"))\n",
    "            else:\n",
    "                print(\"Name already exists.\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # Write methods\n",
    "    def save_predict(self, test_x, test_y):\n",
    "        \"\"\"Saves a summary of results based on simulated testing data.\"\"\"\n",
    "        folder = \"runs\"\n",
    "        op = os.path.join(os.getcwd(), folder)\n",
    "\n",
    "        if not os.path.exists(op):\n",
    "            os.makedirs(folder)\n",
    "\n",
    "        # Predicted\n",
    "        with open(os.path.join(op, \"predict.txt\"), \"a\") as pred:\n",
    "            pred.write(\"Date and time: {}, {}\\n\".format(self.date, self.time))\n",
    "            pred.write(\"Optimizer: {}\\n\".format(type(self).__name__))\n",
    "            pred.write(\"NN structure: {}\\n\".format(self.layers))\n",
    "            pred.write(\"lrate={}, \".format(self.lrate))\n",
    "            pred.write(\"momentum={}, \".format(self.momentum))\n",
    "            rmse = self.compute_error(test_x, test_y)\n",
    "            pred.write(\"error: {}\\n\\n\".format(rmse))\n",
    "\n",
    "    def save_summary(self, filename, layers: list):\n",
    "        \"\"\"Exports the training summary.\"\"\"\n",
    "        folder = \"runs\"\n",
    "        op = os.path.join(os.getcwd(), folder)\n",
    "\n",
    "        if not os.path.exists(op):\n",
    "            os.makedirs(folder)\n",
    "\n",
    "        # Compute weight of each factor\n",
    "        m_ave = []\n",
    "        m_max = []\n",
    "        weights = self.W1\n",
    "\n",
    "        for w in weights:\n",
    "            m_ave.append(np.mean(w))\n",
    "            m_max.append(np.amax(w))\n",
    "\n",
    "        # Map weights to factors\n",
    "        f = layers\n",
    "        d_ave = dict(zip(f, m_ave[:10]))    # exclude bias, average weight\n",
    "        d_max = dict(zip(f, m_max[:10]))    # exclude bias, max weight\n",
    "\n",
    "        # Save weights if error in testing data is reasonable\n",
    "        with open(os.path.join(op, filename), \"a\") as data:\n",
    "            data.write(\"Date and time: {}, {}\\n\".format(self.date, self.time))\n",
    "            data.write(\"NN structure: {}\\n\".format(self.layers[:-1]))\n",
    "            data.write(\"Hyperparameters: \")\n",
    "            data.write(\"lrate={}, \".format(self.lrate))\n",
    "            data.write(\"mu={}, \".format(self.momentum))\n",
    "\n",
    "            try:\n",
    "                # For MGD\n",
    "                data.write(\"batch size={}\".format(self.batch_size))\n",
    "                data.write(\"\\nCycle: {} for 1 epoch (max of {}), \".format(self.total_batch, epoch))\n",
    "                data.write(\"terminated at epoch={}, iteration={}\".format(self.term_epoch, self.term_iter))\n",
    "\n",
    "            except AttributeError:\n",
    "                # For SGD and BGD\n",
    "                data.write(\"\\nEpoch={}, terminated at:{}\".format(self.epoch, self.term_epoch))\n",
    "\n",
    "            data.write(\"\\nSet and actual errors: \")\n",
    "            data.write(\"RMSE: {}, \".format(self.rmse))\n",
    "            data.write(\"val error: {}\\n\".format(np.amin(self.loss_val)))\n",
    "            data.write(\"Input to hidden weights:\\n {}\\n\".format(self.W1))\n",
    "            data.write(\"Hidden to output weights:\\n {}\\n\".format(self.W2))\n",
    "            data.write(\"Weight of each layer (ave):\\n {}\\n\".format(d_ave))\n",
    "            data.write(\"Weight of each layer (max):\\n {}\\n\\n\".format(d_max))\n",
    "\n",
    "        # Check if the file exists\n",
    "        assert os.path.isfile(os.path.join(op, filename)), \"The file was not created.\"\n",
    "        print(\"\\nSummary of training was exported.\")\n",
    "\n",
    "    def export_to_image(self, ref_path, out_path, array):\n",
    "        \"\"\"Exports array to image (default GeoTiff).\"\"\"\n",
    "        print(\"\\nExporting array to image...\")\n",
    "\n",
    "        # Reference data\n",
    "        data0 = gdal.Open(ref_path)\n",
    "\n",
    "        if data0 is None:\n",
    "            print(\"No reference data.\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        # get rows and columns of array\n",
    "        row = array.shape[0]  # number of pixels in y\n",
    "        col = array.shape[1]  # number of pixels in x\n",
    "\n",
    "        driver = gdal.GetDriverByName('GTiff')\n",
    "        dataset = driver.Create(out_path, col, row, 1, gdal.GDT_Float32)\n",
    "\n",
    "        if dataset is None:\n",
    "            print(\"Could not create lsi.tif\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        dataset.GetRasterBand(1).WriteArray(lsi)\n",
    "        dataset.GetRasterBand(1).SetNodataValue(-9999)\n",
    "\n",
    "        # Add GeoTranform and Projection\n",
    "        geotrans = data0.GetGeoTransform()  # get GeoTranform from ref 'data0'\n",
    "        proj = data0.GetProjection()  # get GetProjection from ref 'data0'\n",
    "        dataset.SetGeoTransform(geotrans)\n",
    "        dataset.SetProjection(proj)\n",
    "        dataset.FlushCache()\n",
    "        dataset = None\n",
    "\n",
    "        # Check if exported correctly\n",
    "        if os.path.isfile(out_path):\n",
    "            print(\"\\nLandslide susceptility index was exported.\")\n",
    "\n",
    "    # Getters\n",
    "    def get_structure(self):\n",
    "        \"\"\"Returns the initialized NN structure.\"\"\"\n",
    "        return self.layers\n",
    "\n",
    "    def get_weights(self):\n",
    "        \"\"\"Returns the weights of the model.\"\"\"\n",
    "        return [self.W1, self.W2]\n",
    "\n",
    "    def get_lrate(self):\n",
    "        \"\"\"Returns the set learning rate.\"\"\"\n",
    "        return self.lrate\n",
    "\n",
    "    def get_momentum(self):\n",
    "        \"\"\"Returns the set momentum factor.\"\"\"\n",
    "        return self.momentum\n",
    "\n",
    "    # Setters\n",
    "    def set_weights(self, weight):\n",
    "        \"\"\"Set the weights for final forward pass (excluding bias weights).\"\"\"\n",
    "        self.W1 = weight[0][0:-1]  # exclude the bias for lsi prediction\n",
    "        self.W2 = weight[1]\n",
    "\n",
    "# end of Neural_Network class\n",
    "\n",
    "\n",
    "# Class optimizers\n",
    "class BGD(Neural_Network):\n",
    "    \"\"\"Batch Gradient Descent.\"\"\"\n",
    "\n",
    "    def train_BGD(self, *args):\n",
    "        \"\"\"Performs simulation using batch gradien descent.\"\"\"\n",
    "\n",
    "        print(f\"\\nStarting BGD Training with NN structure:{self.layers}, lrate={self.lrate}, mu={self.momentum}:\")\n",
    "        training = args[0][0]\n",
    "        validation = args[0][1]\n",
    "\n",
    "        self.rmse = args[1]\n",
    "        self.epoch = args[2]\n",
    "\n",
    "        # X and y training sets\n",
    "        X_train_1o = training[..., :10]\n",
    "        y_train_1o = training[..., [-1]]\n",
    "\n",
    "        # X and y validation sets\n",
    "        X_val_1o = validation[..., :10]\n",
    "        y_val_1o = validation[..., [-1]]\n",
    "\n",
    "        self.loss_train = []\n",
    "        self.loss_val = []\n",
    "\n",
    "        for i in range(epoch):\n",
    "            # Shuffle training set every epoch\n",
    "            np.random.shuffle(training)\n",
    "\n",
    "            # Start training\n",
    "            self.train(X_train_1o, y_train_1o)\n",
    "\n",
    "            # Training loss\n",
    "            rmse_train = self.compute_error(X_train_1o, y_train_1o)\n",
    "\n",
    "            # Validation loss\n",
    "            rmse_val = self.compute_error(X_val_1o, y_val_1o)\n",
    "\n",
    "            # Store errors\n",
    "            self.loss_val.append(rmse_val)\n",
    "            self.loss_train.append(rmse_train)\n",
    "\n",
    "            # Print result per epoch\n",
    "            print(\"epoch: {}\".format(i))\n",
    "            print(\"Training Loss: {}\".format(rmse_train))\n",
    "            print(\"Validation Loss: {}\".format(rmse_val))\n",
    "            print()\n",
    "\n",
    "            # Threshold error\n",
    "            if rmse_val <= self.rmse:\n",
    "                break\n",
    "\n",
    "        # Termination information\n",
    "        self.term_epoch = i\n",
    "\n",
    "        # Training statistics\n",
    "        min_loss_train = np.amin(self.loss_train)\n",
    "        max_loss_train = np.amax(self.loss_train)\n",
    "        mean_loss_train = np.mean(self.loss_train)\n",
    "        min_val_train = np.amin(self.loss_val)\n",
    "        max_val_train = np.amax(self.loss_val)\n",
    "        mean_val_train = np.mean(self.loss_val)\n",
    "\n",
    "        print(\"\\nTraining statistics:\")\n",
    "        print(\"Minimum train error:\", min_loss_train)\n",
    "        print(\"Maximum train error:\", max_loss_train)\n",
    "        print(\"Mean train error:\", mean_loss_train)\n",
    "        print()\n",
    "        print(\"Minimum val error:\", min_val_train)\n",
    "        print(\"Maximum val error:\", max_val_train)\n",
    "        print(\"Mean val error:\", mean_val_train)\n",
    "\n",
    "\n",
    "class SGD(Neural_Network):\n",
    "    \"\"\"Stochastic Gradient Descent.\"\"\"\n",
    "\n",
    "    def train_SGD(self, *args):\n",
    "        \"\"\"Performs simulation using stochastic gradien descent.\"\"\"\n",
    "        print(f\"\\nStarting SGD Training with NN structure:{self.layers}, lrate={self.lrate}, mu={self.momentum}:\")\n",
    "        training = args[0][0]\n",
    "        validation = args[0][1]\n",
    "\n",
    "        self.rmse = args[1]\n",
    "        self.epoch = args[2]\n",
    "\n",
    "        # X and y training sets\n",
    "        X_train_1o = training[..., :10]\n",
    "        y_train_1o = training[..., [-1]]\n",
    "\n",
    "        # X and y validation sets\n",
    "        X_val_1o = validation[..., :10]\n",
    "        y_val_1o = validation[..., [-1]]\n",
    "\n",
    "        self.loss_train = []\n",
    "        self.loss_val = []\n",
    "\n",
    "        for i in range(epoch):\n",
    "            # Stochastic sampling\n",
    "            n = np.random.choice(X_train_1o.shape[0])\n",
    "\n",
    "            # Start training\n",
    "            self.train(X_train_1o[n], y_train_1o[n])\n",
    "\n",
    "            # Training loss\n",
    "            rmse_train = self.compute_error(X_train_1o[n], y_train_1o[n])\n",
    "\n",
    "            # Validation loss\n",
    "            rmse_val = self.compute_error(X_val_1o, y_val_1o)\n",
    "\n",
    "            # Store errors\n",
    "            self.loss_val.append(rmse_val)\n",
    "            self.loss_train.append(rmse_train)\n",
    "\n",
    "            # Print result per epoch\n",
    "            print(\"epoch: {}\".format(i))\n",
    "            print(\"Training Loss: {}\".format(rmse_train))\n",
    "            print(\"Validation Loss: {}\".format(rmse_val))\n",
    "            print()\n",
    "\n",
    "            # Threshold error\n",
    "            if rmse_val <= self.rmse:\n",
    "                break\n",
    "\n",
    "        # Termination information\n",
    "        self.term_epoch = i\n",
    "\n",
    "        # Training statistics\n",
    "        min_loss_train = np.amin(self.loss_train)\n",
    "        max_loss_train = np.amax(self.loss_train)\n",
    "        mean_loss_train = np.mean(self.loss_train)\n",
    "        min_val_train = np.amin(self.loss_val)\n",
    "        max_val_train = np.amax(self.loss_val)\n",
    "        mean_val_train = np.mean(self.loss_val)\n",
    "\n",
    "        print(\"\\nTraining statistics:\")\n",
    "        print(\"Minimum train error:\", min_loss_train)\n",
    "        print(\"Maximum train error:\", max_loss_train)\n",
    "        print(\"Mean train error:\", mean_loss_train)\n",
    "        print()\n",
    "        print(\"Minimum val error:\", min_val_train)\n",
    "        print(\"Maximum val error:\", max_val_train)\n",
    "        print(\"Mean val error:\", mean_val_train)\n",
    "\n",
    "\n",
    "class MGD(Neural_Network):\n",
    "    \"\"\"Mini-batch Gradient Descent.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        Neural_Network.__init__(self, *args, **kwargs)\n",
    "        self.batch_size = args[-1]\n",
    "\n",
    "    def train_MGD(self, *args):\n",
    "        \"\"\"Performs simulation using mini-batch gradien descent\"\"\"\n",
    "        print(f\"\\nStarting MGD Training with NN structure:{self.layers}, lrate={self.lrate}, mu={self.momentum}:\")\n",
    "\n",
    "        training = args[0][0]\n",
    "        validation = args[0][1]\n",
    "\n",
    "        self.rmse = args[1]\n",
    "        self.epoch = args[2]\n",
    "\n",
    "        # X and y training sets\n",
    "        X_train_1o = training[..., :10]\n",
    "        y_train_1o = training[..., [-1]]\n",
    "\n",
    "        # X and y validation sets\n",
    "        X_val_1o = validation[..., :10]\n",
    "        y_val_1o = validation[..., [-1]]\n",
    "\n",
    "        # Initialize number of iterations\n",
    "        self.total_batch = X_train_1o.shape[0] // (self.batch_size - 1)  # exlude the last index\n",
    "        val_batch = X_val_1o.shape[0] // (self.batch_size - 1)\n",
    "\n",
    "        self.loss_train = []\n",
    "        self.loss_val = []\n",
    "        for i in range(epoch):\n",
    "\n",
    "            # Shuffle datasets each epoch (after completely seeing the entire dataset)\n",
    "            np.random.shuffle(training)\n",
    "\n",
    "            # Reassign X and y training vectors\n",
    "            X_train_1o = training[..., :10]\n",
    "            y_train_1o = training[..., [-1]]\n",
    "\n",
    "            X_batches = np.array_split(X_train_1o, self.total_batch)\n",
    "            y_batches = np.array_split(y_train_1o, self.total_batch)\n",
    "\n",
    "            # Initialize rmse\n",
    "            # Will be reset after completing 1 epoch\n",
    "            ave_cost_train = 0\n",
    "\n",
    "            # Train the neural network per mini-batch\n",
    "            for j in range(self.total_batch):\n",
    "                minibatch_x = X_batches[j]\n",
    "                minibatch_y = y_batches[j]\n",
    "\n",
    "                # Train per mini-batch\n",
    "                self.train(minibatch_x, minibatch_y)\n",
    "\n",
    "                # Train loss for each mini-batch\n",
    "                rmse_train = self.compute_error(minibatch_x, minibatch_y)\n",
    "\n",
    "                # Get average training cost for each epoch\n",
    "                ave_cost_train += rmse_train / self.total_batch\n",
    "\n",
    "            self.loss_train.append(ave_cost_train)\n",
    "\n",
    "#             # Validation using the whole validation set\n",
    "#             rmse_val = self.compute_error(X_val_1o, y_val_1o)\n",
    "#             self.loss_val.append(rmse_val)\n",
    "\n",
    "#             # Print result per epoch\n",
    "#             print(\"epoch: {}\".format(i))\n",
    "#             print(\"Training loss: {}\".format(ave_cost_train))\n",
    "#             print(\"Validation loss: {}\".format(rmse_val))\n",
    "#             print()\n",
    "\n",
    "#             if rmse_val <= RMSE:\n",
    "#                 break\n",
    "\n",
    "            # Per mini-batch validation\n",
    "            ave_cost_val = 0\n",
    "            x_val = np.array_split(X_val_1o, val_batch)\n",
    "            y_val = np.array_split(y_val_1o, val_batch)\n",
    "            for v in range(val_batch):\n",
    "                mb_val_x = x_val[v]\n",
    "                mb_val_y = y_val[v]\n",
    "\n",
    "                # Validation loss\n",
    "                rmse_val = self.compute_error(mb_val_x, mb_val_y)\n",
    "\n",
    "                # Get average val cost for each epoch\n",
    "                ave_cost_val += rmse_val / val_batch\n",
    "\n",
    "            self.loss_val.append(ave_cost_val)\n",
    "\n",
    "            # Print result per epoch\n",
    "            print(\"epoch: {}\".format(i))\n",
    "            print(\"Training loss: {}\".format(ave_cost_train))\n",
    "            print(\"Validation loss: {}\".format(ave_cost_val))\n",
    "            print()\n",
    "\n",
    "            # Stoping criterion using validation set\n",
    "            if ave_cost_val <= self.rmse:\n",
    "                print(\"Training done.\")\n",
    "                break\n",
    "\n",
    "        # Termination information\n",
    "        self.term_epoch = i\n",
    "        self.term_iter = j\n",
    "\n",
    "        # Training statistics\n",
    "        min_loss_train = np.amin(self.loss_train)\n",
    "        max_loss_train = np.amax(self.loss_train)\n",
    "        mean_loss_train = np.mean(self.loss_train)\n",
    "        min_val_train = np.amin(self.loss_val)\n",
    "        max_val_train = np.amax(self.loss_val)\n",
    "        mean_val_train = np.mean(self.loss_val)\n",
    "\n",
    "        print(\"\\nTraining statistics:\")\n",
    "        print(\"Minimum train error:\", min_loss_train)\n",
    "        print(\"Maximum train error:\", max_loss_train)\n",
    "        print(\"Mean train error:\", mean_loss_train)\n",
    "        print()\n",
    "        print(\"Minimum val error:\", min_val_train)\n",
    "        print(\"Maximum val error:\", max_val_train)\n",
    "        print(\"Mean val error:\", mean_val_train)\n",
    "\n",
    "# end of class optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Start time\n",
    "    start_time = time.process_time()\n",
    "\n",
    "    # Initiate seed for easier debugging\n",
    "    seed = 100\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Filepath for landslide prone and nonprone samples\n",
    "    fp_prone = r\"D:\\ms gme\\thesis\\final parameters\\samples\\Final\\landslide\\fixed_nd\"\n",
    "    fp_notprone = r\"D:\\MS Gme\\Thesis\\Final Parameters\\Samples\\Final\\no_ls_5deg_slope\\fixed_nd\"\n",
    "\n",
    "    my_thesis = Dataset(fp_prone, fp_notprone)\n",
    "\n",
    "    # Load landslide factors\n",
    "    datasets = my_thesis.load_layers()\n",
    "    train_val_sets = datasets[0:2]\n",
    "    test_set = datasets[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Start training\n",
    "    # Instantiate NN class\n",
    "    in_size = 9  # input layer\n",
    "    h_size = 25  # hidden layer\n",
    "    out_size = 1  # output layer\n",
    "\n",
    "    # Hyperparameters\n",
    "    lrate = 0.01\n",
    "    momentum = 0.9\n",
    "    batch_size = 32\n",
    "    mgd = MGD(in_size, h_size, out_size, batch_size, lr=lrate, mu=momentum)\n",
    "\n",
    "    # Initialize Root Mean Square Error\n",
    "    RMSE = 0.01\n",
    "    # Initialize number of epochs\n",
    "    epoch = 3000\n",
    "\n",
    "    mgd.train_MGD(train_val_sets, RMSE, epoch)\n",
    "    # end of training\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Plot loss curve\n",
    "    xlabel = \"Epoch\"\n",
    "    ylabel = \"Average Cost\"\n",
    "    name = \"MGD_leaky_new\"\n",
    "    mgd.plot_loss_curve(xlabel, ylabel, name, save=False)\n",
    "\n",
    "    # Predict using testing samples\n",
    "    # Forward pass using the whole dataset\n",
    "    X_test_1o = test_set[..., :10]\n",
    "    y_test_1o = test_set[..., [-1]]\n",
    "\n",
    "    mgd.predict(X_test_1o, y_test_1o)\n",
    "\n",
    "    # Write results to disk\n",
    "    # Predicted\n",
    "#     mgd.save_predict(X_test_1o, y_test_1o)\n",
    "\n",
    "    # Summary of results\n",
    "    # Get layers name with the same index as input vector\n",
    "    layers = [file for file in os.listdir(fp_prone) if os.path.isfile(os.path.join(fp_prone, file))]\n",
    "    filename = \"runs_MGD_leaky_trial.txt\"\n",
    "    mgd.save_summary(filename, layers)\n",
    "\n",
    "    # Plot target and predicted values\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.xlabel(\"Test samples\")\n",
    "    plt.ylabel(\"Cell Value\")\n",
    "\n",
    "    x, y = copy.deepcopy(X_test_1o), copy.deepcopy(y_test_1o)\n",
    "\n",
    "    # Draw target values\n",
    "    plt.plot(y, \"o\", color=\"b\", label=\"Target value\")\n",
    "\n",
    "    # Draw network output values\n",
    "    for i in range(X_test_1o.shape[0]):\n",
    "        y[i] = mgd.forward(x[i])\n",
    "    plt.plot(y, '.', color='r', alpha=0.6, label=\"Predicted value\")\n",
    "\n",
    "    plt.legend(loc=2)\n",
    "    # Save plot\n",
    "    op = r\"D:\\MS Gme\\Thesis\\Final Parameters\\ANN\\sublime_run\\plots\"\n",
    "    fn = f\"{str(h_size)}_{str(mgd.get_lrate())}_{str(mgd.get_momentum())}_{str(mgd.term_iter)}_{mean_error}\"\n",
    "\n",
    "#     if not os.path.isfile(os.path.join(op, fn)):\n",
    "#         plt.savefig(os.path.join(op, f\"MGD_leaky_test_new{fn}.jpg\"))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"\\nThe script finished its execution after %.2f seconds\" % (time.process_time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Generate LSI using the optimized weights\n",
    "    print(\"\\nGenerating lsi using the best fit model.\")\n",
    "    fuzzy_path = r\"D:\\MS Gme\\Thesis\\Final Parameters\\Samples\\for_lsi\\Fuzzy\\fuzzy2\"\n",
    "    tiff = \"*.tif\"\n",
    "\n",
    "    lsi_ds = my_thesis.load_fuzzified_layers(fuzzy_path, tiff)\n",
    "    X_lsi = lsi_ds[0]\n",
    "    y_lsi = lsi_ds[1]\n",
    "\n",
    "    mgd.set_weights(mgd.get_weights())  # exxlude the bias weights\n",
    "    # Execute forward pass to the whole area per sample\n",
    "    for i in range(X_lsi.shape[0]):\n",
    "        y_lsi[i] = mgd.forward(x[i])\n",
    "\n",
    "    # Reshape computed lsi to 2D array\n",
    "    lsi = y_lsi.reshape(6334, 3877)\n",
    "\n",
    "    # Export generated lsi\n",
    "    folder = \"lsi\"\n",
    "    out_path = os.getcwd()\n",
    "    op = os.path.join(out_path, folder)\n",
    "    if not os.path.exists(op):\n",
    "        os.mkdir(folder)\n",
    "\n",
    "    fn = \"lsi_MGD_leaky\"\n",
    "    ref_data = os.path.join(fuzzy_path, \"itogon_grid.tif\")\n",
    "    mgd.export_to_image(ref_data, os.path.join(op, fn), lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!usr/bin/env python3\n",
    "# Author: Paulo Quilao\n",
    "# MS Geomatics Engineering Thesis\n",
    "# Module for prepartion of datasets for neural network\n",
    "\n",
    "\n",
    "# Import necessary packages\n",
    "import os\n",
    "import glob\n",
    "import gdal\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def convert_layer(filepath) -> np.ndarray:\n",
    "    \"\"\"Converts image file to an array.\"\"\"\n",
    "    try:\n",
    "        # Read params using gdal and convert band 1 to np array\n",
    "        image = gdal.Open(filepath)\n",
    "        if image is None:\n",
    "            print(\"Cannot locate image.\")\n",
    "            return\n",
    "        # Convert raster to numpy array\n",
    "        layer = image.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(repr(e))\n",
    "\n",
    "    finally:\n",
    "        # Close dataset\n",
    "        image = None\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "def get_path_ds(filepath, extension):\n",
    "    \"\"\"Returns path of tif image.\"\"\"\n",
    "    ds_path = glob.glob(os.path.join(filepath, extension))\n",
    "    # assert len(ds_path) == 9, \"Path should contain 9 files\"\n",
    "    return ds_path\n",
    "\n",
    "\n",
    "def get_relevant_val(array):\n",
    "    \"\"\"Returns an array with nontrivial scalars.\"\"\"\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "    array = array[array > -999]\n",
    "    return array\n",
    "\n",
    "# end of helper functions\n",
    "\n",
    "\n",
    "# Class for preparation of datasets\n",
    "class Dataset:\n",
    "    def __init__(self, prone, notprone):\n",
    "        self.prone = prone\n",
    "        self.notprone = notprone\n",
    "        self.file_ext = \"*.tif\"\n",
    "\n",
    "    def load_layers(self):\n",
    "        \"\"\"Prepare layers for neural network model.\"\"\"\n",
    "\n",
    "        # Read all landslide drivers\n",
    "        print(\"Loading layers for the creation of input vector...\")\n",
    "        print()\n",
    "\n",
    "        # Path to datasets\n",
    "        ds_prone = get_path_ds(self.prone, self.file_ext)\n",
    "        ds_np = get_path_ds(self.notprone, self.file_ext)\n",
    "\n",
    "        # Prone datasets\n",
    "        X_p = []\n",
    "        for image in ds_prone:\n",
    "            X_p.append(convert_layer(image))\n",
    "\n",
    "        # Not prone datasets\n",
    "        X_np = []\n",
    "        for image in ds_np:\n",
    "            X_np.append(convert_layer(image))\n",
    "\n",
    "        # Check shape of each array if read correctly\n",
    "        # Expected output:(6334, 3877)\n",
    "        # If the list is not empty\n",
    "        if X_p and X_np:\n",
    "            print(\"Shape of converted layers to numpy array:\")\n",
    "            flag = True\n",
    "            for factor1, factor2 in zip(X_p, X_np):\n",
    "                assert factor1.shape == factor2.shape, \"Vectorized images should have the same shape.\"\n",
    "                flag = True\n",
    "        if flag:\n",
    "            print(\"prone factor: {}, not prone factor: {}\".format(X_p[0].shape, X_np[0].shape))\n",
    "            print()\n",
    "\n",
    "        else:\n",
    "            print(\"Error in input images.\")\n",
    "\n",
    "        # Convert arrays to row vector\n",
    "        # Landslide prone samples\n",
    "        X_p_flat = []\n",
    "        for factor in X_p:\n",
    "            X_p_flat.append(factor.flatten().astype(\"float32\"))\n",
    "\n",
    "        # Not prone to landslide samples\n",
    "        X_np_flat = []\n",
    "        for factor in X_np:\n",
    "            X_np_flat.append(factor.flatten().astype(\"float32\"))\n",
    "\n",
    "        # Get flattened shape and dtype\n",
    "        if X_p_flat and X_np_flat:\n",
    "            print(\"Converting arrays to 1D arrays...\")\n",
    "            flag = True\n",
    "            for vector1, vector2 in zip(X_p_flat, X_np_flat):\n",
    "                assert vector1.shape == vector2.shape, \"Row vectors should have the same shape.\"\n",
    "                flag = True\n",
    "\n",
    "        if flag:\n",
    "            print(\"Shape of 1D arrays:\")\n",
    "            print(f\"prone factor: {X_p_flat[0].shape}, dtype: {X_p_flat[0].dtype} | not prone factor: {X_np_flat[0].shape}, dtype: {X_np_flat[0].dtype}\")\n",
    "            print()\n",
    "\n",
    "        # Boolean filtering\n",
    "        # Get \"non-nodata\" values for each factor\n",
    "        print(\"Applying boolean filtering to 1D arrays...\")\n",
    "        for i in range(len(X_p_flat)):\n",
    "            X_p_flat[i] = get_relevant_val(X_p_flat[i])\n",
    "            X_np_flat[i] = get_relevant_val(X_np_flat[i])\n",
    "\n",
    "        # Add columns to those factors with less than columns of relevant dataset\n",
    "        for i in range(len(X_np_flat)):\n",
    "            ref = X_p_flat[0].shape[0]\n",
    "            col = X_np_flat[i].shape[0]\n",
    "            if col != ref:\n",
    "                X_np_flat[i] = np.concatenate((X_np_flat[i], [np.mean(X_np_flat[i])] * (ref - col)))\n",
    "\n",
    "        # Get filtered shape and dtype\n",
    "        if X_p_flat and X_np_flat:\n",
    "            flag = True\n",
    "            for vector1, vector2 in zip(X_p_flat, X_np_flat):\n",
    "                assert vector1.shape == vector2.shape, \"Row vectors should have the same shape.\"\n",
    "                flag = True\n",
    "\n",
    "        if flag:\n",
    "            print(\"Shape of filtered 1D arrays:\")\n",
    "            print(f\"prone factor: {X_p_flat[0].shape}, dtype: {X_p_flat[0].dtype} | not prone factor: {X_np_flat[0].shape}, dtype: {X_np_flat[0].dtype}\")\n",
    "            print()\n",
    "\n",
    "        # Scale values of each row vector\n",
    "        print(\"Scaling all input vectors using min-max scaling...\")\n",
    "        X_p_norm = []\n",
    "        X_np_norm = []\n",
    "        for i in range(len(X_p)):\n",
    "            norm_p = (X_p_flat[i] - min(X_p_flat[i])) / (max(X_p_flat[i]) - min(X_p_flat[i]))\n",
    "            norm_np = (X_np_flat[i] - min(X_np_flat[i])) / (max(X_np_flat[i]) - min(X_np_flat[i]))\n",
    "\n",
    "            X_p_norm.append(norm_p)\n",
    "            X_np_norm.append(norm_np)\n",
    "        print(\"All vectors are scaled in the interval [0, 1].\")\n",
    "        print()\n",
    "\n",
    "        # Create input vector X from all parameters including bias\n",
    "        print(\"Creating X vector with bias...\")\n",
    "        bias_p = np.ones(X_p_norm[0].shape[0])\n",
    "        bias_np = np.ones(X_np_norm[0].shape[0])\n",
    "\n",
    "        X_p = np.vstack([X_p_norm, bias_p]).T.astype(\"float32\")\n",
    "        X_np = np.vstack([X_np_norm, bias_np]).T.astype(\"float32\")\n",
    "\n",
    "        print(f\"Landslide prone input vector: {X_p.shape} and data type: {X_p.dtype} \\nNot prone to landslide input vector: {X_np.shape} and data type {X_np.dtype}\")\n",
    "        print()\n",
    "\n",
    "        # Split dataset to training set and testing set\n",
    "        print(\"Splitting samples to training, validation, and testing sets...\")\n",
    "        train_split = int(0.8 * X_p.shape[0])  # 80% of total dataset\n",
    "\n",
    "        X_p_split = np.split(X_p, [train_split])\n",
    "        X_p_train = X_p_split[0]  # get the 80% of the dataset\n",
    "        X_p_test = X_p_split[1]  # get the remaining 80% of the dataset\n",
    "\n",
    "        X_np_split = np.split(X_np, [train_split])\n",
    "        X_np_train = X_np_split[0]\n",
    "        X_np_test = X_np_split[1]\n",
    "\n",
    "        X_train = np.vstack([X_p_train, X_np_train])\n",
    "        X_test = np.vstack([X_p_test, X_np_test])\n",
    "\n",
    "        # These target values resulted to continuous lsi (probability between 0.1 to 0.9)\n",
    "        # For one output node with 0.1 and 0.9 target values\n",
    "        # 0.1 -> absence of landslide; 0.9 -> presence of landslide\n",
    "        y_p_train = np.full(X_p_train.shape[0], 0.9, dtype=\"float32\")\n",
    "        y_p_test = np.full(X_p_test.shape[0], 0.9, dtype=\"float32\")\n",
    "\n",
    "        y_np_train = np.full(X_np_train.shape[0], 0.1, dtype=\"float32\")\n",
    "        y_np_test = np.full(X_np_test.shape[0], 0.1, dtype=\"float32\")\n",
    "\n",
    "        y_train = np.concatenate([y_p_train, y_np_train])\n",
    "        y_train = np.vstack(y_train)\n",
    "        y_test = np.concatenate([y_p_test, y_np_test])\n",
    "        y_test = np.vstack(y_test)\n",
    "\n",
    "        # Check if the total number of dataset matched with training and testing sets\n",
    "        assert (X_train.shape[0] + X_test.shape[0]) == (X_p.shape[0] + X_np.shape[0]), \"Number of rows should match the total number of datasets.\"\n",
    "        assert (X_train.shape[1] == X_p.shape[1]) and (X_test.shape[1] == X_p.shape[1]), \"Number of columns should match the total number of datasets.\"\n",
    "\n",
    "        print(\"Finalizing samples...\")\n",
    "        # Create a union of X and y vectors\n",
    "        dataset_train = np.concatenate((X_train, y_train), axis=1)\n",
    "        dataset_test = np.concatenate((X_test, y_test), axis=1)\n",
    "\n",
    "        # Shuffle datasets\n",
    "        np.random.shuffle(dataset_train)\n",
    "        np.random.shuffle(dataset_test)\n",
    "\n",
    "        # For 1 output node\n",
    "        X_train_1o = dataset_train[..., :10]\n",
    "        y_train_1o = dataset_train[..., [-1]]\n",
    "\n",
    "        # Split train into train and validation sets\n",
    "        # 80:20 ratio\n",
    "        train_val_split = int(0.8 * X_train_1o.shape[0])\n",
    "        X_train_split = np.split(X_train_1o, [train_val_split])\n",
    "        y_train_split = np.split(y_train_1o, [train_val_split])\n",
    "\n",
    "        # X vector\n",
    "        X_train_1o = X_train_split[0]\n",
    "        X_val_1o = X_train_split[1]\n",
    "\n",
    "        # y vector\n",
    "        y_train_1o = y_train_split[0]\n",
    "        y_val_1o = y_train_split[1]\n",
    "\n",
    "        # X and y test sets\n",
    "        X_test_1o = dataset_test[..., :10]\n",
    "        y_test_1o = dataset_test[..., [-1]]\n",
    "\n",
    "        if X_train_1o.shape[0] == y_train_1o.shape[0] and X_test_1o.shape[0] == y_test_1o.shape[0] and X_val_1o.shape[0] == y_val_1o.shape[0]:\n",
    "            print(\"Datasets created with shape: train={}, val={}, test={}\".format(X_train_1o.shape, X_val_1o.shape, y_test_1o.shape))\n",
    "            print()\n",
    "\n",
    "        else:\n",
    "            print(\"Error in dataset shapes.\")\n",
    "\n",
    "        # For one output node\n",
    "        # Merge final train, test, and validation datasets (excluding validation set)\n",
    "        # For shuffling in training regimen\n",
    "        train_xy = np.concatenate([X_train_1o, y_train_1o], axis=1)\n",
    "        val_xy = np.concatenate([X_val_1o, y_val_1o], axis=1)\n",
    "        test_xy = np.concatenate([X_test_1o, y_test_1o], axis=1)\n",
    "\n",
    "        return [train_xy, val_xy, test_xy]\n",
    "\n",
    "    def load_fuzzified_layers(self, path):\n",
    "        \"\"\"Prepare arrays for lsi generation\"\"\"\n",
    "        lsi_path = get_path_ds(path, self.file_ext)\n",
    "\n",
    "        # Read all landslide drivers\n",
    "        X_raw = []\n",
    "        for factor in lsi_path:\n",
    "            X_raw.append(convert_layer(factor))\n",
    "\n",
    "        # Convert to 1D array\n",
    "        X_lsi_flat = []\n",
    "        for array in X_raw:\n",
    "            X_lsi_flat.append(array.flatten())\n",
    "\n",
    "        # Create input vector X from all contributing factors\n",
    "        x_lsi = np.vstack(X_lsi_flat).T.astype(\"float32\")\n",
    "        # Finalize datasets\n",
    "        y_lsi = x_lsi[..., [-1]]\n",
    "#         x_lsi = x_lsi[..., :-1]\n",
    "\n",
    "        return [x_lsi, y_lsi]\n",
    "\n",
    "# end of Dataset class\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Filepath for landslide prone and nonprone samples\n",
    "    fp_prone = r\"D:\\ms gme\\thesis\\final parameters\\samples\\Final\\landslide\"\n",
    "    fp_notprone = r\"D:\\ms gme\\thesis\\final parameters\\Samples\\Final\\no_landslide\"\n",
    "\n",
    "    my_thesis = Dataset(fp_prone, fp_notprone)\n",
    "    datasets = my_thesis.load_layers()\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass\n",
    "    # main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Author: Paulo Quilao\n",
    "# MS Geomatics Engineering Thesis\n",
    "# Simulation of landslides using a two-layer neural network\n",
    "\n",
    "\n",
    "# Import necessary packages\n",
    "import os\n",
    "import sys\n",
    "import gdal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import time\n",
    "import copy\n",
    "# from dataset import *  # user-defined module\n",
    "\n",
    "\n",
    "# Neural network parent class\n",
    "class Neural_Network(object):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # Layers\n",
    "        self.layers = args\n",
    "\n",
    "        self.input_size = self.layers[0] + 1  # + 1 for bias\n",
    "        self.hidden_size = self.layers[1]  # number of hidden neurons\n",
    "        self.output_size = self.layers[2]  # number of output neurons\n",
    "\n",
    "        # Parameters\n",
    "        self.W1 = np.random.uniform(0.1, 0.3, size=(self.input_size, self.hidden_size))\n",
    "        self.W2 = np.random.uniform(0.1, 0.3, size=(self.hidden_size, self.output_size))\n",
    "\n",
    "        # Hyperparameters\n",
    "        if \"lr\" not in kwargs:\n",
    "            self.lrate = 0.1  # default lrate value\n",
    "        else:\n",
    "            self.lrate = kwargs[\"lr\"]\n",
    "                \n",
    "        if \"mu\" not in kwargs:\n",
    "            self.momentum = 0.1  # default momentum value\n",
    "        else:\n",
    "            self.momentum = kwargs[\"mu\"]\n",
    "\n",
    "        self.cache_W1 = np.zeros(self.W1.shape)  # holds input-to-hidden weights for momentum\n",
    "        self.cache_W2 = np.zeros(self.W2.shape)  # holds hidden-to-output weights for momentum\n",
    "\n",
    "        # Date/time information for exporting\n",
    "        dt = datetime.datetime.now()\n",
    "        self.date = dt.strftime(\"%x\")  # local date\n",
    "        self.time = dt.strftime(\"%X\")  # local time\n",
    "        \n",
    "    # Training methods\n",
    "    # Activation function in output layer\n",
    "    def sigmoid(self, s):\n",
    "        \"\"\"Logistic activation function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "\n",
    "    def sigmoid_prime(self, s):\n",
    "        \"\"\"Derivative of logistic function.\"\"\"\n",
    "        return self.sigmoid(s) * (1 - self.sigmoid(s))\n",
    "\n",
    "    # Activation function in hidden layer\n",
    "    def lrelu(self, x):\n",
    "        \"\"\"Leaky rectified linear unit.\"\"\"\n",
    "        return np.where(x > 0, x, x * 0.01)\n",
    "\n",
    "    def lrelu_prime(self, x):\n",
    "        \"\"\"Derivative of leaky rectified linear unit.\"\"\"\n",
    "        dx = np.ones_like(x)\n",
    "        dx[x < 0] = 0.01\n",
    "        return dx\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward propagation of input vectors up to output layer.\"\"\"\n",
    "        self.z = np.dot(X, self.W1)\n",
    "        self.z2 = self.lrelu(self.z)  # activation on hidden layer\n",
    "        self.z3 = np.dot(self.z2, self.W2)\n",
    "        o = self.sigmoid(self.z3)  # final activation on output layer\n",
    "        return o\n",
    "\n",
    "    def backward(self, X, y, o):\n",
    "        \"\"\"Backward propagation of error using chain rule to opmtimize weights.\"\"\"\n",
    "        self.o_error = y - o  # loss function\n",
    "        self.o_delta = self.o_error * self.sigmoid_prime(o)  # applying derivative of logistic to error\n",
    "\n",
    "        self.z2_error = self.o_delta.dot(self.W2.T)\n",
    "        self.z2_delta = self.z2_error * self.lrelu_prime(self.z2)  # applying derivative of leaky relu to z2 error\n",
    "\n",
    "        # Convert to 2d array to handle 1x1 vectors\n",
    "        X = np.atleast_2d(X)\n",
    "        self.z2_delta = np.atleast_2d(self.z2_delta)\n",
    "\n",
    "        # Momentum update to input-to-hidden weights\n",
    "        dw1 = (X.T.dot(self.z2_delta) * self.lrate) + (self.momentum * self.cache_W1)  # delta input to hidden weights\n",
    "        self.W1 += dw1\n",
    "\n",
    "        # Convert to 2d array to handle 1x1 vectors\n",
    "        self.z2 = np.atleast_2d(self.z2)\n",
    "        self.o_delta = np.atleast_2d(self.o_delta)\n",
    "\n",
    "        # Momentum update to hidden-to-output weights\n",
    "        dw2 = (self.z2.T.dot(self.o_delta) * self.lrate) + (self.momentum * self.cache_W2)  # delta hidden to output weights\n",
    "        self.W2 += dw2\n",
    "\n",
    "        # Store previous weights\n",
    "        self.cache_W1 = dw1\n",
    "        self.cache_W2 = dw2\n",
    "        \n",
    "        # RMSE\n",
    "        error = np.sqrt(np.mean(self.o_error**2))\n",
    "        return error\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"Main learning method of the model.\"\"\"\n",
    "        o = self.forward(X)\n",
    "        rmse_train = self.backward(X, y, o)\n",
    "        return rmse_train\n",
    "\n",
    "    def compute_error(self, X, y):\n",
    "        \"\"\"Computes RMSE of feeded dataset after a full pass.\"\"\"\n",
    "        feed = self.forward(X)\n",
    "        error = np.sqrt(np.mean((y - feed)**2))\n",
    "        return error\n",
    "\n",
    "    def predict(self, test_x, test_y):\n",
    "        \"\"\"Simulates y based on feeded testing data.\"\"\"\n",
    "        print(\"Predicted data based on trained weights:\")\n",
    "        print(\"Predicted values: {}\\n\".format(self.forward(test_x)))\n",
    "        print(\"Target values: {}\\n\".format(test_y))\n",
    "        rmse = self.compute_error(test_x, test_y)\n",
    "        print(\"Testing loss: {}\\n\".format(rmse))\n",
    "\n",
    "    # end of training methods\n",
    "\n",
    "    # Plot method\n",
    "    def plot_loss_curve(self, xlabel, ylabel, name, save=True):\n",
    "        \"\"\"Creates a loss curve plot.\"\"\"\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.loss_train, label=\"Training Loss\")\n",
    "        plt.plot(self.loss_val, label=\"Validation Loss\")\n",
    "        plt.title(f\"h={self.hidden_size}, lrate={self.lrate}, momentum={self.momentum}\")\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.legend(loc=1)\n",
    "\n",
    "        if save:\n",
    "            # Save plot\n",
    "            folder = \"plots\"\n",
    "            op = os.path.join(os.getcwd(), folder)\n",
    "\n",
    "            if not os.path.exists(op):\n",
    "                os.makedirs(folder)\n",
    "\n",
    "            suffix = f\"{str(self.hidden_size)}_{str(self.lrate)}_{str(self.momentum)}\"\n",
    "\n",
    "            if not os.path.isfile(os.path.join(op, name)):\n",
    "                plt.savefig(os.path.join(op, f\"{name}_{suffix}.jpg\"))\n",
    "            else:\n",
    "                print(\"Name already exists.\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # Write methods\n",
    "    def save_predict(self, test_x, test_y):\n",
    "        \"\"\"Saves a summary of results based on simulated testing data.\"\"\"\n",
    "        folder = \"runs\"\n",
    "        op = os.path.join(os.getcwd(), folder)\n",
    "\n",
    "        if not os.path.exists(op):\n",
    "            os.makedirs(folder)\n",
    "\n",
    "        # Predicted\n",
    "        with open(os.path.join(op, \"predict.txt\"), \"a\") as pred:\n",
    "            pred.write(\"Date and time: {}, {}\\n\".format(self.date, self.time))\n",
    "            pred.write(\"Optimizer: {}\\n\".format(type(self).__name__))\n",
    "            pred.write(\"NN structure: {}\\n\".format(self.layers))\n",
    "            pred.write(\"lrate={}, \".format(self.lrate))\n",
    "            pred.write(\"momentum={}, \".format(self.momentum))\n",
    "            rmse = self.compute_error(test_x, test_y)\n",
    "            pred.write(\"error: {}\\n\\n\".format(rmse))\n",
    "\n",
    "    def save_summary(self, filename, layers: list):\n",
    "        \"\"\"Exports the training summary.\"\"\"\n",
    "        folder = \"runs\"\n",
    "        op = os.path.join(os.getcwd(), folder)\n",
    "\n",
    "        if not os.path.exists(op):\n",
    "            os.makedirs(folder)\n",
    "\n",
    "        # Compute weight of each factor\n",
    "        m_ave = []\n",
    "        m_max = []\n",
    "        weights = self.W1\n",
    "\n",
    "        for w in weights:\n",
    "            m_ave.append(np.mean(w))\n",
    "            m_max.append(np.amax(w))\n",
    "\n",
    "        # Map weights to factors\n",
    "        f = layers\n",
    "        d_ave = dict(zip(f, m_ave[:-1]))    # exclude bias, average weight\n",
    "        d_max = dict(zip(f, m_max[:-1]))    # exclude bias, max weight\n",
    "\n",
    "        # Save weights if error in testing data is reasonable\n",
    "        with open(os.path.join(op, filename), \"a\") as data:\n",
    "            data.write(\"Date and time: {}, {}\\n\".format(self.date, self.time))\n",
    "            data.write(\"NN structure: {}\\n\".format(self.layers))\n",
    "            data.write(\"Hyperparameters: \")\n",
    "            data.write(\"lrate={}, \".format(self.lrate))\n",
    "            data.write(\"mu={}, \".format(self.momentum))\n",
    "\n",
    "            try:\n",
    "                # For MGD\n",
    "                data.write(\"batch size={}\".format(self.batch_size))\n",
    "                data.write(\"\\nCycle: {} for 1 epoch (max of {}), \".format(self.total_batch, epoch))\n",
    "                data.write(\"terminated at epoch={}, iteration={}\".format(self.term_epoch, self.term_iter))\n",
    "\n",
    "            except AttributeError:\n",
    "                # For SGD and BGD\n",
    "                data.write(\"\\nEpoch={}, terminated at:{}\".format(self.epoch, self.term_epoch))\n",
    "\n",
    "            data.write(\"\\nSet and actual errors: \")\n",
    "            data.write(\"RMSE: {}, \".format(self.rmse))\n",
    "            data.write(\"val error: {}\\n\".format(np.amin(self.loss_val)))\n",
    "            data.write(\"Input to hidden weights:\\n {}\\n\".format(self.W1))\n",
    "            data.write(\"Hidden to output weights:\\n {}\\n\".format(self.W2))\n",
    "            data.write(\"Weight of each layer (ave):\\n {}\\n\".format(d_ave))\n",
    "            data.write(\"Weight of each layer (max):\\n {}\\n\\n\".format(d_max))\n",
    "\n",
    "        # Check if the file exists\n",
    "        assert os.path.isfile(os.path.join(op, filename)), \"The file was not created.\"\n",
    "        print(\"\\nSummary of training was exported.\")\n",
    "\n",
    "    def export_to_image(self, ref_path, out_path, array):\n",
    "        \"\"\"Exports array to image (default GeoTiff).\"\"\"\n",
    "        print(\"\\nExporting array to image...\")\n",
    "\n",
    "        # Reference data\n",
    "        data0 = gdal.Open(ref_path)\n",
    "\n",
    "        if data0 is None:\n",
    "            print(\"No reference data.\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        # get rows and columns of array\n",
    "        row = array.shape[0]  # number of pixels in y\n",
    "        col = array.shape[1]  # number of pixels in x\n",
    "\n",
    "        driver = gdal.GetDriverByName('GTiff')\n",
    "        dataset = driver.Create(out_path, col, row, 1, gdal.GDT_Float32)\n",
    "\n",
    "        if dataset is None:\n",
    "            print(\"Could not create lsi.tif\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        dataset.GetRasterBand(1).WriteArray(lsi)\n",
    "        dataset.GetRasterBand(1).SetNoDataValue(-9999)\n",
    "\n",
    "        # Add GeoTranform and Projection\n",
    "        geotrans = data0.GetGeoTransform()  # get GeoTranform from ref 'data0'\n",
    "        proj = data0.GetProjection()  # get GetProjection from ref 'data0'\n",
    "        dataset.SetGeoTransform(geotrans)\n",
    "        dataset.SetProjection(proj)\n",
    "        dataset.FlushCache()\n",
    "        dataset = None\n",
    "\n",
    "        # Check if exported correctly\n",
    "        if os.path.isfile(out_path):\n",
    "            print(\"Landslide susceptility index was exported.\")\n",
    "\n",
    "    def set_weights(self):\n",
    "        \"\"\"Set the weights for final forward pass (excluding bias weights).\"\"\"\n",
    "        self.W1 = self.W1[0:-1]  # exclude the bias for prediction\n",
    "\n",
    "# end of Neural_Network class\n",
    "\n",
    "\n",
    "# Class optimizers\n",
    "class BGD(Neural_Network):\n",
    "    \"\"\"Batch Gradient Descent.\"\"\"\n",
    "\n",
    "    def train_BGD(self, *args):\n",
    "        \"\"\"Performs simulation using batch gradien descent.\"\"\"\n",
    "\n",
    "        print(f\"\\nStarting BGD Training with NN structure:{self.layers}, lrate={self.lrate}, mu={self.momentum}:\")\n",
    "        training = args[0][0]\n",
    "        validation = args[0][1]\n",
    "\n",
    "        self.rmse = args[1]\n",
    "        self.epoch = args[2]\n",
    "\n",
    "        # X and y training sets\n",
    "        X_train_1o = training[..., :10]\n",
    "        y_train_1o = training[..., [-1]]\n",
    "\n",
    "        # X and y validation sets\n",
    "        X_val_1o = validation[..., :10]\n",
    "        y_val_1o = validation[..., [-1]]\n",
    "\n",
    "        self.loss_train = []\n",
    "        self.loss_val = []\n",
    "\n",
    "        for i in range(epoch):\n",
    "            # Shuffle training set every epoch\n",
    "#             np.random.shuffle(training)\n",
    "\n",
    "            # Training loss\n",
    "            rmse_train = self.train(X_train_1o, y_train_1o)\n",
    "\n",
    "            # Validation loss\n",
    "            rmse_val = self.compute_error(X_val_1o, y_val_1o)\n",
    "\n",
    "            # Store errors\n",
    "            self.loss_val.append(rmse_val)\n",
    "            self.loss_train.append(rmse_train)\n",
    "\n",
    "            # Print result per epoch\n",
    "            print(\"epoch: {}\".format(i))\n",
    "            print(\"Training Loss: {}\".format(rmse_train))\n",
    "            print(\"Validation Loss: {}\".format(rmse_val))\n",
    "            print()\n",
    "\n",
    "            # Threshold error\n",
    "            if rmse_val <= self.rmse:\n",
    "                break\n",
    "\n",
    "        # Termination information\n",
    "        self.term_epoch = i\n",
    "\n",
    "        # Training statistics\n",
    "        min_loss_train = np.amin(self.loss_train)\n",
    "        max_loss_train = np.amax(self.loss_train)\n",
    "        mean_loss_train = np.mean(self.loss_train)\n",
    "        min_val_train = np.amin(self.loss_val)\n",
    "        max_val_train = np.amax(self.loss_val)\n",
    "        mean_val_train = np.mean(self.loss_val)\n",
    "\n",
    "        print(\"\\nTraining statistics:\")\n",
    "        print(\"Minimum train error:\", min_loss_train)\n",
    "        print(\"Maximum train error:\", max_loss_train)\n",
    "        print(\"Mean train error:\", mean_loss_train)\n",
    "        print()\n",
    "        print(\"Minimum val error:\", min_val_train)\n",
    "        print(\"Maximum val error:\", max_val_train)\n",
    "        print(\"Mean val error:\", mean_val_train)\n",
    "\n",
    "\n",
    "class SGD(Neural_Network):\n",
    "    \"\"\"Stochastic Gradient Descent.\"\"\"\n",
    "\n",
    "    def train_SGD(self, *args):\n",
    "        \"\"\"Performs simulation using stochastic gradien descent.\"\"\"\n",
    "        print(f\"\\nStarting SGD Training with NN structure:{self.layers}, lrate={self.lrate}, mu={self.momentum}:\")\n",
    "        training = args[0][0]\n",
    "        validation = args[0][1]\n",
    "\n",
    "        self.rmse = args[1]\n",
    "        self.epoch = args[2]\n",
    "\n",
    "        # X and y training sets\n",
    "        X_train_1o = training[..., :10]\n",
    "        y_train_1o = training[..., [-1]]\n",
    "\n",
    "        # X and y validation sets\n",
    "        X_val_1o = validation[..., :10]\n",
    "        y_val_1o = validation[..., [-1]]\n",
    "\n",
    "        self.loss_train = []\n",
    "        self.loss_val = []\n",
    "\n",
    "        for i in range(epoch):\n",
    "            # Stochastic sampling\n",
    "            n = np.random.choice(X_train_1o.shape[0])\n",
    "\n",
    "            # Training loss\n",
    "            rmse_train = self.train(X_train_1o[n], y_train_1o[n])\n",
    "\n",
    "            # Validation loss\n",
    "            rmse_val = self.compute_error(X_val_1o, y_val_1o)\n",
    "\n",
    "            # Store errors\n",
    "            self.loss_val.append(rmse_val)\n",
    "            self.loss_train.append(rmse_train)\n",
    "\n",
    "            # Print result per epoch\n",
    "            print(\"epoch: {}\".format(i))\n",
    "            print(\"Training Loss: {}\".format(rmse_train))\n",
    "            print(\"Validation Loss: {}\".format(rmse_val))\n",
    "            print()\n",
    "\n",
    "            # Threshold error\n",
    "            if rmse_val <= self.rmse:\n",
    "                break\n",
    "\n",
    "        # Termination information\n",
    "        self.term_epoch = i\n",
    "\n",
    "        # Training statistics\n",
    "        min_loss_train = np.amin(self.loss_train)\n",
    "        max_loss_train = np.amax(self.loss_train)\n",
    "        mean_loss_train = np.mean(self.loss_train)\n",
    "        min_val_train = np.amin(self.loss_val)\n",
    "        max_val_train = np.amax(self.loss_val)\n",
    "        mean_val_train = np.mean(self.loss_val)\n",
    "\n",
    "        print(\"\\nTraining statistics:\")\n",
    "        print(\"Minimum train error:\", min_loss_train)\n",
    "        print(\"Maximum train error:\", max_loss_train)\n",
    "        print(\"Mean train error:\", mean_loss_train)\n",
    "        print()\n",
    "        print(\"Minimum val error:\", min_val_train)\n",
    "        print(\"Maximum val error:\", max_val_train)\n",
    "        print(\"Mean val error:\", mean_val_train)\n",
    "\n",
    "\n",
    "class MGD(Neural_Network):\n",
    "    \"\"\"Mini-batch Gradient Descent.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        Neural_Network.__init__(self, *args[:-1], **kwargs)\n",
    "        self.batch_size = args[-1]\n",
    "\n",
    "    def train_MGD(self, *args):\n",
    "        \"\"\"Performs simulation using mini-batch gradien descent.\"\"\"\n",
    "        print(f\"\\nStarting MGD Training with NN structure:{self.layers}, lrate={self.lrate}, mu={self.momentum}:\")\n",
    "\n",
    "        training = args[0][0]\n",
    "        validation = args[0][1]\n",
    "\n",
    "        self.rmse = args[1]\n",
    "        self.epoch = args[2]\n",
    "\n",
    "        # X and y training sets\n",
    "        X_train_1o = training[..., :10]\n",
    "        y_train_1o = training[..., [-1]]\n",
    "\n",
    "        # X and y validation sets\n",
    "        X_val_1o = validation[..., :10]\n",
    "        y_val_1o = validation[..., [-1]]\n",
    "\n",
    "        # Initialize number of iterations\n",
    "        self.total_batch = X_train_1o.shape[0] // (self.batch_size - 1)  # exlude the last index\n",
    "        val_batch = X_val_1o.shape[0] // (self.batch_size - 1)\n",
    "\n",
    "        self.loss_train = []\n",
    "        self.loss_val = []\n",
    "        for i in range(epoch):\n",
    "\n",
    "            # Shuffle datasets each epoch (after completely seeing the entire dataset)\n",
    "#             np.random.shuffle(training)\n",
    "\n",
    "#             # Reassign X and y training vectors\n",
    "#             X_train_1o = training[..., :10]\n",
    "#             y_train_1o = training[..., [-1]]\n",
    "\n",
    "            X_batches = np.array_split(X_train_1o, self.total_batch)\n",
    "            y_batches = np.array_split(y_train_1o, self.total_batch)\n",
    "\n",
    "            # Initialize rmse\n",
    "            # Will be reset after completing 1 epoch\n",
    "            ave_cost_train = 0\n",
    "\n",
    "            # Train the neural network per mini-batch\n",
    "            for j in range(self.total_batch):\n",
    "                minibatch_x = X_batches[j]\n",
    "                minibatch_y = y_batches[j]\n",
    "\n",
    "                # Train per mini-batch\n",
    "                rmse_train = self.train(minibatch_x, minibatch_y)\n",
    "\n",
    "                # Get average training cost for each epoch\n",
    "                ave_cost_train += rmse_train / self.total_batch\n",
    "\n",
    "            self.loss_train.append(ave_cost_train)\n",
    "\n",
    "#             # Validation using the whole validation set\n",
    "#             rmse_val = self.compute_error(X_val_1o, y_val_1o)\n",
    "#             self.loss_val.append(rmse_val)\n",
    "\n",
    "#             # Print result per epoch\n",
    "#             print(\"epoch: {}\".format(i))\n",
    "#             print(\"Training loss: {}\".format(ave_cost_train))\n",
    "#             print(\"Validation loss: {}\".format(rmse_val))\n",
    "#             print()\n",
    "\n",
    "#             if rmse_val <= RMSE:\n",
    "#                 break\n",
    "\n",
    "            # Per mini-batch validation\n",
    "            ave_cost_val = 0\n",
    "        \n",
    "            x_val = np.array_split(X_val_1o, val_batch)\n",
    "            y_val = np.array_split(y_val_1o, val_batch)\n",
    "            \n",
    "            for v in range(val_batch):\n",
    "                mb_val_x = x_val[v]\n",
    "                mb_val_y = y_val[v]\n",
    "\n",
    "                # Validation loss\n",
    "                rmse_val = self.compute_error(mb_val_x, mb_val_y)\n",
    "\n",
    "                # Get average val cost for each epoch\n",
    "                ave_cost_val += rmse_val / val_batch\n",
    "\n",
    "            self.loss_val.append(ave_cost_val)\n",
    "\n",
    "            # Print result per epoch\n",
    "            print(\"epoch: {}\".format(i))\n",
    "            print(\"Training loss: {}\".format(ave_cost_train))\n",
    "            print(\"Validation loss: {}\".format(ave_cost_val))\n",
    "            print()\n",
    "\n",
    "            # Stoping criterion using validation set\n",
    "            if ave_cost_val <= self.rmse:\n",
    "                print(\"Training done.\")\n",
    "                break\n",
    "\n",
    "        # Termination information\n",
    "        self.term_epoch = i\n",
    "        self.term_iter = j\n",
    "\n",
    "        # Training statistics\n",
    "        min_loss_train = np.amin(self.loss_train)\n",
    "        max_loss_train = np.amax(self.loss_train)\n",
    "        mean_loss_train = np.mean(self.loss_train)\n",
    "        min_val_train = np.amin(self.loss_val)\n",
    "        max_val_train = np.amax(self.loss_val)\n",
    "        mean_val_train = np.mean(self.loss_val)\n",
    "\n",
    "        print(\"\\nTraining statistics:\")\n",
    "        print(\"Minimum train error:\", min_loss_train)\n",
    "        print(\"Maximum train error:\", max_loss_train)\n",
    "        print(\"Mean train error:\", mean_loss_train)\n",
    "        print()\n",
    "        print(\"Minimum val error:\", min_val_train)\n",
    "        print(\"Maximum val error:\", max_val_train)\n",
    "        print(\"Mean val error:\", mean_val_train)\n",
    "\n",
    "# end of class optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading layers for the creation of input vector...\n",
      "\n",
      "Shape of converted layers to numpy array:\n",
      "prone factor: (6334, 3877), not prone factor: (6334, 3877)\n",
      "\n",
      "Converting arrays to 1D arrays...\n",
      "Shape of 1D arrays:\n",
      "prone factor: (24556918,), dtype: float32 | not prone factor: (24556918,), dtype: float32\n",
      "\n",
      "Applying boolean filtering to 1D arrays...\n",
      "Shape of filtered 1D arrays:\n",
      "prone factor: (18862,), dtype: float32 | not prone factor: (18862,), dtype: float32\n",
      "\n",
      "Scaling all input vectors using min-max scaling...\n",
      "All vectors are scaled in the interval [0, 1].\n",
      "\n",
      "Creating X vector with bias...\n",
      "Landslide prone input vector: (18862, 10) and data type: float32 \n",
      "Not prone to landslide input vector: (18862, 10) and data type float32\n",
      "\n",
      "Splitting samples to training, validation, and testing sets...\n",
      "Finalizing samples...\n",
      "Datasets created with shape: train=(24142, 10), val=(6036, 10), test=(7546, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Start time\n",
    "    start_time = time.process_time()\n",
    "\n",
    "    # Filepath for landslide prone and nonprone samples\n",
    "    fp_prone = r\"D:\\ms gme\\thesis\\final parameters\\samples\\Final\\landslide\\fixed_nd\"\n",
    "    fp_notprone = r\"D:\\MS Gme\\Thesis\\Final Parameters\\Samples\\Final\\no_ls_5deg_slope\\fixed_nd\"\n",
    "\n",
    "    my_thesis = Dataset(fp_prone, fp_notprone)\n",
    "\n",
    "    # Load landslide factors\n",
    "    datasets = my_thesis.load_layers()\n",
    "    train_val_sets = datasets[0:2]\n",
    "    test_set = datasets[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting MGD Training with NN structure:(9, 16, 1), lrate=0.1, mu=0.1:\n",
      "epoch: 0\n",
      "Training loss: 0.14017612466465554\n",
      "Validation loss: 0.044964833146654185\n",
      "\n",
      "epoch: 1\n",
      "Training loss: 0.041923918926930905\n",
      "Validation loss: 0.038921267666328224\n",
      "\n",
      "epoch: 2\n",
      "Training loss: 0.03850829881678967\n",
      "Validation loss: 0.03274902280935166\n",
      "\n",
      "epoch: 3\n",
      "Training loss: 0.03299678313270694\n",
      "Validation loss: 0.025800208388807536\n",
      "\n",
      "epoch: 4\n",
      "Training loss: 0.02757109646111293\n",
      "Validation loss: 0.022313719902277235\n",
      "\n",
      "epoch: 5\n",
      "Training loss: 0.024650848298449515\n",
      "Validation loss: 0.020390225979348045\n",
      "\n",
      "epoch: 6\n",
      "Training loss: 0.023099900807658794\n",
      "Validation loss: 0.019297184475784667\n",
      "\n",
      "epoch: 7\n",
      "Training loss: 0.022203656207023557\n",
      "Validation loss: 0.0186171857812648\n",
      "\n",
      "epoch: 8\n",
      "Training loss: 0.021570635606182942\n",
      "Validation loss: 0.018085815298698318\n",
      "\n",
      "epoch: 9\n",
      "Training loss: 0.02113425953268555\n",
      "Validation loss: 0.017858797269255763\n",
      "\n",
      "epoch: 10\n",
      "Training loss: 0.02088735417215866\n",
      "Validation loss: 0.017712009705703472\n",
      "\n",
      "epoch: 11\n",
      "Training loss: 0.020580062077929027\n",
      "Validation loss: 0.01742508786642672\n",
      "\n",
      "epoch: 12\n",
      "Training loss: 0.02026943824671489\n",
      "Validation loss: 0.01723034482751182\n",
      "\n",
      "epoch: 13\n",
      "Training loss: 0.01999024693759379\n",
      "Validation loss: 0.0171052962366479\n",
      "\n",
      "epoch: 14\n",
      "Training loss: 0.019810509824384877\n",
      "Validation loss: 0.017058415985669293\n",
      "\n",
      "epoch: 15\n",
      "Training loss: 0.01973031720372912\n",
      "Validation loss: 0.016907445952950958\n",
      "\n",
      "epoch: 16\n",
      "Training loss: 0.019691831148092132\n",
      "Validation loss: 0.01712424335254991\n",
      "\n",
      "epoch: 17\n",
      "Training loss: 0.01967152925211013\n",
      "Validation loss: 0.017194873928777533\n",
      "\n",
      "epoch: 18\n",
      "Training loss: 0.019666033854890297\n",
      "Validation loss: 0.017261663294714645\n",
      "\n",
      "epoch: 19\n",
      "Training loss: 0.019716814132696645\n",
      "Validation loss: 0.017332750142946326\n",
      "\n",
      "epoch: 20\n",
      "Training loss: 0.019667752320525998\n",
      "Validation loss: 0.017310599752477132\n",
      "\n",
      "epoch: 21\n",
      "Training loss: 0.019644271737392534\n",
      "Validation loss: 0.01737403330995991\n",
      "\n",
      "epoch: 22\n",
      "Training loss: 0.019612634379269092\n",
      "Validation loss: 0.01734047244239037\n",
      "\n",
      "epoch: 23\n",
      "Training loss: 0.019587633852448157\n",
      "Validation loss: 0.017383972521048462\n",
      "\n",
      "epoch: 24\n",
      "Training loss: 0.01956973995175387\n",
      "Validation loss: 0.01737532690331943\n",
      "\n",
      "epoch: 25\n",
      "Training loss: 0.01957119946377287\n",
      "Validation loss: 0.01736354097590155\n",
      "\n",
      "epoch: 26\n",
      "Training loss: 0.019529940926309716\n",
      "Validation loss: 0.0173860699968039\n",
      "\n",
      "epoch: 27\n",
      "Training loss: 0.0195303805551498\n",
      "Validation loss: 0.017349882578043454\n",
      "\n",
      "epoch: 28\n",
      "Training loss: 0.019455226407059163\n",
      "Validation loss: 0.017393463888560376\n",
      "\n",
      "epoch: 29\n",
      "Training loss: 0.019486758222798113\n",
      "Validation loss: 0.017336462361114992\n",
      "\n",
      "epoch: 30\n",
      "Training loss: 0.019479201475820468\n",
      "Validation loss: 0.01734265902870503\n",
      "\n",
      "epoch: 31\n",
      "Training loss: 0.019424258986055196\n",
      "Validation loss: 0.017337606864545316\n",
      "\n",
      "epoch: 32\n",
      "Training loss: 0.0194345092703966\n",
      "Validation loss: 0.017363473503031007\n",
      "\n",
      "epoch: 33\n",
      "Training loss: 0.019434114144297285\n",
      "Validation loss: 0.017408483418785457\n",
      "\n",
      "epoch: 34\n",
      "Training loss: 0.019451114721572793\n",
      "Validation loss: 0.017381429562151343\n",
      "\n",
      "epoch: 35\n",
      "Training loss: 0.019442268097146647\n",
      "Validation loss: 0.017427962589174336\n",
      "\n",
      "epoch: 36\n",
      "Training loss: 0.01947127472221765\n",
      "Validation loss: 0.017434458955687283\n",
      "\n",
      "epoch: 37\n",
      "Training loss: 0.019491952234731758\n",
      "Validation loss: 0.0173485545726886\n",
      "\n",
      "epoch: 38\n",
      "Training loss: 0.01948268989621284\n",
      "Validation loss: 0.017600024548290807\n",
      "\n",
      "epoch: 39\n",
      "Training loss: 0.01952134046959801\n",
      "Validation loss: 0.017630594364266866\n",
      "\n",
      "epoch: 40\n",
      "Training loss: 0.01956250128551843\n",
      "Validation loss: 0.017581404032521813\n",
      "\n",
      "epoch: 41\n",
      "Training loss: 0.019534406832081533\n",
      "Validation loss: 0.017751753202501625\n",
      "\n",
      "epoch: 42\n",
      "Training loss: 0.01960022463536022\n",
      "Validation loss: 0.017618774692081323\n",
      "\n",
      "epoch: 43\n",
      "Training loss: 0.019615921123226906\n",
      "Validation loss: 0.017596342063473342\n",
      "\n",
      "epoch: 44\n",
      "Training loss: 0.019594271669020127\n",
      "Validation loss: 0.01774368303905864\n",
      "\n",
      "epoch: 45\n",
      "Training loss: 0.019624365670044434\n",
      "Validation loss: 0.01764719723407977\n",
      "\n",
      "epoch: 46\n",
      "Training loss: 0.01962267089917665\n",
      "Validation loss: 0.017610989676807635\n",
      "\n",
      "epoch: 47\n",
      "Training loss: 0.019626523549598332\n",
      "Validation loss: 0.017788116733149822\n",
      "\n",
      "epoch: 48\n",
      "Training loss: 0.019624177597856694\n",
      "Validation loss: 0.01791138656022271\n",
      "\n",
      "epoch: 49\n",
      "Training loss: 0.01959137752159562\n",
      "Validation loss: 0.01804687626832829\n",
      "\n",
      "epoch: 50\n",
      "Training loss: 0.019609091467835123\n",
      "Validation loss: 0.017874971481583155\n",
      "\n",
      "epoch: 51\n",
      "Training loss: 0.01959826793928605\n",
      "Validation loss: 0.017961579761859706\n",
      "\n",
      "epoch: 52\n",
      "Training loss: 0.019544545759697395\n",
      "Validation loss: 0.01777675355624827\n",
      "\n",
      "epoch: 53\n",
      "Training loss: 0.019556956378917346\n",
      "Validation loss: 0.017887810608549735\n",
      "\n",
      "epoch: 54\n",
      "Training loss: 0.01948022751605418\n",
      "Validation loss: 0.017906230575618635\n",
      "\n",
      "epoch: 55\n",
      "Training loss: 0.019458836110034072\n",
      "Validation loss: 0.01804162487512586\n",
      "\n",
      "epoch: 56\n",
      "Training loss: 0.01945197186783417\n",
      "Validation loss: 0.018096602144173926\n",
      "\n",
      "epoch: 57\n",
      "Training loss: 0.019439298984799236\n",
      "Validation loss: 0.017968272533279865\n",
      "\n",
      "epoch: 58\n",
      "Training loss: 0.019399183087916553\n",
      "Validation loss: 0.018018347425962693\n",
      "\n",
      "epoch: 59\n",
      "Training loss: 0.01936931093410906\n",
      "Validation loss: 0.018250372432936417\n",
      "\n",
      "epoch: 60\n",
      "Training loss: 0.019360931990219783\n",
      "Validation loss: 0.018151349027057614\n",
      "\n",
      "epoch: 61\n",
      "Training loss: 0.019300152441329956\n",
      "Validation loss: 0.018289853349293922\n",
      "\n",
      "epoch: 62\n",
      "Training loss: 0.019314772996029298\n",
      "Validation loss: 0.018373305944796452\n",
      "\n",
      "epoch: 63\n",
      "Training loss: 0.019287592096632576\n",
      "Validation loss: 0.018286224907189206\n",
      "\n",
      "epoch: 64\n",
      "Training loss: 0.019240850011708735\n",
      "Validation loss: 0.018493678383584865\n",
      "\n",
      "epoch: 65\n",
      "Training loss: 0.019220244585339302\n",
      "Validation loss: 0.01862977435162069\n",
      "\n",
      "epoch: 66\n",
      "Training loss: 0.01919129708730447\n",
      "Validation loss: 0.018899243831891916\n",
      "\n",
      "epoch: 67\n",
      "Training loss: 0.019179830688716516\n",
      "Validation loss: 0.019033091021954964\n",
      "\n",
      "epoch: 68\n",
      "Training loss: 0.019132698078023078\n",
      "Validation loss: 0.019285411456266194\n",
      "\n",
      "epoch: 69\n",
      "Training loss: 0.01914933096179424\n",
      "Validation loss: 0.019340147751871126\n",
      "\n",
      "epoch: 70\n",
      "Training loss: 0.019086432759269908\n",
      "Validation loss: 0.019456199907351413\n",
      "\n",
      "epoch: 71\n",
      "Training loss: 0.01904042167388216\n",
      "Validation loss: 0.019574673053409427\n",
      "\n",
      "epoch: 72\n",
      "Training loss: 0.019000529677348516\n",
      "Validation loss: 0.01969908825471414\n",
      "\n",
      "epoch: 73\n",
      "Training loss: 0.01893175973513422\n",
      "Validation loss: 0.016794038003626915\n",
      "\n",
      "epoch: 74\n",
      "Training loss: 0.01886652380671367\n",
      "Validation loss: 0.016741494274174182\n",
      "\n",
      "epoch: 75\n",
      "Training loss: 0.01880007203280891\n",
      "Validation loss: 0.016709110851627275\n",
      "\n",
      "epoch: 76\n",
      "Training loss: 0.018715845896714534\n",
      "Validation loss: 0.01654896014707552\n",
      "\n",
      "epoch: 77\n",
      "Training loss: 0.018619762506032988\n",
      "Validation loss: 0.016454733286370128\n",
      "\n",
      "epoch: 78\n",
      "Training loss: 0.01853129882379141\n",
      "Validation loss: 0.016360162401830552\n",
      "\n",
      "epoch: 79\n",
      "Training loss: 0.01835423181733773\n",
      "Validation loss: 0.01612367222243562\n",
      "\n",
      "epoch: 80\n",
      "Training loss: 0.018278852633846116\n",
      "Validation loss: 0.01603118146401391\n",
      "\n",
      "epoch: 81\n",
      "Training loss: 0.018132157249709246\n",
      "Validation loss: 0.015933759258876714\n",
      "\n",
      "epoch: 82\n",
      "Training loss: 0.017997006579245575\n",
      "Validation loss: 0.015868197092116263\n",
      "\n",
      "epoch: 83\n",
      "Training loss: 0.017898327327407863\n",
      "Validation loss: 0.01581659342652007\n",
      "\n",
      "epoch: 84\n",
      "Training loss: 0.017799330791558075\n",
      "Validation loss: 0.015726986436349247\n",
      "\n",
      "epoch: 85\n",
      "Training loss: 0.01771929416948644\n",
      "Validation loss: 0.01565682720550728\n",
      "\n",
      "epoch: 86\n",
      "Training loss: 0.017630892879221805\n",
      "Validation loss: 0.015582050492289312\n",
      "\n",
      "epoch: 87\n",
      "Training loss: 0.017538069872827976\n",
      "Validation loss: 0.015561674471256306\n",
      "\n",
      "epoch: 88\n",
      "Training loss: 0.017478237227458327\n",
      "Validation loss: 0.015538363744688439\n",
      "\n",
      "epoch: 89\n",
      "Training loss: 0.017390534876228204\n",
      "Validation loss: 0.01549495825007934\n",
      "\n",
      "epoch: 90\n",
      "Training loss: 0.017337795840891828\n",
      "Validation loss: 0.015447879189169539\n",
      "\n",
      "epoch: 91\n",
      "Training loss: 0.017287736904240562\n",
      "Validation loss: 0.015410592521219672\n",
      "\n",
      "epoch: 92\n",
      "Training loss: 0.01723306704139249\n",
      "Validation loss: 0.015414478313946042\n",
      "\n",
      "epoch: 93\n",
      "Training loss: 0.01720856203721238\n",
      "Validation loss: 0.015354596988527364\n",
      "\n",
      "epoch: 94\n",
      "Training loss: 0.017166280706862113\n",
      "Validation loss: 0.015322262927953849\n",
      "\n",
      "epoch: 95\n",
      "Training loss: 0.01713439799612036\n",
      "Validation loss: 0.015315333477894422\n",
      "\n",
      "epoch: 96\n",
      "Training loss: 0.017087390739982566\n",
      "Validation loss: 0.015299686398665779\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 97\n",
      "Training loss: 0.017054035365652778\n",
      "Validation loss: 0.015277931296847744\n",
      "\n",
      "epoch: 98\n",
      "Training loss: 0.017028729695839774\n",
      "Validation loss: 0.015266961279018804\n",
      "\n",
      "epoch: 99\n",
      "Training loss: 0.017000454358447707\n",
      "Validation loss: 0.015218634282223349\n",
      "\n",
      "epoch: 100\n",
      "Training loss: 0.016967149185339484\n",
      "Validation loss: 0.015207472592451407\n",
      "\n",
      "epoch: 101\n",
      "Training loss: 0.016945483991678657\n",
      "Validation loss: 0.015164704417664219\n",
      "\n",
      "epoch: 102\n",
      "Training loss: 0.016926272496120876\n",
      "Validation loss: 0.015138811992771494\n",
      "\n",
      "epoch: 103\n",
      "Training loss: 0.016902104129350493\n",
      "Validation loss: 0.015169179995909958\n",
      "\n",
      "epoch: 104\n",
      "Training loss: 0.016905526407200448\n",
      "Validation loss: 0.015131932167837089\n",
      "\n",
      "epoch: 105\n",
      "Training loss: 0.016869372310872724\n",
      "Validation loss: 0.015153501580351189\n",
      "\n",
      "epoch: 106\n",
      "Training loss: 0.01685009491921751\n",
      "Validation loss: 0.015171404278708014\n",
      "\n",
      "epoch: 107\n",
      "Training loss: 0.016826776331347673\n",
      "Validation loss: 0.015106989324511391\n",
      "\n",
      "epoch: 108\n",
      "Training loss: 0.016795626439623436\n",
      "Validation loss: 0.015155562977288359\n",
      "\n",
      "epoch: 109\n",
      "Training loss: 0.01678206311899269\n",
      "Validation loss: 0.015088058728534314\n",
      "\n",
      "epoch: 110\n",
      "Training loss: 0.016742963290648696\n",
      "Validation loss: 0.015091350771885703\n",
      "\n",
      "epoch: 111\n",
      "Training loss: 0.016723513741189286\n",
      "Validation loss: 0.015078638241570175\n",
      "\n",
      "epoch: 112\n",
      "Training loss: 0.01669511690038263\n",
      "Validation loss: 0.015066167142890049\n",
      "\n",
      "epoch: 113\n",
      "Training loss: 0.016675210757456406\n",
      "Validation loss: 0.014954140677303633\n",
      "\n",
      "epoch: 114\n",
      "Training loss: 0.016660599191968658\n",
      "Validation loss: 0.014990287119268869\n",
      "\n",
      "epoch: 115\n",
      "Training loss: 0.01665025734375568\n",
      "Validation loss: 0.014926152487196179\n",
      "\n",
      "epoch: 116\n",
      "Training loss: 0.01661613564293286\n",
      "Validation loss: 0.01492441689432113\n",
      "\n",
      "epoch: 117\n",
      "Training loss: 0.016595776276420116\n",
      "Validation loss: 0.014890634624474758\n",
      "\n",
      "epoch: 118\n",
      "Training loss: 0.01658352300746622\n",
      "Validation loss: 0.014866419027962017\n",
      "\n",
      "epoch: 119\n",
      "Training loss: 0.016568433047428978\n",
      "Validation loss: 0.014854648998835731\n",
      "\n",
      "epoch: 120\n",
      "Training loss: 0.016529009872947115\n",
      "Validation loss: 0.014930995914223215\n",
      "\n",
      "epoch: 121\n",
      "Training loss: 0.016531496295049235\n",
      "Validation loss: 0.014981270471245528\n",
      "\n",
      "epoch: 122\n",
      "Training loss: 0.016519558906177772\n",
      "Validation loss: 0.014879095187844733\n",
      "\n",
      "epoch: 123\n",
      "Training loss: 0.016495329360768977\n",
      "Validation loss: 0.014848802728314195\n",
      "\n",
      "epoch: 124\n",
      "Training loss: 0.01645997602502512\n",
      "Validation loss: 0.014829191484201405\n",
      "\n",
      "epoch: 125\n",
      "Training loss: 0.01646712997309438\n",
      "Validation loss: 0.014814335147232138\n",
      "\n",
      "epoch: 126\n",
      "Training loss: 0.016433802469524608\n",
      "Validation loss: 0.014809872213675827\n",
      "\n",
      "epoch: 127\n",
      "Training loss: 0.016424881581229282\n",
      "Validation loss: 0.014903823193814733\n",
      "\n",
      "epoch: 128\n",
      "Training loss: 0.01641190453595871\n",
      "Validation loss: 0.014844968090941353\n",
      "\n",
      "epoch: 129\n",
      "Training loss: 0.0163850613896238\n",
      "Validation loss: 0.0148430207893656\n",
      "\n",
      "epoch: 130\n",
      "Training loss: 0.016375105645664117\n",
      "Validation loss: 0.01479009426741392\n",
      "\n",
      "epoch: 131\n",
      "Training loss: 0.016370847518214368\n",
      "Validation loss: 0.014867408926214324\n",
      "\n",
      "epoch: 132\n",
      "Training loss: 0.016345361243271937\n",
      "Validation loss: 0.014822814778793708\n",
      "\n",
      "epoch: 133\n",
      "Training loss: 0.016321535919230003\n",
      "Validation loss: 0.01479704729425069\n",
      "\n",
      "epoch: 134\n",
      "Training loss: 0.016317662799819426\n",
      "Validation loss: 0.014704107595307636\n",
      "\n",
      "epoch: 135\n",
      "Training loss: 0.016281434368965904\n",
      "Validation loss: 0.014723977050354987\n",
      "\n",
      "epoch: 136\n",
      "Training loss: 0.016292816171841813\n",
      "Validation loss: 0.014811562566728833\n",
      "\n",
      "epoch: 137\n",
      "Training loss: 0.0162729468804213\n",
      "Validation loss: 0.014814352936010376\n",
      "\n",
      "epoch: 138\n",
      "Training loss: 0.016270253134137554\n",
      "Validation loss: 0.014803709748477462\n",
      "\n",
      "epoch: 139\n",
      "Training loss: 0.016206223510286808\n",
      "Validation loss: 0.014800982620115482\n",
      "\n",
      "epoch: 140\n",
      "Training loss: 0.016236714830166574\n",
      "Validation loss: 0.01470941508015203\n",
      "\n",
      "epoch: 141\n",
      "Training loss: 0.01621960107327891\n",
      "Validation loss: 0.014704010865454033\n",
      "\n",
      "epoch: 142\n",
      "Training loss: 0.016205347695387697\n",
      "Validation loss: 0.014744181115078636\n",
      "\n",
      "epoch: 143\n",
      "Training loss: 0.016195452264179\n",
      "Validation loss: 0.014820102868376164\n",
      "\n",
      "epoch: 144\n",
      "Training loss: 0.016196412942807614\n",
      "Validation loss: 0.01483825149733125\n",
      "\n",
      "epoch: 145\n",
      "Training loss: 0.016198434887008745\n",
      "Validation loss: 0.014787149303674028\n",
      "\n",
      "epoch: 146\n",
      "Training loss: 0.016181101308073834\n",
      "Validation loss: 0.014833384137914631\n",
      "\n",
      "epoch: 147\n",
      "Training loss: 0.016180363290548255\n",
      "Validation loss: 0.014823514986541298\n",
      "\n",
      "epoch: 148\n",
      "Training loss: 0.016171451598315317\n",
      "Validation loss: 0.014864892871411515\n",
      "\n",
      "epoch: 149\n",
      "Training loss: 0.01619440821978405\n",
      "Validation loss: 0.014878792755409589\n",
      "\n",
      "epoch: 150\n",
      "Training loss: 0.016185284078378027\n",
      "Validation loss: 0.014851239101504264\n",
      "\n",
      "epoch: 151\n",
      "Training loss: 0.01617649910866448\n",
      "Validation loss: 0.014843142171368551\n",
      "\n",
      "epoch: 152\n",
      "Training loss: 0.01616132062149538\n",
      "Validation loss: 0.014841974922797705\n",
      "\n",
      "epoch: 153\n",
      "Training loss: 0.016172763706559894\n",
      "Validation loss: 0.014899273520832592\n",
      "\n",
      "epoch: 154\n",
      "Training loss: 0.01617067050712391\n",
      "Validation loss: 0.014911003593494041\n",
      "\n",
      "epoch: 155\n",
      "Training loss: 0.01618124488264266\n",
      "Validation loss: 0.014933430936165793\n",
      "\n",
      "epoch: 156\n",
      "Training loss: 0.016197530491893306\n",
      "Validation loss: 0.014890823430112526\n",
      "\n",
      "epoch: 157\n",
      "Training loss: 0.016184363206632746\n",
      "Validation loss: 0.015031048933458304\n",
      "\n",
      "epoch: 158\n",
      "Training loss: 0.01622867737921226\n",
      "Validation loss: 0.014989506227656631\n",
      "\n",
      "epoch: 159\n",
      "Training loss: 0.016214086192765646\n",
      "Validation loss: 0.01508722836718873\n",
      "\n",
      "epoch: 160\n",
      "Training loss: 0.016241857714732973\n",
      "Validation loss: 0.015081790591716255\n",
      "\n",
      "epoch: 161\n",
      "Training loss: 0.01624626272096421\n",
      "Validation loss: 0.015119223187886334\n",
      "\n",
      "epoch: 162\n",
      "Training loss: 0.016250776554159636\n",
      "Validation loss: 0.015145208336543248\n",
      "\n",
      "epoch: 163\n",
      "Training loss: 0.016267145856274062\n",
      "Validation loss: 0.015167784260636369\n",
      "\n",
      "epoch: 164\n",
      "Training loss: 0.01626126216979775\n",
      "Validation loss: 0.015177086810000339\n",
      "\n",
      "epoch: 165\n",
      "Training loss: 0.016261308514160515\n",
      "Validation loss: 0.015213387767923236\n",
      "\n",
      "epoch: 166\n",
      "Training loss: 0.016246783485227238\n",
      "Validation loss: 0.015159393661068995\n",
      "\n",
      "epoch: 167\n",
      "Training loss: 0.01623083666229487\n",
      "Validation loss: 0.015169700880852847\n",
      "\n",
      "epoch: 168\n",
      "Training loss: 0.016215445487928675\n",
      "Validation loss: 0.015124035448328675\n",
      "\n",
      "epoch: 169\n",
      "Training loss: 0.016200376563101353\n",
      "Validation loss: 0.015130788174860313\n",
      "\n",
      "epoch: 170\n",
      "Training loss: 0.01616991623708558\n",
      "Validation loss: 0.015046927087524127\n",
      "\n",
      "epoch: 171\n",
      "Training loss: 0.016164619781479893\n",
      "Validation loss: 0.01507575665269286\n",
      "\n",
      "epoch: 172\n",
      "Training loss: 0.01614604454092991\n",
      "Validation loss: 0.015076433411893334\n",
      "\n",
      "epoch: 173\n",
      "Training loss: 0.016138788926652853\n",
      "Validation loss: 0.01506747080055168\n",
      "\n",
      "epoch: 174\n",
      "Training loss: 0.01610559637303755\n",
      "Validation loss: 0.01508343040453982\n",
      "\n",
      "epoch: 175\n",
      "Training loss: 0.016099098377239107\n",
      "Validation loss: 0.015062142824023942\n",
      "\n",
      "epoch: 176\n",
      "Training loss: 0.01607929000043308\n",
      "Validation loss: 0.015098078841847771\n",
      "\n",
      "epoch: 177\n",
      "Training loss: 0.0160596950015482\n",
      "Validation loss: 0.015078730271199053\n",
      "\n",
      "epoch: 178\n",
      "Training loss: 0.016043350322324207\n",
      "Validation loss: 0.015094523806996718\n",
      "\n",
      "epoch: 179\n",
      "Training loss: 0.01604420106070415\n",
      "Validation loss: 0.015089647080143527\n",
      "\n",
      "epoch: 180\n",
      "Training loss: 0.016022959851725874\n",
      "Validation loss: 0.015097695347078265\n",
      "\n",
      "epoch: 181\n",
      "Training loss: 0.016004564878367336\n",
      "Validation loss: 0.015098363597384371\n",
      "\n",
      "epoch: 182\n",
      "Training loss: 0.01599597898050677\n",
      "Validation loss: 0.01513542454300236\n",
      "\n",
      "epoch: 183\n",
      "Training loss: 0.015965314345579743\n",
      "Validation loss: 0.015066157742578606\n",
      "\n",
      "epoch: 184\n",
      "Training loss: 0.015952469725673192\n",
      "Validation loss: 0.015081326647362205\n",
      "\n",
      "epoch: 185\n",
      "Training loss: 0.015927150142728764\n",
      "Validation loss: 0.014993433880305806\n",
      "\n",
      "epoch: 186\n",
      "Training loss: 0.01591728654703831\n",
      "Validation loss: 0.014961938399997505\n",
      "\n",
      "epoch: 187\n",
      "Training loss: 0.015906663101509226\n",
      "Validation loss: 0.014978195450274916\n",
      "\n",
      "epoch: 188\n",
      "Training loss: 0.0158801980302553\n",
      "Validation loss: 0.01497386391715898\n",
      "\n",
      "epoch: 189\n",
      "Training loss: 0.015875742205245045\n",
      "Validation loss: 0.014970838395227606\n",
      "\n",
      "epoch: 190\n",
      "Training loss: 0.01586186909883202\n",
      "Validation loss: 0.01496798847209823\n",
      "\n",
      "epoch: 191\n",
      "Training loss: 0.015829559531758554\n",
      "Validation loss: 0.014941058705447628\n",
      "\n",
      "epoch: 192\n",
      "Training loss: 0.01581542577717652\n",
      "Validation loss: 0.014917206536820328\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 193\n",
      "Training loss: 0.01579901457394659\n",
      "Validation loss: 0.01489366604993984\n",
      "\n",
      "epoch: 194\n",
      "Training loss: 0.015782811690770768\n",
      "Validation loss: 0.014912384784340461\n",
      "\n",
      "epoch: 195\n",
      "Training loss: 0.01576524177997166\n",
      "Validation loss: 0.014838949413249083\n",
      "\n",
      "epoch: 196\n",
      "Training loss: 0.01573056202231061\n",
      "Validation loss: 0.014824941377849217\n",
      "\n",
      "epoch: 197\n",
      "Training loss: 0.015720810592137605\n",
      "Validation loss: 0.014880117771795483\n",
      "\n",
      "epoch: 198\n",
      "Training loss: 0.015702422329297586\n",
      "Validation loss: 0.014817541079703433\n",
      "\n",
      "epoch: 199\n",
      "Training loss: 0.015669191626087355\n",
      "Validation loss: 0.014936546280670796\n",
      "\n",
      "epoch: 200\n",
      "Training loss: 0.015674576611433007\n",
      "Validation loss: 0.014818221616346695\n",
      "\n",
      "epoch: 201\n",
      "Training loss: 0.015652839984876255\n",
      "Validation loss: 0.01471695844540404\n",
      "\n",
      "epoch: 202\n",
      "Training loss: 0.015630084029565554\n",
      "Validation loss: 0.014739546189485048\n",
      "\n",
      "epoch: 203\n",
      "Training loss: 0.015607294681427245\n",
      "Validation loss: 0.014834692972610003\n",
      "\n",
      "epoch: 204\n",
      "Training loss: 0.015604796773079804\n",
      "Validation loss: 0.014732843056470293\n",
      "\n",
      "epoch: 205\n",
      "Training loss: 0.01557227188010972\n",
      "Validation loss: 0.014630422242055574\n",
      "\n",
      "epoch: 206\n",
      "Training loss: 0.015548642936312758\n",
      "Validation loss: 0.014774717664260804\n",
      "\n",
      "epoch: 207\n",
      "Training loss: 0.015527997953841459\n",
      "Validation loss: 0.014613109066751208\n",
      "\n",
      "epoch: 208\n",
      "Training loss: 0.015516592551865107\n",
      "Validation loss: 0.014754140385830675\n",
      "\n",
      "epoch: 209\n",
      "Training loss: 0.01552483632977856\n",
      "Validation loss: 0.014737878159548329\n",
      "\n",
      "epoch: 210\n",
      "Training loss: 0.015515078155219346\n",
      "Validation loss: 0.014744589703737614\n",
      "\n",
      "epoch: 211\n",
      "Training loss: 0.015478950363440762\n",
      "Validation loss: 0.014714626340895839\n",
      "\n",
      "epoch: 212\n",
      "Training loss: 0.015491340651730995\n",
      "Validation loss: 0.014672257373798589\n",
      "\n",
      "epoch: 213\n",
      "Training loss: 0.01545273563664081\n",
      "Validation loss: 0.014674600470182986\n",
      "\n",
      "epoch: 214\n",
      "Training loss: 0.015423243330733614\n",
      "Validation loss: 0.014611535765089754\n",
      "\n",
      "epoch: 215\n",
      "Training loss: 0.015414761852294127\n",
      "Validation loss: 0.014593096686768595\n",
      "\n",
      "epoch: 216\n",
      "Training loss: 0.015406850511861619\n",
      "Validation loss: 0.01459285002783186\n",
      "\n",
      "epoch: 217\n",
      "Training loss: 0.015393721152352373\n",
      "Validation loss: 0.014568988457470563\n",
      "\n",
      "epoch: 218\n",
      "Training loss: 0.015380849898744844\n",
      "Validation loss: 0.014560278541686008\n",
      "\n",
      "epoch: 219\n",
      "Training loss: 0.015373777452858568\n",
      "Validation loss: 0.014476745343064852\n",
      "\n",
      "epoch: 220\n",
      "Training loss: 0.015343529640527286\n",
      "Validation loss: 0.014503910174056162\n",
      "\n",
      "epoch: 221\n",
      "Training loss: 0.015336701805560168\n",
      "Validation loss: 0.014501551344945367\n",
      "\n",
      "epoch: 222\n",
      "Training loss: 0.01531266243881647\n",
      "Validation loss: 0.014504446315988215\n",
      "\n",
      "epoch: 223\n",
      "Training loss: 0.01531761728271746\n",
      "Validation loss: 0.014481663706270563\n",
      "\n",
      "epoch: 224\n",
      "Training loss: 0.015302043554912052\n",
      "Validation loss: 0.014483657093670945\n",
      "\n",
      "epoch: 225\n",
      "Training loss: 0.015308903941612821\n",
      "Validation loss: 0.014477635473230159\n",
      "\n",
      "epoch: 226\n",
      "Training loss: 0.015287028798554956\n",
      "Validation loss: 0.014463304828557129\n",
      "\n",
      "epoch: 227\n",
      "Training loss: 0.015269014939539133\n",
      "Validation loss: 0.014469576415131987\n",
      "\n",
      "epoch: 228\n",
      "Training loss: 0.015257834510368653\n",
      "Validation loss: 0.014440405558772485\n",
      "\n",
      "epoch: 229\n",
      "Training loss: 0.015238945436873787\n",
      "Validation loss: 0.014378831789115075\n",
      "\n",
      "epoch: 230\n",
      "Training loss: 0.015222656304465891\n",
      "Validation loss: 0.014404209840185502\n",
      "\n",
      "epoch: 231\n",
      "Training loss: 0.015216194309008953\n",
      "Validation loss: 0.014356619739721669\n",
      "\n",
      "epoch: 232\n",
      "Training loss: 0.015202890703752301\n",
      "Validation loss: 0.014391830183778558\n",
      "\n",
      "epoch: 233\n",
      "Training loss: 0.015203035401459867\n",
      "Validation loss: 0.014365831569577102\n",
      "\n",
      "epoch: 234\n",
      "Training loss: 0.015181450972890547\n",
      "Validation loss: 0.014363750584385476\n",
      "\n",
      "epoch: 235\n",
      "Training loss: 0.015167322654878121\n",
      "Validation loss: 0.014353271881417051\n",
      "\n",
      "epoch: 236\n",
      "Training loss: 0.015157472627855047\n",
      "Validation loss: 0.014295203602147958\n",
      "\n",
      "epoch: 237\n",
      "Training loss: 0.015136368886149882\n",
      "Validation loss: 0.01429050499379098\n",
      "\n",
      "epoch: 238\n",
      "Training loss: 0.015138614743226936\n",
      "Validation loss: 0.014312388150793194\n",
      "\n",
      "epoch: 239\n",
      "Training loss: 0.01513048441163884\n",
      "Validation loss: 0.014277726728546715\n",
      "\n",
      "epoch: 240\n",
      "Training loss: 0.015115916876879609\n",
      "Validation loss: 0.014237943373128026\n",
      "\n",
      "epoch: 241\n",
      "Training loss: 0.015111297524123272\n",
      "Validation loss: 0.014241667869134897\n",
      "\n",
      "epoch: 242\n",
      "Training loss: 0.015102997808076658\n",
      "Validation loss: 0.014215039513904324\n",
      "\n",
      "epoch: 243\n",
      "Training loss: 0.015089503959449573\n",
      "Validation loss: 0.014202419794994443\n",
      "\n",
      "epoch: 244\n",
      "Training loss: 0.01507096974270011\n",
      "Validation loss: 0.014182722629388139\n",
      "\n",
      "epoch: 245\n",
      "Training loss: 0.015054075429546168\n",
      "Validation loss: 0.014145152645474801\n",
      "\n",
      "epoch: 246\n",
      "Training loss: 0.015053221429114222\n",
      "Validation loss: 0.014154367068488136\n",
      "\n",
      "epoch: 247\n",
      "Training loss: 0.015033363293352827\n",
      "Validation loss: 0.014104544702434739\n",
      "\n",
      "epoch: 248\n",
      "Training loss: 0.01503467153330854\n",
      "Validation loss: 0.014105586940782195\n",
      "\n",
      "epoch: 249\n",
      "Training loss: 0.015019150497322045\n",
      "Validation loss: 0.014084814013594868\n",
      "\n",
      "epoch: 250\n",
      "Training loss: 0.015006578232889789\n",
      "Validation loss: 0.014030715150083707\n",
      "\n",
      "epoch: 251\n",
      "Training loss: 0.01499914662360501\n",
      "Validation loss: 0.014029291688566543\n",
      "\n",
      "epoch: 252\n",
      "Training loss: 0.014990116205590749\n",
      "Validation loss: 0.014042749755129983\n",
      "\n",
      "epoch: 253\n",
      "Training loss: 0.014989243675052165\n",
      "Validation loss: 0.014017745659290575\n",
      "\n",
      "epoch: 254\n",
      "Training loss: 0.014972722987965608\n",
      "Validation loss: 0.01401265037538544\n",
      "\n",
      "epoch: 255\n",
      "Training loss: 0.014969810009200626\n",
      "Validation loss: 0.014015408545875862\n",
      "\n",
      "epoch: 256\n",
      "Training loss: 0.014959079842200455\n",
      "Validation loss: 0.013981778221300234\n",
      "\n",
      "epoch: 257\n",
      "Training loss: 0.014942988644253093\n",
      "Validation loss: 0.013997948204950138\n",
      "\n",
      "epoch: 258\n",
      "Training loss: 0.01494612345053093\n",
      "Validation loss: 0.013984502952956555\n",
      "\n",
      "epoch: 259\n",
      "Training loss: 0.01493773310157874\n",
      "Validation loss: 0.013982805093548045\n",
      "\n",
      "epoch: 260\n",
      "Training loss: 0.01492692711531589\n",
      "Validation loss: 0.013978572329823904\n",
      "\n",
      "epoch: 261\n",
      "Training loss: 0.014919660858573866\n",
      "Validation loss: 0.013965945485900442\n",
      "\n",
      "epoch: 262\n",
      "Training loss: 0.014900244946579424\n",
      "Validation loss: 0.013957466162595784\n",
      "\n",
      "epoch: 263\n",
      "Training loss: 0.014904098021691578\n",
      "Validation loss: 0.013935525717662297\n",
      "\n",
      "epoch: 264\n",
      "Training loss: 0.014882088119038156\n",
      "Validation loss: 0.013921116992559586\n",
      "\n",
      "epoch: 265\n",
      "Training loss: 0.014883166962397729\n",
      "Validation loss: 0.013901292541729672\n",
      "\n",
      "epoch: 266\n",
      "Training loss: 0.014882291889266307\n",
      "Validation loss: 0.013932847498310294\n",
      "\n",
      "epoch: 267\n",
      "Training loss: 0.014867671892341587\n",
      "Validation loss: 0.013924652041547213\n",
      "\n",
      "epoch: 268\n",
      "Training loss: 0.014864162085988316\n",
      "Validation loss: 0.013916421138083116\n",
      "\n",
      "epoch: 269\n",
      "Training loss: 0.014853687248572646\n",
      "Validation loss: 0.013882177817386564\n",
      "\n",
      "epoch: 270\n",
      "Training loss: 0.014852896583287228\n",
      "Validation loss: 0.013885692803977516\n",
      "\n",
      "epoch: 271\n",
      "Training loss: 0.014836629919289622\n",
      "Validation loss: 0.013904113072809272\n",
      "\n",
      "epoch: 272\n",
      "Training loss: 0.014831171374979567\n",
      "Validation loss: 0.013866703820513044\n",
      "\n",
      "epoch: 273\n",
      "Training loss: 0.014827833957388398\n",
      "Validation loss: 0.013878094622861962\n",
      "\n",
      "epoch: 274\n",
      "Training loss: 0.014814137999859645\n",
      "Validation loss: 0.013879246500938214\n",
      "\n",
      "epoch: 275\n",
      "Training loss: 0.014807075758769451\n",
      "Validation loss: 0.01383303080529257\n",
      "\n",
      "epoch: 276\n",
      "Training loss: 0.014799549712009303\n",
      "Validation loss: 0.013851795593828221\n",
      "\n",
      "epoch: 277\n",
      "Training loss: 0.014790789154780095\n",
      "Validation loss: 0.013812340972874303\n",
      "\n",
      "epoch: 278\n",
      "Training loss: 0.014790214167908057\n",
      "Validation loss: 0.013827963352785935\n",
      "\n",
      "epoch: 279\n",
      "Training loss: 0.01477505173161103\n",
      "Validation loss: 0.013814928812222262\n",
      "\n",
      "epoch: 280\n",
      "Training loss: 0.014768496627493655\n",
      "Validation loss: 0.013816939247862248\n",
      "\n",
      "epoch: 281\n",
      "Training loss: 0.014771666733983863\n",
      "Validation loss: 0.013799311213549612\n",
      "\n",
      "epoch: 282\n",
      "Training loss: 0.014759212283775962\n",
      "Validation loss: 0.013807751693028603\n",
      "\n",
      "epoch: 283\n",
      "Training loss: 0.014750162192254676\n",
      "Validation loss: 0.013795218715783769\n",
      "\n",
      "epoch: 284\n",
      "Training loss: 0.014761248546796012\n",
      "Validation loss: 0.013790707523704426\n",
      "\n",
      "epoch: 285\n",
      "Training loss: 0.014744489783398033\n",
      "Validation loss: 0.013775254547723133\n",
      "\n",
      "epoch: 286\n",
      "Training loss: 0.014744695800509295\n",
      "Validation loss: 0.013771546614216762\n",
      "\n",
      "epoch: 287\n",
      "Training loss: 0.014723897553383611\n",
      "Validation loss: 0.013767140679877787\n",
      "\n",
      "epoch: 288\n",
      "Training loss: 0.014744344807387923\n",
      "Validation loss: 0.013752452804229856\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 289\n",
      "Training loss: 0.01472520692803853\n",
      "Validation loss: 0.013752838725706326\n",
      "\n",
      "epoch: 290\n",
      "Training loss: 0.014723875069745313\n",
      "Validation loss: 0.013750343991276467\n",
      "\n",
      "epoch: 291\n",
      "Training loss: 0.014704190838140802\n",
      "Validation loss: 0.013769119132881959\n",
      "\n",
      "epoch: 292\n",
      "Training loss: 0.014706704573244819\n",
      "Validation loss: 0.013717429154322978\n",
      "\n",
      "epoch: 293\n",
      "Training loss: 0.014675423849707674\n",
      "Validation loss: 0.013743180731603003\n",
      "\n",
      "epoch: 294\n",
      "Training loss: 0.014674980670977892\n",
      "Validation loss: 0.013723952556450874\n",
      "\n",
      "epoch: 295\n",
      "Training loss: 0.014691115574168438\n",
      "Validation loss: 0.013708721078601771\n",
      "\n",
      "epoch: 296\n",
      "Training loss: 0.014682906441952033\n",
      "Validation loss: 0.013706066740022902\n",
      "\n",
      "epoch: 297\n",
      "Training loss: 0.014677778167673908\n",
      "Validation loss: 0.013705768928341482\n",
      "\n",
      "epoch: 298\n",
      "Training loss: 0.014666611249372824\n",
      "Validation loss: 0.013673975817334523\n",
      "\n",
      "epoch: 299\n",
      "Training loss: 0.01464580038858598\n",
      "Validation loss: 0.01370392160030408\n",
      "\n",
      "epoch: 300\n",
      "Training loss: 0.014652268462868625\n",
      "Validation loss: 0.013700177302147074\n",
      "\n",
      "epoch: 301\n",
      "Training loss: 0.014644760701407113\n",
      "Validation loss: 0.013670256846079702\n",
      "\n",
      "epoch: 302\n",
      "Training loss: 0.014641514268954606\n",
      "Validation loss: 0.013666492400056547\n",
      "\n",
      "epoch: 303\n",
      "Training loss: 0.014614830989400042\n",
      "Validation loss: 0.013667730087599726\n",
      "\n",
      "epoch: 304\n",
      "Training loss: 0.014629593292151922\n",
      "Validation loss: 0.013671019512240822\n",
      "\n",
      "epoch: 305\n",
      "Training loss: 0.01462513797538482\n",
      "Validation loss: 0.013648607350588149\n",
      "\n",
      "epoch: 306\n",
      "Training loss: 0.0146170802146546\n",
      "Validation loss: 0.013657322508522678\n",
      "\n",
      "epoch: 307\n",
      "Training loss: 0.01461263452488853\n",
      "Validation loss: 0.013652357340686008\n",
      "\n",
      "epoch: 308\n",
      "Training loss: 0.014604022939610616\n",
      "Validation loss: 0.013671889871969115\n",
      "\n",
      "epoch: 309\n",
      "Training loss: 0.014599464951120307\n",
      "Validation loss: 0.013637673626638602\n",
      "\n",
      "epoch: 310\n",
      "Training loss: 0.014590392303379567\n",
      "Validation loss: 0.013665905546231467\n",
      "\n",
      "epoch: 311\n",
      "Training loss: 0.014589826040129239\n",
      "Validation loss: 0.013662470467018786\n",
      "\n",
      "epoch: 312\n",
      "Training loss: 0.014563466887023035\n",
      "Validation loss: 0.013640257467831318\n",
      "\n",
      "epoch: 313\n",
      "Training loss: 0.014580463804989922\n",
      "Validation loss: 0.013648816417071233\n",
      "\n",
      "epoch: 314\n",
      "Training loss: 0.014548103575888887\n",
      "Validation loss: 0.013652879416825227\n",
      "\n",
      "epoch: 315\n",
      "Training loss: 0.014549287829612912\n",
      "Validation loss: 0.013631190172225619\n",
      "\n",
      "epoch: 316\n",
      "Training loss: 0.014565972261078086\n",
      "Validation loss: 0.01361300303545027\n",
      "\n",
      "epoch: 317\n",
      "Training loss: 0.014555256050316455\n",
      "Validation loss: 0.013626391530422695\n",
      "\n",
      "epoch: 318\n",
      "Training loss: 0.01454909383087278\n",
      "Validation loss: 0.013628192372871554\n",
      "\n",
      "epoch: 319\n",
      "Training loss: 0.014522004356451226\n",
      "Validation loss: 0.013614969992084648\n",
      "\n",
      "epoch: 320\n",
      "Training loss: 0.014516143299215906\n",
      "Validation loss: 0.013622003161268973\n",
      "\n",
      "epoch: 321\n",
      "Training loss: 0.014509743061907901\n",
      "Validation loss: 0.013623539994882848\n",
      "\n",
      "epoch: 322\n",
      "Training loss: 0.01452858109798057\n",
      "Validation loss: 0.013589611011435805\n",
      "\n",
      "epoch: 323\n",
      "Training loss: 0.014515179706461516\n",
      "Validation loss: 0.013594503960164695\n",
      "\n",
      "epoch: 324\n",
      "Training loss: 0.01449631147012405\n",
      "Validation loss: 0.013589666949795829\n",
      "\n",
      "epoch: 325\n",
      "Training loss: 0.014494485284358151\n",
      "Validation loss: 0.01358212235747091\n",
      "\n",
      "epoch: 326\n",
      "Training loss: 0.014479393260653168\n",
      "Validation loss: 0.013597525340559769\n",
      "\n",
      "epoch: 327\n",
      "Training loss: 0.014508159977887968\n",
      "Validation loss: 0.013550103901704653\n",
      "\n",
      "epoch: 328\n",
      "Training loss: 0.014473292028109448\n",
      "Validation loss: 0.013558989878176332\n",
      "\n",
      "epoch: 329\n",
      "Training loss: 0.014469111627797779\n",
      "Validation loss: 0.013552090150717051\n",
      "\n",
      "epoch: 330\n",
      "Training loss: 0.01449141126816363\n",
      "Validation loss: 0.013554543624775456\n",
      "\n",
      "epoch: 331\n",
      "Training loss: 0.014482980768273725\n",
      "Validation loss: 0.013572953139271654\n",
      "\n",
      "epoch: 332\n",
      "Training loss: 0.014473285779645486\n",
      "Validation loss: 0.013571829514751855\n",
      "\n",
      "epoch: 333\n",
      "Training loss: 0.01448162871868431\n",
      "Validation loss: 0.013575953838731386\n",
      "\n",
      "epoch: 334\n",
      "Training loss: 0.014468364936421996\n",
      "Validation loss: 0.013555008275788517\n",
      "\n",
      "epoch: 335\n",
      "Training loss: 0.014465415100152623\n",
      "Validation loss: 0.0135807837019343\n",
      "\n",
      "epoch: 336\n",
      "Training loss: 0.014454135511030095\n",
      "Validation loss: 0.013557824042588654\n",
      "\n",
      "epoch: 337\n",
      "Training loss: 0.014449432693473027\n",
      "Validation loss: 0.013573613562655702\n",
      "\n",
      "epoch: 338\n",
      "Training loss: 0.014475074209386813\n",
      "Validation loss: 0.013572453347521083\n",
      "\n",
      "epoch: 339\n",
      "Training loss: 0.014460846062750768\n",
      "Validation loss: 0.01360456558466179\n",
      "\n",
      "epoch: 340\n",
      "Training loss: 0.014454449299246245\n",
      "Validation loss: 0.01359844047245797\n",
      "\n",
      "epoch: 341\n",
      "Training loss: 0.014474664331808997\n",
      "Validation loss: 0.013583177676093853\n",
      "\n",
      "epoch: 342\n",
      "Training loss: 0.014473538785338674\n",
      "Validation loss: 0.013578863538316808\n",
      "\n",
      "epoch: 343\n",
      "Training loss: 0.014445014949497001\n",
      "Validation loss: 0.013615601596644274\n",
      "\n",
      "epoch: 344\n",
      "Training loss: 0.0144506592989883\n",
      "Validation loss: 0.013548406671141081\n",
      "\n",
      "epoch: 345\n",
      "Training loss: 0.014446436930170954\n",
      "Validation loss: 0.01353784364855401\n",
      "\n",
      "epoch: 346\n",
      "Training loss: 0.014434479047160147\n",
      "Validation loss: 0.013577494099951704\n",
      "\n",
      "epoch: 347\n",
      "Training loss: 0.0144426321453159\n",
      "Validation loss: 0.013592100280979014\n",
      "\n",
      "epoch: 348\n",
      "Training loss: 0.014435949015377914\n",
      "Validation loss: 0.013554644351299808\n",
      "\n",
      "epoch: 349\n",
      "Training loss: 0.014410624758503413\n",
      "Validation loss: 0.013551296441734932\n",
      "\n",
      "epoch: 350\n",
      "Training loss: 0.014415828543814638\n",
      "Validation loss: 0.013523886717181912\n",
      "\n",
      "epoch: 351\n",
      "Training loss: 0.014399871055217528\n",
      "Validation loss: 0.013520000854500967\n",
      "\n",
      "epoch: 352\n",
      "Training loss: 0.014398783337061526\n",
      "Validation loss: 0.013546336236150872\n",
      "\n",
      "epoch: 353\n",
      "Training loss: 0.014443941453501882\n",
      "Validation loss: 0.013528206360842026\n",
      "\n",
      "epoch: 354\n",
      "Training loss: 0.01442858305281452\n",
      "Validation loss: 0.013526423518697907\n",
      "\n",
      "epoch: 355\n",
      "Training loss: 0.014424134510017155\n",
      "Validation loss: 0.013569031640272167\n",
      "\n",
      "epoch: 356\n",
      "Training loss: 0.014431033594974252\n",
      "Validation loss: 0.013554439885040955\n",
      "\n",
      "epoch: 357\n",
      "Training loss: 0.01441997642133802\n",
      "Validation loss: 0.01352954983126779\n",
      "\n",
      "epoch: 358\n",
      "Training loss: 0.014418177575481314\n",
      "Validation loss: 0.013517383791416263\n",
      "\n",
      "epoch: 359\n",
      "Training loss: 0.014408002582048616\n",
      "Validation loss: 0.013539968044922602\n",
      "\n",
      "epoch: 360\n",
      "Training loss: 0.014406638099696141\n",
      "Validation loss: 0.013514334712447295\n",
      "\n",
      "epoch: 361\n",
      "Training loss: 0.014401841162235772\n",
      "Validation loss: 0.013503170823130314\n",
      "\n",
      "epoch: 362\n",
      "Training loss: 0.014398713667495627\n",
      "Validation loss: 0.013472265193420613\n",
      "\n",
      "epoch: 363\n",
      "Training loss: 0.01439262572839334\n",
      "Validation loss: 0.013466500404641337\n",
      "\n",
      "epoch: 364\n",
      "Training loss: 0.014379170733377177\n",
      "Validation loss: 0.013455873199823474\n",
      "\n",
      "epoch: 365\n",
      "Training loss: 0.014378892930047684\n",
      "Validation loss: 0.013487629814049789\n",
      "\n",
      "epoch: 366\n",
      "Training loss: 0.014378017848305821\n",
      "Validation loss: 0.013470134388188382\n",
      "\n",
      "epoch: 367\n",
      "Training loss: 0.014376719665266175\n",
      "Validation loss: 0.013475657052985928\n",
      "\n",
      "epoch: 368\n",
      "Training loss: 0.014357728197487605\n",
      "Validation loss: 0.013456262170997284\n",
      "\n",
      "epoch: 369\n",
      "Training loss: 0.014358888468640646\n",
      "Validation loss: 0.013500249889994532\n",
      "\n",
      "epoch: 370\n",
      "Training loss: 0.014360999393441461\n",
      "Validation loss: 0.013455716708686841\n",
      "\n",
      "epoch: 371\n",
      "Training loss: 0.014352491778256813\n",
      "Validation loss: 0.013462803640172232\n",
      "\n",
      "epoch: 372\n",
      "Training loss: 0.014336449170151727\n",
      "Validation loss: 0.013442739911840158\n",
      "\n",
      "epoch: 373\n",
      "Training loss: 0.014340050581458944\n",
      "Validation loss: 0.0134223917426812\n",
      "\n",
      "epoch: 374\n",
      "Training loss: 0.014337764542808473\n",
      "Validation loss: 0.013376708703724398\n",
      "\n",
      "epoch: 375\n",
      "Training loss: 0.014327549665376691\n",
      "Validation loss: 0.013435838537224501\n",
      "\n",
      "epoch: 376\n",
      "Training loss: 0.014318536785970481\n",
      "Validation loss: 0.013427981890537547\n",
      "\n",
      "epoch: 377\n",
      "Training loss: 0.014314552528369174\n",
      "Validation loss: 0.013390766995702148\n",
      "\n",
      "epoch: 378\n",
      "Training loss: 0.014319939426756906\n",
      "Validation loss: 0.01339324664174748\n",
      "\n",
      "epoch: 379\n",
      "Training loss: 0.014317913611209088\n",
      "Validation loss: 0.013355264266706716\n",
      "\n",
      "epoch: 380\n",
      "Training loss: 0.014308962648585062\n",
      "Validation loss: 0.013334897722760194\n",
      "\n",
      "epoch: 381\n",
      "Training loss: 0.01429346287129535\n",
      "Validation loss: 0.013387530583231529\n",
      "\n",
      "epoch: 382\n",
      "Training loss: 0.014297025192469382\n",
      "Validation loss: 0.013352864163217619\n",
      "\n",
      "epoch: 383\n",
      "Training loss: 0.014277654311016179\n",
      "Validation loss: 0.013374830380554894\n",
      "\n",
      "epoch: 384\n",
      "Training loss: 0.014279308493036519\n",
      "Validation loss: 0.013374165003404924\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 385\n",
      "Training loss: 0.01427271125609085\n",
      "Validation loss: 0.013370940744791357\n",
      "\n",
      "epoch: 386\n",
      "Training loss: 0.01426757475756625\n",
      "Validation loss: 0.013373515218433708\n",
      "\n",
      "epoch: 387\n",
      "Training loss: 0.014263920701468506\n",
      "Validation loss: 0.013344332841719613\n",
      "\n",
      "epoch: 388\n",
      "Training loss: 0.01425887332132287\n",
      "Validation loss: 0.013378307335253749\n",
      "\n",
      "epoch: 389\n",
      "Training loss: 0.014256960654051638\n",
      "Validation loss: 0.013381731844806672\n",
      "\n",
      "epoch: 390\n",
      "Training loss: 0.014250189111841886\n",
      "Validation loss: 0.013339169870198076\n",
      "\n",
      "epoch: 391\n",
      "Training loss: 0.014245624655152916\n",
      "Validation loss: 0.013334194658644992\n",
      "\n",
      "epoch: 392\n",
      "Training loss: 0.01424484093222782\n",
      "Validation loss: 0.01336398463879015\n",
      "\n",
      "epoch: 393\n",
      "Training loss: 0.014238889469201186\n",
      "Validation loss: 0.01334846356749414\n",
      "\n",
      "epoch: 394\n",
      "Training loss: 0.014227392572778982\n",
      "Validation loss: 0.013365446523859216\n",
      "\n",
      "epoch: 395\n",
      "Training loss: 0.014243709059446842\n",
      "Validation loss: 0.01333095539364722\n",
      "\n",
      "epoch: 396\n",
      "Training loss: 0.014234253540921366\n",
      "Validation loss: 0.013351975935858842\n",
      "\n",
      "epoch: 397\n",
      "Training loss: 0.014236417855725261\n",
      "Validation loss: 0.013312513923995989\n",
      "\n",
      "epoch: 398\n",
      "Training loss: 0.014218262230439803\n",
      "Validation loss: 0.013311471623793886\n",
      "\n",
      "epoch: 399\n",
      "Training loss: 0.014223103868297931\n",
      "Validation loss: 0.013315927912568038\n",
      "\n",
      "epoch: 400\n",
      "Training loss: 0.01421938123546614\n",
      "Validation loss: 0.013314011971077734\n",
      "\n",
      "epoch: 401\n",
      "Training loss: 0.01419860157718896\n",
      "Validation loss: 0.01333352178203155\n",
      "\n",
      "epoch: 402\n",
      "Training loss: 0.01420620348509385\n",
      "Validation loss: 0.013302425826852139\n",
      "\n",
      "epoch: 403\n",
      "Training loss: 0.014201462992441101\n",
      "Validation loss: 0.013297609752520636\n",
      "\n",
      "epoch: 404\n",
      "Training loss: 0.014196456361541461\n",
      "Validation loss: 0.013261256802923477\n",
      "\n",
      "epoch: 405\n",
      "Training loss: 0.014191911145506868\n",
      "Validation loss: 0.013283983616603308\n",
      "\n",
      "epoch: 406\n",
      "Training loss: 0.014172839470762658\n",
      "Validation loss: 0.013284615763013843\n",
      "\n",
      "epoch: 407\n",
      "Training loss: 0.014184955247923703\n",
      "Validation loss: 0.013299628935137323\n",
      "\n",
      "epoch: 408\n",
      "Training loss: 0.01419906924056405\n",
      "Validation loss: 0.013284204920437831\n",
      "\n",
      "epoch: 409\n",
      "Training loss: 0.014184147997973627\n",
      "Validation loss: 0.013302726842697291\n",
      "\n",
      "epoch: 410\n",
      "Training loss: 0.014198562411379631\n",
      "Validation loss: 0.01328031479766783\n",
      "\n",
      "epoch: 411\n",
      "Training loss: 0.014189220489466647\n",
      "Validation loss: 0.013281977681019434\n",
      "\n",
      "epoch: 412\n",
      "Training loss: 0.01420001554517973\n",
      "Validation loss: 0.013244351796974262\n",
      "\n",
      "epoch: 413\n",
      "Training loss: 0.014178473442711436\n",
      "Validation loss: 0.013242215379242651\n",
      "\n",
      "epoch: 414\n",
      "Training loss: 0.014173600948037702\n",
      "Validation loss: 0.01324995401816922\n",
      "\n",
      "epoch: 415\n",
      "Training loss: 0.014163385672780998\n",
      "Validation loss: 0.013242734957711413\n",
      "\n",
      "epoch: 416\n",
      "Training loss: 0.014167748196938423\n",
      "Validation loss: 0.013231879521563504\n",
      "\n",
      "epoch: 417\n",
      "Training loss: 0.014163942004511515\n",
      "Validation loss: 0.013221263669554539\n",
      "\n",
      "epoch: 418\n",
      "Training loss: 0.014154915446255556\n",
      "Validation loss: 0.013223276558729164\n",
      "\n",
      "epoch: 419\n",
      "Training loss: 0.014156061703723011\n",
      "Validation loss: 0.013222528416486265\n",
      "\n",
      "epoch: 420\n",
      "Training loss: 0.014153120328683108\n",
      "Validation loss: 0.01319183229660975\n",
      "\n",
      "epoch: 421\n",
      "Training loss: 0.014157738120327445\n",
      "Validation loss: 0.013183460345166022\n",
      "\n",
      "epoch: 422\n",
      "Training loss: 0.01415077339213284\n",
      "Validation loss: 0.013197717140614539\n",
      "\n",
      "epoch: 423\n",
      "Training loss: 0.014153607170830479\n",
      "Validation loss: 0.01319037578324246\n",
      "\n",
      "epoch: 424\n",
      "Training loss: 0.014134585212646059\n",
      "Validation loss: 0.013193447989982315\n",
      "\n",
      "epoch: 425\n",
      "Training loss: 0.01414985396271378\n",
      "Validation loss: 0.013196309145450827\n",
      "\n",
      "epoch: 426\n",
      "Training loss: 0.014136354621228442\n",
      "Validation loss: 0.01318951509598962\n",
      "\n",
      "epoch: 427\n",
      "Training loss: 0.014146066975682166\n",
      "Validation loss: 0.013186016332138853\n",
      "\n",
      "epoch: 428\n",
      "Training loss: 0.014131228548331841\n",
      "Validation loss: 0.013165820067926952\n",
      "\n",
      "epoch: 429\n",
      "Training loss: 0.014137711731870953\n",
      "Validation loss: 0.013161513913564571\n",
      "\n",
      "epoch: 430\n",
      "Training loss: 0.014128252837004243\n",
      "Validation loss: 0.013167664471000168\n",
      "\n",
      "epoch: 431\n",
      "Training loss: 0.014118127825291628\n",
      "Validation loss: 0.013162662087716829\n",
      "\n",
      "epoch: 432\n",
      "Training loss: 0.014122300229791076\n",
      "Validation loss: 0.013155873249849619\n",
      "\n",
      "epoch: 433\n",
      "Training loss: 0.014113075938134777\n",
      "Validation loss: 0.013152275218197165\n",
      "\n",
      "epoch: 434\n",
      "Training loss: 0.014122224731373678\n",
      "Validation loss: 0.01316671967813194\n",
      "\n",
      "epoch: 435\n",
      "Training loss: 0.014117875691692948\n",
      "Validation loss: 0.013168949230609973\n",
      "\n",
      "epoch: 436\n",
      "Training loss: 0.014113422138016552\n",
      "Validation loss: 0.013159826242940448\n",
      "\n",
      "epoch: 437\n",
      "Training loss: 0.014103233330061657\n",
      "Validation loss: 0.013160334854873917\n",
      "\n",
      "epoch: 438\n",
      "Training loss: 0.01411039611443616\n",
      "Validation loss: 0.013156376295764343\n",
      "\n",
      "epoch: 439\n",
      "Training loss: 0.014110067407875048\n",
      "Validation loss: 0.01315290460143481\n",
      "\n",
      "epoch: 440\n",
      "Training loss: 0.014097132361947502\n",
      "Validation loss: 0.013138891353764542\n",
      "\n",
      "epoch: 441\n",
      "Training loss: 0.014123836168699617\n",
      "Validation loss: 0.013147973110887891\n",
      "\n",
      "epoch: 442\n",
      "Training loss: 0.01410512945541383\n",
      "Validation loss: 0.013132764566371502\n",
      "\n",
      "epoch: 443\n",
      "Training loss: 0.014107872061931457\n",
      "Validation loss: 0.013134173531895854\n",
      "\n",
      "epoch: 444\n",
      "Training loss: 0.01409813550942661\n",
      "Validation loss: 0.013134652773122183\n",
      "\n",
      "epoch: 445\n",
      "Training loss: 0.014111181923977357\n",
      "Validation loss: 0.013133294207662759\n",
      "\n",
      "epoch: 446\n",
      "Training loss: 0.014083762230441133\n",
      "Validation loss: 0.013120349926245483\n",
      "\n",
      "epoch: 447\n",
      "Training loss: 0.014107185010162117\n",
      "Validation loss: 0.01311878409762208\n",
      "\n",
      "epoch: 448\n",
      "Training loss: 0.014102997283159863\n",
      "Validation loss: 0.01312302950450508\n",
      "\n",
      "epoch: 449\n",
      "Training loss: 0.014087529828697825\n",
      "Validation loss: 0.013114133940813415\n",
      "\n",
      "epoch: 450\n",
      "Training loss: 0.014087440689898393\n",
      "Validation loss: 0.013134093878426041\n",
      "\n",
      "epoch: 451\n",
      "Training loss: 0.014081341839258818\n",
      "Validation loss: 0.013144961970936899\n",
      "\n",
      "epoch: 452\n",
      "Training loss: 0.014088385560598927\n",
      "Validation loss: 0.013155968901969572\n",
      "\n",
      "epoch: 453\n",
      "Training loss: 0.014088298387704748\n",
      "Validation loss: 0.013153554246251115\n",
      "\n",
      "epoch: 454\n",
      "Training loss: 0.014078821832408496\n",
      "Validation loss: 0.013156149742725316\n",
      "\n",
      "epoch: 455\n",
      "Training loss: 0.014096875921161128\n",
      "Validation loss: 0.013181510304216678\n",
      "\n",
      "epoch: 456\n",
      "Training loss: 0.014087178610814727\n",
      "Validation loss: 0.013193965626960163\n",
      "\n",
      "epoch: 457\n",
      "Training loss: 0.014092757273407957\n",
      "Validation loss: 0.013199552940653394\n",
      "\n",
      "epoch: 458\n",
      "Training loss: 0.01408891294500722\n",
      "Validation loss: 0.013188894367426943\n",
      "\n",
      "epoch: 459\n",
      "Training loss: 0.014088768003033315\n",
      "Validation loss: 0.01320617841776386\n",
      "\n",
      "epoch: 460\n",
      "Training loss: 0.014094125795664025\n",
      "Validation loss: 0.01321343596038587\n",
      "\n",
      "epoch: 461\n",
      "Training loss: 0.014089122774169807\n",
      "Validation loss: 0.013208197067024653\n",
      "\n",
      "epoch: 462\n",
      "Training loss: 0.014087061045864302\n",
      "Validation loss: 0.01321278346440165\n",
      "\n",
      "epoch: 463\n",
      "Training loss: 0.014088955133673633\n",
      "Validation loss: 0.013215687893755955\n",
      "\n",
      "epoch: 464\n",
      "Training loss: 0.014084600341424574\n",
      "Validation loss: 0.013215849951408102\n",
      "\n",
      "epoch: 465\n",
      "Training loss: 0.014085240630415365\n",
      "Validation loss: 0.013205266811570138\n",
      "\n",
      "epoch: 466\n",
      "Training loss: 0.014089365936312324\n",
      "Validation loss: 0.013213368995842953\n",
      "\n",
      "epoch: 467\n",
      "Training loss: 0.014092302187203084\n",
      "Validation loss: 0.013209063812294296\n",
      "\n",
      "epoch: 468\n",
      "Training loss: 0.01406144587118948\n",
      "Validation loss: 0.013201870422037996\n",
      "\n",
      "epoch: 469\n",
      "Training loss: 0.014086737945340605\n",
      "Validation loss: 0.013201113637638146\n",
      "\n",
      "epoch: 470\n",
      "Training loss: 0.014061340766622613\n",
      "Validation loss: 0.013207386254225567\n",
      "\n",
      "epoch: 471\n",
      "Training loss: 0.014071876053420931\n",
      "Validation loss: 0.013188013212690638\n",
      "\n",
      "epoch: 472\n",
      "Training loss: 0.01406231990737004\n",
      "Validation loss: 0.01319811656861404\n",
      "\n",
      "epoch: 473\n",
      "Training loss: 0.014070354336576349\n",
      "Validation loss: 0.01319643222842241\n",
      "\n",
      "epoch: 474\n",
      "Training loss: 0.014043435488879739\n",
      "Validation loss: 0.013219134995390196\n",
      "\n",
      "epoch: 475\n",
      "Training loss: 0.01406922717959864\n",
      "Validation loss: 0.013190230660284276\n",
      "\n",
      "epoch: 476\n",
      "Training loss: 0.014052536324462376\n",
      "Validation loss: 0.01321376722504609\n",
      "\n",
      "epoch: 477\n",
      "Training loss: 0.014044083771311546\n",
      "Validation loss: 0.013215311880306325\n",
      "\n",
      "epoch: 478\n",
      "Training loss: 0.014038680394145128\n",
      "Validation loss: 0.013190932137015101\n",
      "\n",
      "epoch: 479\n",
      "Training loss: 0.014053773075307694\n",
      "Validation loss: 0.013196283056746952\n",
      "\n",
      "epoch: 480\n",
      "Training loss: 0.014029131511136186\n",
      "Validation loss: 0.013186400364165265\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 481\n",
      "Training loss: 0.014046311310092622\n",
      "Validation loss: 0.01318467208849356\n",
      "\n",
      "epoch: 482\n",
      "Training loss: 0.014032518582920974\n",
      "Validation loss: 0.013184422160907415\n",
      "\n",
      "epoch: 483\n",
      "Training loss: 0.014036371598584665\n",
      "Validation loss: 0.013200559781976033\n",
      "\n",
      "epoch: 484\n",
      "Training loss: 0.014022467162032688\n",
      "Validation loss: 0.01316721085815234\n",
      "\n",
      "epoch: 485\n",
      "Training loss: 0.014025526807112953\n",
      "Validation loss: 0.013172849427021105\n",
      "\n",
      "epoch: 486\n",
      "Training loss: 0.014017983576898062\n",
      "Validation loss: 0.01317711122362861\n",
      "\n",
      "epoch: 487\n",
      "Training loss: 0.014018518614371553\n",
      "Validation loss: 0.013182182904217053\n",
      "\n",
      "epoch: 488\n",
      "Training loss: 0.014008601546259679\n",
      "Validation loss: 0.013172148681391872\n",
      "\n",
      "epoch: 489\n",
      "Training loss: 0.014002204444719375\n",
      "Validation loss: 0.013165706123778084\n",
      "\n",
      "epoch: 490\n",
      "Training loss: 0.014006823738970033\n",
      "Validation loss: 0.013162764077240322\n",
      "\n",
      "epoch: 491\n",
      "Training loss: 0.014006501257895898\n",
      "Validation loss: 0.013161860793693672\n",
      "\n",
      "epoch: 492\n",
      "Training loss: 0.014011932910984161\n",
      "Validation loss: 0.013144801909818173\n",
      "\n",
      "epoch: 493\n",
      "Training loss: 0.014005027825434783\n",
      "Validation loss: 0.013147265230551913\n",
      "\n",
      "epoch: 494\n",
      "Training loss: 0.0139940148649936\n",
      "Validation loss: 0.013134284095731655\n",
      "\n",
      "epoch: 495\n",
      "Training loss: 0.013999250897186719\n",
      "Validation loss: 0.013140195460619385\n",
      "\n",
      "epoch: 496\n",
      "Training loss: 0.013990839230280756\n",
      "Validation loss: 0.013120099100437616\n",
      "\n",
      "epoch: 497\n",
      "Training loss: 0.013977911118175593\n",
      "Validation loss: 0.013124668397643235\n",
      "\n",
      "epoch: 498\n",
      "Training loss: 0.013982991694656846\n",
      "Validation loss: 0.013132087373239665\n",
      "\n",
      "epoch: 499\n",
      "Training loss: 0.013979140876074988\n",
      "Validation loss: 0.013126075259475966\n",
      "\n",
      "epoch: 500\n",
      "Training loss: 0.013992689346848879\n",
      "Validation loss: 0.013122770315131054\n",
      "\n",
      "epoch: 501\n",
      "Training loss: 0.013974978392498442\n",
      "Validation loss: 0.013114256684819086\n",
      "\n",
      "epoch: 502\n",
      "Training loss: 0.013978131226345357\n",
      "Validation loss: 0.013094873768669213\n",
      "\n",
      "epoch: 503\n",
      "Training loss: 0.013970035545156016\n",
      "Validation loss: 0.013115571576305208\n",
      "\n",
      "epoch: 504\n",
      "Training loss: 0.013957712097513966\n",
      "Validation loss: 0.013121735490334109\n",
      "\n",
      "epoch: 505\n",
      "Training loss: 0.01397350697461383\n",
      "Validation loss: 0.013119014354837723\n",
      "\n",
      "epoch: 506\n",
      "Training loss: 0.01396696940712528\n",
      "Validation loss: 0.013116242464749342\n",
      "\n",
      "epoch: 507\n",
      "Training loss: 0.013958108280045045\n",
      "Validation loss: 0.013097746254222747\n",
      "\n",
      "epoch: 508\n",
      "Training loss: 0.013962901017125594\n",
      "Validation loss: 0.013121344792420915\n",
      "\n",
      "epoch: 509\n",
      "Training loss: 0.013969412986101807\n",
      "Validation loss: 0.013120715174729028\n",
      "\n",
      "epoch: 510\n",
      "Training loss: 0.013956610931419651\n",
      "Validation loss: 0.013127826415622003\n",
      "\n",
      "epoch: 511\n",
      "Training loss: 0.013953472369674175\n",
      "Validation loss: 0.013090699969943088\n",
      "\n",
      "epoch: 512\n",
      "Training loss: 0.013948168695864\n",
      "Validation loss: 0.013120852813572201\n",
      "\n",
      "epoch: 513\n",
      "Training loss: 0.013956112392567046\n",
      "Validation loss: 0.013126738769805153\n",
      "\n",
      "epoch: 514\n",
      "Training loss: 0.013961200191416823\n",
      "Validation loss: 0.013109287333587613\n",
      "\n",
      "epoch: 515\n",
      "Training loss: 0.013944961473206749\n",
      "Validation loss: 0.01312576809731136\n",
      "\n",
      "epoch: 516\n",
      "Training loss: 0.013956562540651252\n",
      "Validation loss: 0.013133689708314751\n",
      "\n",
      "epoch: 517\n",
      "Training loss: 0.01395324288875199\n",
      "Validation loss: 0.013130601017368806\n",
      "\n",
      "epoch: 518\n",
      "Training loss: 0.013942901052904494\n",
      "Validation loss: 0.013128288443274941\n",
      "\n",
      "epoch: 519\n",
      "Training loss: 0.013941358890907396\n",
      "Validation loss: 0.01312820224083353\n",
      "\n",
      "epoch: 520\n",
      "Training loss: 0.013946701542640839\n",
      "Validation loss: 0.013134570148760344\n",
      "\n",
      "epoch: 521\n",
      "Training loss: 0.013946530635981695\n",
      "Validation loss: 0.013122433450266448\n",
      "\n",
      "epoch: 522\n",
      "Training loss: 0.013934540012649313\n",
      "Validation loss: 0.01312495645883338\n",
      "\n",
      "epoch: 523\n",
      "Training loss: 0.013922452017097381\n",
      "Validation loss: 0.013128030529177\n",
      "\n",
      "epoch: 524\n",
      "Training loss: 0.013929405551263879\n",
      "Validation loss: 0.013114526217369074\n",
      "\n",
      "epoch: 525\n",
      "Training loss: 0.013923297517818935\n",
      "Validation loss: 0.013125006170522502\n",
      "\n",
      "epoch: 526\n",
      "Training loss: 0.013923398812201409\n",
      "Validation loss: 0.013112230405617452\n",
      "\n",
      "epoch: 527\n",
      "Training loss: 0.013926175087567886\n",
      "Validation loss: 0.013111370619609434\n",
      "\n",
      "epoch: 528\n",
      "Training loss: 0.013910027950731282\n",
      "Validation loss: 0.01312468911480389\n",
      "\n",
      "epoch: 529\n",
      "Training loss: 0.013918448173558202\n",
      "Validation loss: 0.013120908819930537\n",
      "\n",
      "epoch: 530\n",
      "Training loss: 0.013916474707160339\n",
      "Validation loss: 0.013109564050084035\n",
      "\n",
      "epoch: 531\n",
      "Training loss: 0.013946060090759759\n",
      "Validation loss: 0.01310205208317732\n",
      "\n",
      "epoch: 532\n",
      "Training loss: 0.013924060244404065\n",
      "Validation loss: 0.01313021825417928\n",
      "\n",
      "epoch: 533\n",
      "Training loss: 0.013915041883291531\n",
      "Validation loss: 0.013120166148139897\n",
      "\n",
      "epoch: 534\n",
      "Training loss: 0.01391171105692801\n",
      "Validation loss: 0.013128848312026115\n",
      "\n",
      "epoch: 535\n",
      "Training loss: 0.013932217258213359\n",
      "Validation loss: 0.013112260842610414\n",
      "\n",
      "epoch: 536\n",
      "Training loss: 0.013901717053486777\n",
      "Validation loss: 0.013121398460513505\n",
      "\n",
      "epoch: 537\n",
      "Training loss: 0.013909268011612028\n",
      "Validation loss: 0.0130886435471718\n",
      "\n",
      "epoch: 538\n",
      "Training loss: 0.013888703820758685\n",
      "Validation loss: 0.013125114560940301\n",
      "\n",
      "epoch: 539\n",
      "Training loss: 0.013924390905273696\n",
      "Validation loss: 0.013102576107988364\n",
      "\n",
      "epoch: 540\n",
      "Training loss: 0.013901500180852998\n",
      "Validation loss: 0.013114976955942976\n",
      "\n",
      "epoch: 541\n",
      "Training loss: 0.01391476379260719\n",
      "Validation loss: 0.013090417208432244\n",
      "\n",
      "epoch: 542\n",
      "Training loss: 0.013907729177726714\n",
      "Validation loss: 0.013114375379536676\n",
      "\n",
      "epoch: 543\n",
      "Training loss: 0.013893461045271529\n",
      "Validation loss: 0.013091097541691332\n",
      "\n",
      "epoch: 544\n",
      "Training loss: 0.01388646615964021\n",
      "Validation loss: 0.0131100789317349\n",
      "\n",
      "epoch: 545\n",
      "Training loss: 0.013910131360873027\n",
      "Validation loss: 0.013094524264963433\n",
      "\n",
      "epoch: 546\n",
      "Training loss: 0.013887822949341452\n",
      "Validation loss: 0.013085295308212178\n",
      "\n",
      "epoch: 547\n",
      "Training loss: 0.013879933276563262\n",
      "Validation loss: 0.01310698188286231\n",
      "\n",
      "epoch: 548\n",
      "Training loss: 0.013904260676066482\n",
      "Validation loss: 0.013081246291620961\n",
      "\n",
      "epoch: 549\n",
      "Training loss: 0.01388739810403508\n",
      "Validation loss: 0.013090082521641165\n",
      "\n",
      "epoch: 550\n",
      "Training loss: 0.013878183810146246\n",
      "Validation loss: 0.013113882607809305\n",
      "\n",
      "epoch: 551\n",
      "Training loss: 0.013908363021743067\n",
      "Validation loss: 0.013074656345605628\n",
      "\n",
      "epoch: 552\n",
      "Training loss: 0.013889879439186134\n",
      "Validation loss: 0.013096394828047269\n",
      "\n",
      "epoch: 553\n",
      "Training loss: 0.0138770101802854\n",
      "Validation loss: 0.013083195400363826\n",
      "\n",
      "epoch: 554\n",
      "Training loss: 0.013873097775605445\n",
      "Validation loss: 0.013127461601385497\n",
      "\n",
      "epoch: 555\n",
      "Training loss: 0.013890480448406258\n",
      "Validation loss: 0.013067012408545181\n",
      "\n",
      "epoch: 556\n",
      "Training loss: 0.013885025090676024\n",
      "Validation loss: 0.013102429839819811\n",
      "\n",
      "epoch: 557\n",
      "Training loss: 0.013875075455462944\n",
      "Validation loss: 0.013094386840638792\n",
      "\n",
      "epoch: 558\n",
      "Training loss: 0.013878774159217533\n",
      "Validation loss: 0.013095114583254207\n",
      "\n",
      "epoch: 559\n",
      "Training loss: 0.01385975661545586\n",
      "Validation loss: 0.01309885412007271\n",
      "\n",
      "epoch: 560\n",
      "Training loss: 0.01388145455724227\n",
      "Validation loss: 0.013090926648109562\n",
      "\n",
      "epoch: 561\n",
      "Training loss: 0.013865639298329815\n",
      "Validation loss: 0.013080993266781282\n",
      "\n",
      "epoch: 562\n",
      "Training loss: 0.013871761313431505\n",
      "Validation loss: 0.013083572204338468\n",
      "\n",
      "epoch: 563\n",
      "Training loss: 0.013865720127176158\n",
      "Validation loss: 0.013099545473009384\n",
      "\n",
      "epoch: 564\n",
      "Training loss: 0.01386790683012302\n",
      "Validation loss: 0.013083746726330425\n",
      "\n",
      "epoch: 565\n",
      "Training loss: 0.013851368807479822\n",
      "Validation loss: 0.013066966197112378\n",
      "\n",
      "epoch: 566\n",
      "Training loss: 0.013881365266701192\n",
      "Validation loss: 0.013025173647288257\n",
      "\n",
      "epoch: 567\n",
      "Training loss: 0.01386746905863035\n",
      "Validation loss: 0.01307230667882258\n",
      "\n",
      "epoch: 568\n",
      "Training loss: 0.01388061350704342\n",
      "Validation loss: 0.013063372947141322\n",
      "\n",
      "epoch: 569\n",
      "Training loss: 0.013869300762644247\n",
      "Validation loss: 0.013036103835759736\n",
      "\n",
      "epoch: 570\n",
      "Training loss: 0.013860333755677096\n",
      "Validation loss: 0.01303236829022902\n",
      "\n",
      "epoch: 571\n",
      "Training loss: 0.013865738044011552\n",
      "Validation loss: 0.013025762008795756\n",
      "\n",
      "epoch: 572\n",
      "Training loss: 0.013834991633032213\n",
      "Validation loss: 0.013027974838089288\n",
      "\n",
      "epoch: 573\n",
      "Training loss: 0.01384709815948155\n",
      "Validation loss: 0.01301887389040495\n",
      "\n",
      "epoch: 574\n",
      "Training loss: 0.013840628129785695\n",
      "Validation loss: 0.013041781813510532\n",
      "\n",
      "epoch: 575\n",
      "Training loss: 0.01385728146941823\n",
      "Validation loss: 0.013019266930590528\n",
      "\n",
      "epoch: 576\n",
      "Training loss: 0.013861499747782991\n",
      "Validation loss: 0.013043908389481324\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 577\n",
      "Training loss: 0.013843064591093976\n",
      "Validation loss: 0.013024747024073196\n",
      "\n",
      "epoch: 578\n",
      "Training loss: 0.013864798473618876\n",
      "Validation loss: 0.013042635444131282\n",
      "\n",
      "epoch: 579\n",
      "Training loss: 0.01385316101737595\n",
      "Validation loss: 0.013049739375277864\n",
      "\n",
      "epoch: 580\n",
      "Training loss: 0.013845191410386048\n",
      "Validation loss: 0.013051925394419111\n",
      "\n",
      "epoch: 581\n",
      "Training loss: 0.013857943475682066\n",
      "Validation loss: 0.01304036244424389\n",
      "\n",
      "epoch: 582\n",
      "Training loss: 0.013847841363680008\n",
      "Validation loss: 0.01304732391155033\n",
      "\n",
      "epoch: 583\n",
      "Training loss: 0.013857898645482777\n",
      "Validation loss: 0.013037075997724685\n",
      "\n",
      "epoch: 584\n",
      "Training loss: 0.013854956903683251\n",
      "Validation loss: 0.01305525824004672\n",
      "\n",
      "epoch: 585\n",
      "Training loss: 0.013860772797461647\n",
      "Validation loss: 0.013047508492100396\n",
      "\n",
      "epoch: 586\n",
      "Training loss: 0.013861171029480895\n",
      "Validation loss: 0.013065485749352632\n",
      "\n",
      "epoch: 587\n",
      "Training loss: 0.013850925569116039\n",
      "Validation loss: 0.013066153423230299\n",
      "\n",
      "epoch: 588\n",
      "Training loss: 0.013857373733239045\n",
      "Validation loss: 0.013062092586883222\n",
      "\n",
      "epoch: 589\n",
      "Training loss: 0.013864434276447913\n",
      "Validation loss: 0.013064631212263953\n",
      "\n",
      "epoch: 590\n",
      "Training loss: 0.013862356013512449\n",
      "Validation loss: 0.013085250276842462\n",
      "\n",
      "epoch: 591\n",
      "Training loss: 0.01388459128995171\n",
      "Validation loss: 0.013120706990430596\n",
      "\n",
      "epoch: 592\n",
      "Training loss: 0.013881144844228092\n",
      "Validation loss: 0.013190285953958205\n",
      "\n",
      "epoch: 593\n",
      "Training loss: 0.01389969502296472\n",
      "Validation loss: 0.013195916254981692\n",
      "\n",
      "epoch: 594\n",
      "Training loss: 0.013894454217498436\n",
      "Validation loss: 0.013191067467222742\n",
      "\n",
      "epoch: 595\n",
      "Training loss: 0.013891906146820008\n",
      "Validation loss: 0.013221923166197162\n",
      "\n",
      "epoch: 596\n",
      "Training loss: 0.013904746772346762\n",
      "Validation loss: 0.013236956895982971\n",
      "\n",
      "epoch: 597\n",
      "Training loss: 0.013928992943650633\n",
      "Validation loss: 0.013333903482926497\n",
      "\n",
      "epoch: 598\n",
      "Training loss: 0.013928553301152363\n",
      "Validation loss: 0.013368220678480536\n",
      "\n",
      "epoch: 599\n",
      "Training loss: 0.013955869996009873\n",
      "Validation loss: 0.013425504628586523\n",
      "\n",
      "epoch: 600\n",
      "Training loss: 0.013969821196025958\n",
      "Validation loss: 0.013475352087776124\n",
      "\n",
      "epoch: 601\n",
      "Training loss: 0.013994274749113278\n",
      "Validation loss: 0.013543303724341874\n",
      "\n",
      "epoch: 602\n",
      "Training loss: 0.014001174627364107\n",
      "Validation loss: 0.013593258683779674\n",
      "\n",
      "epoch: 603\n",
      "Training loss: 0.014018925617119402\n",
      "Validation loss: 0.013663477430394583\n",
      "\n",
      "epoch: 604\n",
      "Training loss: 0.01403341413406589\n",
      "Validation loss: 0.013680071310642675\n",
      "\n",
      "epoch: 605\n",
      "Training loss: 0.01404015353744531\n",
      "Validation loss: 0.01367581781459143\n",
      "\n",
      "epoch: 606\n",
      "Training loss: 0.014039376696365088\n",
      "Validation loss: 0.013726239137989638\n",
      "\n",
      "epoch: 607\n",
      "Training loss: 0.014047742872578423\n",
      "Validation loss: 0.013740322367346177\n",
      "\n",
      "epoch: 608\n",
      "Training loss: 0.01403760593874633\n",
      "Validation loss: 0.01371587763967497\n",
      "\n",
      "epoch: 609\n",
      "Training loss: 0.014038216535220497\n",
      "Validation loss: 0.013756034514864398\n",
      "\n",
      "epoch: 610\n",
      "Training loss: 0.014037074729891554\n",
      "Validation loss: 0.013757019179794635\n",
      "\n",
      "epoch: 611\n",
      "Training loss: 0.014048429119777843\n",
      "Validation loss: 0.013755628572630442\n",
      "\n",
      "epoch: 612\n",
      "Training loss: 0.01404134247638122\n",
      "Validation loss: 0.01383579007044647\n",
      "\n",
      "epoch: 613\n",
      "Training loss: 0.01403191562502914\n",
      "Validation loss: 0.013825075181022837\n",
      "\n",
      "epoch: 614\n",
      "Training loss: 0.014035073849297978\n",
      "Validation loss: 0.013808379597129493\n",
      "\n",
      "epoch: 615\n",
      "Training loss: 0.014038897384847017\n",
      "Validation loss: 0.013849332534034208\n",
      "\n",
      "epoch: 616\n",
      "Training loss: 0.014040286226770213\n",
      "Validation loss: 0.013845880604760899\n",
      "\n",
      "epoch: 617\n",
      "Training loss: 0.014042423471243671\n",
      "Validation loss: 0.013888594429993727\n",
      "\n",
      "epoch: 618\n",
      "Training loss: 0.014042983851321973\n",
      "Validation loss: 0.013866640926514491\n",
      "\n",
      "epoch: 619\n",
      "Training loss: 0.01404376904460299\n",
      "Validation loss: 0.013904888291290815\n",
      "\n",
      "epoch: 620\n",
      "Training loss: 0.014050742627652088\n",
      "Validation loss: 0.014015153054567952\n",
      "\n",
      "epoch: 621\n",
      "Training loss: 0.014042716425121345\n",
      "Validation loss: 0.014028614564347544\n",
      "\n",
      "epoch: 622\n",
      "Training loss: 0.014047365058545787\n",
      "Validation loss: 0.014038620090439859\n",
      "\n",
      "epoch: 623\n",
      "Training loss: 0.014051509017058718\n",
      "Validation loss: 0.01396052061651548\n",
      "\n",
      "epoch: 624\n",
      "Training loss: 0.014061097433479247\n",
      "Validation loss: 0.014068612816662334\n",
      "\n",
      "epoch: 625\n",
      "Training loss: 0.014060477979768544\n",
      "Validation loss: 0.014053327920276215\n",
      "\n",
      "epoch: 626\n",
      "Training loss: 0.014054279442744805\n",
      "Validation loss: 0.014049970385308398\n",
      "\n",
      "epoch: 627\n",
      "Training loss: 0.014045857548466774\n",
      "Validation loss: 0.01409524511600501\n",
      "\n",
      "epoch: 628\n",
      "Training loss: 0.014051832143847752\n",
      "Validation loss: 0.014105570794203347\n",
      "\n",
      "epoch: 629\n",
      "Training loss: 0.014053675588141745\n",
      "Validation loss: 0.014099515734425026\n",
      "\n",
      "epoch: 630\n",
      "Training loss: 0.014055218838969355\n",
      "Validation loss: 0.014133566945268305\n",
      "\n",
      "epoch: 631\n",
      "Training loss: 0.014058191584801654\n",
      "Validation loss: 0.014152973405044678\n",
      "\n",
      "epoch: 632\n",
      "Training loss: 0.014063781972287609\n",
      "Validation loss: 0.014154564868122537\n",
      "\n",
      "epoch: 633\n",
      "Training loss: 0.014055782161292404\n",
      "Validation loss: 0.014158369368766778\n",
      "\n",
      "epoch: 634\n",
      "Training loss: 0.014047247265787174\n",
      "Validation loss: 0.014152122081325486\n",
      "\n",
      "epoch: 635\n",
      "Training loss: 0.014053057376322186\n",
      "Validation loss: 0.0141261970631936\n",
      "\n",
      "epoch: 636\n",
      "Training loss: 0.014047688373502377\n",
      "Validation loss: 0.014096092542624616\n",
      "\n",
      "epoch: 637\n",
      "Training loss: 0.014059158761182583\n",
      "Validation loss: 0.014107447537600805\n",
      "\n",
      "epoch: 638\n",
      "Training loss: 0.014054663873445287\n",
      "Validation loss: 0.014088664153215158\n",
      "\n",
      "epoch: 639\n",
      "Training loss: 0.01406771702971285\n",
      "Validation loss: 0.014081799719070927\n",
      "\n",
      "epoch: 640\n",
      "Training loss: 0.01406731955325498\n",
      "Validation loss: 0.014081649513374806\n",
      "\n",
      "epoch: 641\n",
      "Training loss: 0.014067499618509687\n",
      "Validation loss: 0.014073567992421163\n",
      "\n",
      "epoch: 642\n",
      "Training loss: 0.014062086710571194\n",
      "Validation loss: 0.014060025843395938\n",
      "\n",
      "epoch: 643\n",
      "Training loss: 0.014058836525769492\n",
      "Validation loss: 0.014044271745249087\n",
      "\n",
      "epoch: 644\n",
      "Training loss: 0.014056647159133422\n",
      "Validation loss: 0.014030112327400469\n",
      "\n",
      "epoch: 645\n",
      "Training loss: 0.014055047389820425\n",
      "Validation loss: 0.014047053117797912\n",
      "\n",
      "epoch: 646\n",
      "Training loss: 0.014058969273399592\n",
      "Validation loss: 0.01402313664894026\n",
      "\n",
      "epoch: 647\n",
      "Training loss: 0.01405424025876433\n",
      "Validation loss: 0.014040025702317327\n",
      "\n",
      "epoch: 648\n",
      "Training loss: 0.014055261427377128\n",
      "Validation loss: 0.014059802173234008\n",
      "\n",
      "epoch: 649\n",
      "Training loss: 0.014064689665086803\n",
      "Validation loss: 0.01405495405237372\n",
      "\n",
      "epoch: 650\n",
      "Training loss: 0.014070571438745995\n",
      "Validation loss: 0.014047723304672311\n",
      "\n",
      "epoch: 651\n",
      "Training loss: 0.014061151319521377\n",
      "Validation loss: 0.014036802759996922\n",
      "\n",
      "epoch: 652\n",
      "Training loss: 0.014074041567984896\n",
      "Validation loss: 0.014034740420820492\n",
      "\n",
      "epoch: 653\n",
      "Training loss: 0.014070280327941592\n",
      "Validation loss: 0.014019348797108883\n",
      "\n",
      "epoch: 654\n",
      "Training loss: 0.014072732485499001\n",
      "Validation loss: 0.014014990182339525\n",
      "\n",
      "epoch: 655\n",
      "Training loss: 0.014085232566826726\n",
      "Validation loss: 0.01405216868158046\n",
      "\n",
      "epoch: 656\n",
      "Training loss: 0.014080695985883154\n",
      "Validation loss: 0.014038927078193467\n",
      "\n",
      "epoch: 657\n",
      "Training loss: 0.014086439333482744\n",
      "Validation loss: 0.014018990640772746\n",
      "\n",
      "epoch: 658\n",
      "Training loss: 0.01408805129141494\n",
      "Validation loss: 0.014020763123871948\n",
      "\n",
      "epoch: 659\n",
      "Training loss: 0.014083586818576922\n",
      "Validation loss: 0.014034997883703482\n",
      "\n",
      "epoch: 660\n",
      "Training loss: 0.014094917112133575\n",
      "Validation loss: 0.014024663818814915\n",
      "\n",
      "epoch: 661\n",
      "Training loss: 0.014101238129177001\n",
      "Validation loss: 0.014018083544807828\n",
      "\n",
      "epoch: 662\n",
      "Training loss: 0.014098105663633146\n",
      "Validation loss: 0.014009941726711557\n",
      "\n",
      "epoch: 663\n",
      "Training loss: 0.0140935379197368\n",
      "Validation loss: 0.014004141293581948\n",
      "\n",
      "epoch: 664\n",
      "Training loss: 0.014097896058390548\n",
      "Validation loss: 0.013999100014780944\n",
      "\n",
      "epoch: 665\n",
      "Training loss: 0.014105187407340476\n",
      "Validation loss: 0.014022564049594266\n",
      "\n",
      "epoch: 666\n",
      "Training loss: 0.014104654621964462\n",
      "Validation loss: 0.014001428767432442\n",
      "\n",
      "epoch: 667\n",
      "Training loss: 0.014113751400692175\n",
      "Validation loss: 0.014016793150699175\n",
      "\n",
      "epoch: 668\n",
      "Training loss: 0.01410489599446548\n",
      "Validation loss: 0.014010995515243605\n",
      "\n",
      "epoch: 669\n",
      "Training loss: 0.014109361020612485\n",
      "Validation loss: 0.01403182861303161\n",
      "\n",
      "epoch: 670\n",
      "Training loss: 0.014108717418277395\n",
      "Validation loss: 0.01403544642819768\n",
      "\n",
      "epoch: 671\n",
      "Training loss: 0.014112244683013754\n",
      "Validation loss: 0.014016009430061765\n",
      "\n",
      "epoch: 672\n",
      "Training loss: 0.014117531936459878\n",
      "Validation loss: 0.01404552101569539\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 673\n",
      "Training loss: 0.014113024116025493\n",
      "Validation loss: 0.014014466446706854\n",
      "\n",
      "epoch: 674\n",
      "Training loss: 0.014119211520547023\n",
      "Validation loss: 0.014012899649329771\n",
      "\n",
      "epoch: 675\n",
      "Training loss: 0.014127829338649334\n",
      "Validation loss: 0.014064770410584534\n",
      "\n",
      "epoch: 676\n",
      "Training loss: 0.01412051855112102\n",
      "Validation loss: 0.014034901504040615\n",
      "\n",
      "epoch: 677\n",
      "Training loss: 0.014111717972585994\n",
      "Validation loss: 0.014016452544595189\n",
      "\n",
      "epoch: 678\n",
      "Training loss: 0.014117381686857975\n",
      "Validation loss: 0.014024269817345666\n",
      "\n",
      "epoch: 679\n",
      "Training loss: 0.014123468241622875\n",
      "Validation loss: 0.014030013205414306\n",
      "\n",
      "epoch: 680\n",
      "Training loss: 0.014113285228593774\n",
      "Validation loss: 0.014009307010694711\n",
      "\n",
      "epoch: 681\n",
      "Training loss: 0.014115338915002723\n",
      "Validation loss: 0.014032651668369386\n",
      "\n",
      "epoch: 682\n",
      "Training loss: 0.014113778084113548\n",
      "Validation loss: 0.014014386549865454\n",
      "\n",
      "epoch: 683\n",
      "Training loss: 0.014113277960964551\n",
      "Validation loss: 0.01398988408070224\n",
      "\n",
      "epoch: 684\n",
      "Training loss: 0.014119252061101303\n",
      "Validation loss: 0.013998541969845362\n",
      "\n",
      "epoch: 685\n",
      "Training loss: 0.014106970485348454\n",
      "Validation loss: 0.013968940436194998\n",
      "\n",
      "epoch: 686\n",
      "Training loss: 0.014105684976635004\n",
      "Validation loss: 0.013973346038964636\n",
      "\n",
      "epoch: 687\n",
      "Training loss: 0.014112674257084162\n",
      "Validation loss: 0.0139796666108611\n",
      "\n",
      "epoch: 688\n",
      "Training loss: 0.014116255739837909\n",
      "Validation loss: 0.013980386337981592\n",
      "\n",
      "epoch: 689\n",
      "Training loss: 0.014112553649071682\n",
      "Validation loss: 0.013982444963419199\n",
      "\n",
      "epoch: 690\n",
      "Training loss: 0.01411374416909466\n",
      "Validation loss: 0.01398098355782414\n",
      "\n",
      "epoch: 691\n",
      "Training loss: 0.01411206319120187\n",
      "Validation loss: 0.01397676175653178\n",
      "\n",
      "epoch: 692\n",
      "Training loss: 0.014110368395121057\n",
      "Validation loss: 0.01397692144270241\n",
      "\n",
      "epoch: 693\n",
      "Training loss: 0.014110776170530586\n",
      "Validation loss: 0.013984266600298792\n",
      "\n",
      "epoch: 694\n",
      "Training loss: 0.014111991115302355\n",
      "Validation loss: 0.013971727651195358\n",
      "\n",
      "epoch: 695\n",
      "Training loss: 0.014124679872132765\n",
      "Validation loss: 0.013966252619520574\n",
      "\n",
      "epoch: 696\n",
      "Training loss: 0.014104888850942606\n",
      "Validation loss: 0.013949492369228728\n",
      "\n",
      "epoch: 697\n",
      "Training loss: 0.014119463969167202\n",
      "Validation loss: 0.013969644823750866\n",
      "\n",
      "epoch: 698\n",
      "Training loss: 0.014109693750147843\n",
      "Validation loss: 0.013976413801081203\n",
      "\n",
      "epoch: 699\n",
      "Training loss: 0.01411830297514556\n",
      "Validation loss: 0.0139635924012838\n",
      "\n",
      "epoch: 700\n",
      "Training loss: 0.014106805634093304\n",
      "Validation loss: 0.013958934772314587\n",
      "\n",
      "epoch: 701\n",
      "Training loss: 0.014103564186505328\n",
      "Validation loss: 0.013968643725999973\n",
      "\n",
      "epoch: 702\n",
      "Training loss: 0.014102106494422896\n",
      "Validation loss: 0.01396757875689172\n",
      "\n",
      "epoch: 703\n",
      "Training loss: 0.014115899223055171\n",
      "Validation loss: 0.01394700193225512\n",
      "\n",
      "epoch: 704\n",
      "Training loss: 0.014101825912769206\n",
      "Validation loss: 0.013948365002304041\n",
      "\n",
      "epoch: 705\n",
      "Training loss: 0.014114099194724035\n",
      "Validation loss: 0.013937065299148942\n",
      "\n",
      "epoch: 706\n",
      "Training loss: 0.014099039528061108\n",
      "Validation loss: 0.013931500243238587\n",
      "\n",
      "epoch: 707\n",
      "Training loss: 0.014093744939591455\n",
      "Validation loss: 0.01395479664842515\n",
      "\n",
      "epoch: 708\n",
      "Training loss: 0.014104862505479463\n",
      "Validation loss: 0.013933027137562923\n",
      "\n",
      "epoch: 709\n",
      "Training loss: 0.014086676163992215\n",
      "Validation loss: 0.013943150341856501\n",
      "\n",
      "epoch: 710\n",
      "Training loss: 0.014102084316710098\n",
      "Validation loss: 0.013914798373154734\n",
      "\n",
      "epoch: 711\n",
      "Training loss: 0.014086496954585897\n",
      "Validation loss: 0.013920685612380355\n",
      "\n",
      "epoch: 712\n",
      "Training loss: 0.014097244449953604\n",
      "Validation loss: 0.013905283153755025\n",
      "\n",
      "epoch: 713\n",
      "Training loss: 0.01409929748933758\n",
      "Validation loss: 0.01393365377176209\n",
      "\n",
      "epoch: 714\n",
      "Training loss: 0.014090793150093317\n",
      "Validation loss: 0.01391215479014814\n",
      "\n",
      "epoch: 715\n",
      "Training loss: 0.01409843722130048\n",
      "Validation loss: 0.01392470462490623\n",
      "\n",
      "epoch: 716\n",
      "Training loss: 0.014101404098889238\n",
      "Validation loss: 0.013910900278826048\n",
      "\n",
      "epoch: 717\n",
      "Training loss: 0.01408587677271313\n",
      "Validation loss: 0.013892402079794041\n",
      "\n",
      "epoch: 718\n",
      "Training loss: 0.014085533600407714\n",
      "Validation loss: 0.013899494906878248\n",
      "\n",
      "epoch: 719\n",
      "Training loss: 0.014087535327449014\n",
      "Validation loss: 0.013891324865901396\n",
      "\n",
      "epoch: 720\n",
      "Training loss: 0.014091979359972974\n",
      "Validation loss: 0.013899060012862518\n",
      "\n",
      "epoch: 721\n",
      "Training loss: 0.01409786208009934\n",
      "Validation loss: 0.013892083534519331\n",
      "\n",
      "epoch: 722\n",
      "Training loss: 0.014092651735436541\n",
      "Validation loss: 0.013883245558475336\n",
      "\n",
      "epoch: 723\n",
      "Training loss: 0.014102770852741456\n",
      "Validation loss: 0.013893626516223008\n",
      "\n",
      "epoch: 724\n",
      "Training loss: 0.014090518942758439\n",
      "Validation loss: 0.013877794577742082\n",
      "\n",
      "epoch: 725\n",
      "Training loss: 0.014092520169816148\n",
      "Validation loss: 0.01387652127434067\n",
      "\n",
      "epoch: 726\n",
      "Training loss: 0.014096049065630771\n",
      "Validation loss: 0.013871408349878676\n",
      "\n",
      "epoch: 727\n",
      "Training loss: 0.014081502988644426\n",
      "Validation loss: 0.013856816253717997\n",
      "\n",
      "epoch: 728\n",
      "Training loss: 0.014089826106797812\n",
      "Validation loss: 0.013868404312708691\n",
      "\n",
      "epoch: 729\n",
      "Training loss: 0.014082642375903282\n",
      "Validation loss: 0.01386251392429315\n",
      "\n",
      "epoch: 730\n",
      "Training loss: 0.01407969437246064\n",
      "Validation loss: 0.013852833246003792\n",
      "\n",
      "epoch: 731\n",
      "Training loss: 0.01407105063228535\n",
      "Validation loss: 0.013842086214870722\n",
      "\n",
      "epoch: 732\n",
      "Training loss: 0.014067985414422973\n",
      "Validation loss: 0.01382843905803633\n",
      "\n",
      "epoch: 733\n",
      "Training loss: 0.01406409359884287\n",
      "Validation loss: 0.01381893159769051\n",
      "\n",
      "epoch: 734\n",
      "Training loss: 0.014059904058420147\n",
      "Validation loss: 0.01380943800389393\n",
      "\n",
      "epoch: 735\n",
      "Training loss: 0.014063478286449719\n",
      "Validation loss: 0.013782137701609494\n",
      "\n",
      "epoch: 736\n",
      "Training loss: 0.014043899339092902\n",
      "Validation loss: 0.013765530441560547\n",
      "\n",
      "epoch: 737\n",
      "Training loss: 0.014047590729700236\n",
      "Validation loss: 0.013789704244769876\n",
      "\n",
      "epoch: 738\n",
      "Training loss: 0.014038563236597548\n",
      "Validation loss: 0.013742193745945765\n",
      "\n",
      "epoch: 739\n",
      "Training loss: 0.014044973320959955\n",
      "Validation loss: 0.013760322258258507\n",
      "\n",
      "epoch: 740\n",
      "Training loss: 0.014033497164337248\n",
      "Validation loss: 0.013745868659737712\n",
      "\n",
      "epoch: 741\n",
      "Training loss: 0.014019771090860765\n",
      "Validation loss: 0.013722013073398003\n",
      "\n",
      "epoch: 742\n",
      "Training loss: 0.014021227649359013\n",
      "Validation loss: 0.01373059525284003\n",
      "\n",
      "epoch: 743\n",
      "Training loss: 0.014012749552600552\n",
      "Validation loss: 0.013722562986980333\n",
      "\n",
      "epoch: 744\n",
      "Training loss: 0.014014464912794046\n",
      "Validation loss: 0.013713785296934013\n",
      "\n",
      "epoch: 745\n",
      "Training loss: 0.014005888824561387\n",
      "Validation loss: 0.013715361757172643\n",
      "\n",
      "epoch: 746\n",
      "Training loss: 0.013990207099936371\n",
      "Validation loss: 0.01370957260987896\n",
      "\n",
      "epoch: 747\n",
      "Training loss: 0.01399747631387837\n",
      "Validation loss: 0.013707752001865981\n",
      "\n",
      "epoch: 748\n",
      "Training loss: 0.013995142514405805\n",
      "Validation loss: 0.013694137944935798\n",
      "\n",
      "epoch: 749\n",
      "Training loss: 0.013991411621015606\n",
      "Validation loss: 0.013689304185387075\n",
      "\n",
      "epoch: 750\n",
      "Training loss: 0.013984761523940795\n",
      "Validation loss: 0.013706694843241088\n",
      "\n",
      "epoch: 751\n",
      "Training loss: 0.013984354188180052\n",
      "Validation loss: 0.013785034839839097\n",
      "\n",
      "epoch: 752\n",
      "Training loss: 0.013948521916054786\n",
      "Validation loss: 0.013692854644856646\n",
      "\n",
      "epoch: 753\n",
      "Training loss: 0.013862941166298\n",
      "Validation loss: 0.013594360366255885\n",
      "\n",
      "epoch: 754\n",
      "Training loss: 0.013802084828482568\n",
      "Validation loss: 0.013503656287294545\n",
      "\n",
      "epoch: 755\n",
      "Training loss: 0.01375707144532651\n",
      "Validation loss: 0.013413053980946886\n",
      "\n",
      "epoch: 756\n",
      "Training loss: 0.013711474824998428\n",
      "Validation loss: 0.013372471893961776\n",
      "\n",
      "epoch: 757\n",
      "Training loss: 0.013668101513667425\n",
      "Validation loss: 0.013351056549822085\n",
      "\n",
      "epoch: 758\n",
      "Training loss: 0.01364791936735695\n",
      "Validation loss: 0.013324564596187275\n",
      "\n",
      "epoch: 759\n",
      "Training loss: 0.01361349305566143\n",
      "Validation loss: 0.013284229017949161\n",
      "\n",
      "epoch: 760\n",
      "Training loss: 0.013591685786974457\n",
      "Validation loss: 0.013285705567234034\n",
      "\n",
      "epoch: 761\n",
      "Training loss: 0.013573896966705773\n",
      "Validation loss: 0.013262055271590455\n",
      "\n",
      "epoch: 762\n",
      "Training loss: 0.013547469266281722\n",
      "Validation loss: 0.013256170876976892\n",
      "\n",
      "epoch: 763\n",
      "Training loss: 0.013531992837779292\n",
      "Validation loss: 0.013249717493639184\n",
      "\n",
      "epoch: 764\n",
      "Training loss: 0.01350190470227925\n",
      "Validation loss: 0.013209357294491108\n",
      "\n",
      "epoch: 765\n",
      "Training loss: 0.013491922636512454\n",
      "Validation loss: 0.01323142061405576\n",
      "\n",
      "epoch: 766\n",
      "Training loss: 0.013462043814050416\n",
      "Validation loss: 0.01317984275144455\n",
      "\n",
      "epoch: 767\n",
      "Training loss: 0.013432860350019877\n",
      "Validation loss: 0.013204367668696138\n",
      "\n",
      "epoch: 768\n",
      "Training loss: 0.013415399366126998\n",
      "Validation loss: 0.013131500768359772\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 769\n",
      "Training loss: 0.013402718519870061\n",
      "Validation loss: 0.013154970130273851\n",
      "\n",
      "epoch: 770\n",
      "Training loss: 0.013370637617688782\n",
      "Validation loss: 0.013114216909933103\n",
      "\n",
      "epoch: 771\n",
      "Training loss: 0.013348324832685143\n",
      "Validation loss: 0.013093712787786765\n",
      "\n",
      "epoch: 772\n",
      "Training loss: 0.013316575380439538\n",
      "Validation loss: 0.013072194640517322\n",
      "\n",
      "epoch: 773\n",
      "Training loss: 0.01331137684810425\n",
      "Validation loss: 0.013047821725998754\n",
      "\n",
      "epoch: 774\n",
      "Training loss: 0.013283141325767904\n",
      "Validation loss: 0.013009438217815748\n",
      "\n",
      "epoch: 775\n",
      "Training loss: 0.013258580197380184\n",
      "Validation loss: 0.012983893037693988\n",
      "\n",
      "epoch: 776\n",
      "Training loss: 0.01323960286448497\n",
      "Validation loss: 0.012937408831484542\n",
      "\n",
      "epoch: 777\n",
      "Training loss: 0.013200985888526539\n",
      "Validation loss: 0.012925457622199349\n",
      "\n",
      "epoch: 778\n",
      "Training loss: 0.013165267119203645\n",
      "Validation loss: 0.012895068181895617\n",
      "\n",
      "epoch: 779\n",
      "Training loss: 0.013140767115307598\n",
      "Validation loss: 0.012851211836337705\n",
      "\n",
      "epoch: 780\n",
      "Training loss: 0.013105629419971404\n",
      "Validation loss: 0.012808881223907626\n",
      "\n",
      "epoch: 781\n",
      "Training loss: 0.013066121236556106\n",
      "Validation loss: 0.012801369347508057\n",
      "\n",
      "epoch: 782\n",
      "Training loss: 0.013036067496950122\n",
      "Validation loss: 0.01279411713007596\n",
      "\n",
      "epoch: 783\n",
      "Training loss: 0.01300089299616998\n",
      "Validation loss: 0.012746279818096732\n",
      "\n",
      "epoch: 784\n",
      "Training loss: 0.012952476747274934\n",
      "Validation loss: 0.012740833614258743\n",
      "\n",
      "epoch: 785\n",
      "Training loss: 0.012913283470723675\n",
      "Validation loss: 0.012711298863558776\n",
      "\n",
      "epoch: 786\n",
      "Training loss: 0.012884366115730591\n",
      "Validation loss: 0.012717725226285123\n",
      "\n",
      "epoch: 787\n",
      "Training loss: 0.012850257020716481\n",
      "Validation loss: 0.012667398672094914\n",
      "\n",
      "epoch: 788\n",
      "Training loss: 0.012810159218418311\n",
      "Validation loss: 0.012634817458832515\n",
      "\n",
      "epoch: 789\n",
      "Training loss: 0.012779307283626286\n",
      "Validation loss: 0.012599722192320504\n",
      "\n",
      "epoch: 790\n",
      "Training loss: 0.012742253851523775\n",
      "Validation loss: 0.012578214734860474\n",
      "\n",
      "epoch: 791\n",
      "Training loss: 0.012701576037718732\n",
      "Validation loss: 0.012563615816030473\n",
      "\n",
      "epoch: 792\n",
      "Training loss: 0.012664879006383573\n",
      "Validation loss: 0.012545188381638482\n",
      "\n",
      "epoch: 793\n",
      "Training loss: 0.012625499593948281\n",
      "Validation loss: 0.012483687223021147\n",
      "\n",
      "epoch: 794\n",
      "Training loss: 0.012598198901286122\n",
      "Validation loss: 0.012505581357198937\n",
      "\n",
      "epoch: 795\n",
      "Training loss: 0.01254826124466928\n",
      "Validation loss: 0.012460293853319685\n",
      "\n",
      "epoch: 796\n",
      "Training loss: 0.012524727845556083\n",
      "Validation loss: 0.012470004943690903\n",
      "\n",
      "epoch: 797\n",
      "Training loss: 0.012505174926663143\n",
      "Validation loss: 0.012452029023239627\n",
      "\n",
      "epoch: 798\n",
      "Training loss: 0.012458395534939609\n",
      "Validation loss: 0.012415458121923828\n",
      "\n",
      "epoch: 799\n",
      "Training loss: 0.012436980247387222\n",
      "Validation loss: 0.01238093446364083\n",
      "\n",
      "epoch: 800\n",
      "Training loss: 0.01240250282078291\n",
      "Validation loss: 0.012349289147586432\n",
      "\n",
      "epoch: 801\n",
      "Training loss: 0.012371865275870354\n",
      "Validation loss: 0.01230962265340265\n",
      "\n",
      "epoch: 802\n",
      "Training loss: 0.012330529959945935\n",
      "Validation loss: 0.012298569226413248\n",
      "\n",
      "epoch: 803\n",
      "Training loss: 0.012307665129228942\n",
      "Validation loss: 0.012279114246697862\n",
      "\n",
      "epoch: 804\n",
      "Training loss: 0.012275578560512323\n",
      "Validation loss: 0.012330926444375572\n",
      "\n",
      "epoch: 805\n",
      "Training loss: 0.01225365058849191\n",
      "Validation loss: 0.012234860777466697\n",
      "\n",
      "epoch: 806\n",
      "Training loss: 0.012242886885253096\n",
      "Validation loss: 0.012214891476954915\n",
      "\n",
      "epoch: 807\n",
      "Training loss: 0.012193070267769125\n",
      "Validation loss: 0.012266603600681377\n",
      "\n",
      "epoch: 808\n",
      "Training loss: 0.01218602205582612\n",
      "Validation loss: 0.012193778213312216\n",
      "\n",
      "epoch: 809\n",
      "Training loss: 0.01215973731462884\n",
      "Validation loss: 0.012094710309690507\n",
      "\n",
      "epoch: 810\n",
      "Training loss: 0.012148865479350776\n",
      "Validation loss: 0.012060778441140734\n",
      "\n",
      "epoch: 811\n",
      "Training loss: 0.012124460979824768\n",
      "Validation loss: 0.012070451181820594\n",
      "\n",
      "epoch: 812\n",
      "Training loss: 0.012062768892981954\n",
      "Validation loss: 0.012076732429839058\n",
      "\n",
      "epoch: 813\n",
      "Training loss: 0.012037252405147919\n",
      "Validation loss: 0.01208025282115308\n",
      "\n",
      "epoch: 814\n",
      "Training loss: 0.012006381399744165\n",
      "Validation loss: 0.012146326606037148\n",
      "\n",
      "epoch: 815\n",
      "Training loss: 0.011983048930038686\n",
      "Validation loss: 0.01215592517259492\n",
      "\n",
      "epoch: 816\n",
      "Training loss: 0.011950392681624677\n",
      "Validation loss: 0.012159528467661789\n",
      "\n",
      "epoch: 817\n",
      "Training loss: 0.011948527732537115\n",
      "Validation loss: 0.012102891056452602\n",
      "\n",
      "epoch: 818\n",
      "Training loss: 0.011917635146279163\n",
      "Validation loss: 0.012149776219782644\n",
      "\n",
      "epoch: 819\n",
      "Training loss: 0.011898542410491077\n",
      "Validation loss: 0.0120884341453705\n",
      "\n",
      "epoch: 820\n",
      "Training loss: 0.011884713017166627\n",
      "Validation loss: 0.012010309302673739\n",
      "\n",
      "epoch: 821\n",
      "Training loss: 0.011863146201108022\n",
      "Validation loss: 0.012025402655227134\n",
      "\n",
      "epoch: 822\n",
      "Training loss: 0.011833470654644265\n",
      "Validation loss: 0.011998874789041476\n",
      "\n",
      "epoch: 823\n",
      "Training loss: 0.01182225277008381\n",
      "Validation loss: 0.011958666893861148\n",
      "\n",
      "epoch: 824\n",
      "Training loss: 0.011797314975139379\n",
      "Validation loss: 0.011957750300789782\n",
      "\n",
      "epoch: 825\n",
      "Training loss: 0.011784399303589403\n",
      "Validation loss: 0.011886172961687565\n",
      "\n",
      "epoch: 826\n",
      "Training loss: 0.011770324426185098\n",
      "Validation loss: 0.011841868639214714\n",
      "\n",
      "epoch: 827\n",
      "Training loss: 0.011742515048454643\n",
      "Validation loss: 0.011868715556854904\n",
      "\n",
      "epoch: 828\n",
      "Training loss: 0.011745831472439964\n",
      "Validation loss: 0.011794467387155148\n",
      "\n",
      "epoch: 829\n",
      "Training loss: 0.011707011014537328\n",
      "Validation loss: 0.011790237439220226\n",
      "\n",
      "epoch: 830\n",
      "Training loss: 0.011711482318192088\n",
      "Validation loss: 0.011783438660834165\n",
      "\n",
      "epoch: 831\n",
      "Training loss: 0.011678667957531846\n",
      "Validation loss: 0.011746404974421004\n",
      "\n",
      "epoch: 832\n",
      "Training loss: 0.011682941343829443\n",
      "Validation loss: 0.011697595114116614\n",
      "\n",
      "epoch: 833\n",
      "Training loss: 0.011655288929572537\n",
      "Validation loss: 0.01167598219585107\n",
      "\n",
      "epoch: 834\n",
      "Training loss: 0.011624407370904389\n",
      "Validation loss: 0.011715444001534273\n",
      "\n",
      "epoch: 835\n",
      "Training loss: 0.01157862167409144\n",
      "Validation loss: 0.011708794913842023\n",
      "\n",
      "epoch: 836\n",
      "Training loss: 0.011596377644746959\n",
      "Validation loss: 0.01165756863852458\n",
      "\n",
      "epoch: 837\n",
      "Training loss: 0.011539696689779635\n",
      "Validation loss: 0.011585107342732492\n",
      "\n",
      "epoch: 838\n",
      "Training loss: 0.011528324713237956\n",
      "Validation loss: 0.01156937612282897\n",
      "\n",
      "epoch: 839\n",
      "Training loss: 0.011502248524533765\n",
      "Validation loss: 0.0115219261418575\n",
      "\n",
      "epoch: 840\n",
      "Training loss: 0.011482906298014732\n",
      "Validation loss: 0.011520332623529209\n",
      "\n",
      "epoch: 841\n",
      "Training loss: 0.011461437303822085\n",
      "Validation loss: 0.011509417993735706\n",
      "\n",
      "epoch: 842\n",
      "Training loss: 0.01142322614559099\n",
      "Validation loss: 0.01151967243256372\n",
      "\n",
      "epoch: 843\n",
      "Training loss: 0.011413186341565655\n",
      "Validation loss: 0.011456336631616085\n",
      "\n",
      "epoch: 844\n",
      "Training loss: 0.01138960796814255\n",
      "Validation loss: 0.011435174475735202\n",
      "\n",
      "epoch: 845\n",
      "Training loss: 0.011374971123779349\n",
      "Validation loss: 0.011407508220029125\n",
      "\n",
      "epoch: 846\n",
      "Training loss: 0.011331788052956973\n",
      "Validation loss: 0.011438981593369906\n",
      "\n",
      "epoch: 847\n",
      "Training loss: 0.011298226823914919\n",
      "Validation loss: 0.011426689000310384\n",
      "\n",
      "epoch: 848\n",
      "Training loss: 0.011280385890774848\n",
      "Validation loss: 0.011323777443394105\n",
      "\n",
      "epoch: 849\n",
      "Training loss: 0.011233828353269588\n",
      "Validation loss: 0.01137574317829277\n",
      "\n",
      "epoch: 850\n",
      "Training loss: 0.011201552158943993\n",
      "Validation loss: 0.011306220478386886\n",
      "\n",
      "epoch: 851\n",
      "Training loss: 0.011175591205673805\n",
      "Validation loss: 0.011362227315334681\n",
      "\n",
      "epoch: 852\n",
      "Training loss: 0.011145906143553035\n",
      "Validation loss: 0.01131619741737344\n",
      "\n",
      "epoch: 853\n",
      "Training loss: 0.011064477175564288\n",
      "Validation loss: 0.011262321847255497\n",
      "\n",
      "epoch: 854\n",
      "Training loss: 0.011025766948033486\n",
      "Validation loss: 0.011225157050640307\n",
      "\n",
      "epoch: 855\n",
      "Training loss: 0.01101822787361668\n",
      "Validation loss: 0.011193746956761301\n",
      "\n",
      "epoch: 856\n",
      "Training loss: 0.010981450113460429\n",
      "Validation loss: 0.01121223992025674\n",
      "\n",
      "epoch: 857\n",
      "Training loss: 0.010934584607275104\n",
      "Validation loss: 0.011145867866842683\n",
      "\n",
      "epoch: 858\n",
      "Training loss: 0.010887128049997042\n",
      "Validation loss: 0.011044325838247838\n",
      "\n",
      "epoch: 859\n",
      "Training loss: 0.010843146108814055\n",
      "Validation loss: 0.011014064027094738\n",
      "\n",
      "epoch: 860\n",
      "Training loss: 0.010819303469541536\n",
      "Validation loss: 0.010929725061526976\n",
      "\n",
      "epoch: 861\n",
      "Training loss: 0.010780568225742991\n",
      "Validation loss: 0.01096013253283039\n",
      "\n",
      "epoch: 862\n",
      "Training loss: 0.010741264659467281\n",
      "Validation loss: 0.010894486277591302\n",
      "\n",
      "epoch: 863\n",
      "Training loss: 0.010691144328181216\n",
      "Validation loss: 0.010837790212220215\n",
      "\n",
      "epoch: 864\n",
      "Training loss: 0.010662097698793671\n",
      "Validation loss: 0.010779699862985964\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 865\n",
      "Training loss: 0.010613469705677113\n",
      "Validation loss: 0.010764099730554517\n",
      "\n",
      "epoch: 866\n",
      "Training loss: 0.010573049050695229\n",
      "Validation loss: 0.010705205278618552\n",
      "\n",
      "epoch: 867\n",
      "Training loss: 0.010538945930836784\n",
      "Validation loss: 0.01067940894223945\n",
      "\n",
      "epoch: 868\n",
      "Training loss: 0.010494237001665924\n",
      "Validation loss: 0.010659829549904081\n",
      "\n",
      "epoch: 869\n",
      "Training loss: 0.010458746417493418\n",
      "Validation loss: 0.010612876464158085\n",
      "\n",
      "epoch: 870\n",
      "Training loss: 0.010413772952055603\n",
      "Validation loss: 0.010607323202489813\n",
      "\n",
      "epoch: 871\n",
      "Training loss: 0.010376148655947161\n",
      "Validation loss: 0.010553588642246897\n",
      "\n",
      "epoch: 872\n",
      "Training loss: 0.010342732644358766\n",
      "Validation loss: 0.01052135827789995\n",
      "\n",
      "epoch: 873\n",
      "Training loss: 0.010298749646735812\n",
      "Validation loss: 0.010503211763642342\n",
      "\n",
      "epoch: 874\n",
      "Training loss: 0.01026181046380752\n",
      "Validation loss: 0.010461792616853854\n",
      "\n",
      "epoch: 875\n",
      "Training loss: 0.010211027755137365\n",
      "Validation loss: 0.010448388286240328\n",
      "\n",
      "epoch: 876\n",
      "Training loss: 0.010183728822258082\n",
      "Validation loss: 0.010252758526959905\n",
      "\n",
      "epoch: 877\n",
      "Training loss: 0.010145735503586082\n",
      "Validation loss: 0.010229840763171124\n",
      "\n",
      "epoch: 878\n",
      "Training loss: 0.010110724383532606\n",
      "Validation loss: 0.010198925827367837\n",
      "\n",
      "epoch: 879\n",
      "Training loss: 0.010077022162976514\n",
      "Validation loss: 0.010168544646830776\n",
      "\n",
      "epoch: 880\n",
      "Training loss: 0.010067707216031233\n",
      "Validation loss: 0.010254590176529299\n",
      "\n",
      "epoch: 881\n",
      "Training loss: 0.01003198685028092\n",
      "Validation loss: 0.010072301825136888\n",
      "\n",
      "epoch: 882\n",
      "Training loss: 0.01000480841589701\n",
      "Validation loss: 0.01018705194969439\n",
      "\n",
      "epoch: 883\n",
      "Training loss: 0.009970429472219692\n",
      "Validation loss: 0.010091043954859576\n",
      "\n",
      "epoch: 884\n",
      "Training loss: 0.009936854327599916\n",
      "Validation loss: 0.010055846781596834\n",
      "\n",
      "epoch: 885\n",
      "Training loss: 0.009887924618628774\n",
      "Validation loss: 0.010030428117264496\n",
      "\n",
      "epoch: 886\n",
      "Training loss: 0.009857566971855776\n",
      "Validation loss: 0.009996975600227949\n",
      "\n",
      "Training done.\n",
      "\n",
      "Training statistics:\n",
      "Minimum train error: 0.009857566971855776\n",
      "Maximum train error: 0.14017612466465554\n",
      "Mean train error: 0.014918517691467667\n",
      "\n",
      "Minimum val error: 0.009996975600227949\n",
      "Maximum val error: 0.044964833146654185\n",
      "Mean val error: 0.01401379869586885\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # MGD training\n",
    "    # Initiate seed for easier debugging\n",
    "    seed = 99\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Instantiate NN class\n",
    "    in_size = 9   # input layer\n",
    "    h_size = 16   # hidden layer\n",
    "    out_size = 1  # output layer\n",
    "\n",
    "    # Hyperparameters\n",
    "    lrate = 0.1\n",
    "    momentum = 0.1\n",
    "    batch_size = 2\n",
    "    \n",
    "    mgd = MGD(in_size, h_size, out_size, batch_size, lr=lrate, mu=momentum)\n",
    "\n",
    "    # Initialize Root Mean Square Error\n",
    "    RMSE = 0.01\n",
    "    # Initialize number of epochs\n",
    "    epoch = 3000\n",
    "\n",
    "    mgd.train_MGD(train_val_sets, RMSE, epoch)\n",
    "    # end of training\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # BGD training\n",
    "    # Initiate seed for easier debugging\n",
    "    seed = 99\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Instantiate NN class\n",
    "    in_size = 9  # input layer\n",
    "    h_size = 25  # hidden layer\n",
    "    out_size = 1  # output layer\n",
    "\n",
    "    # Hyperparameters\n",
    "    lrate = 0.001\n",
    "    momentum = 0.9\n",
    "    \n",
    "    bgd = BGD(in_size, h_size, out_size, lr=lrate, mu=momentum)\n",
    "\n",
    "    # Initialize Root Mean Square Error\n",
    "    RMSE = 0.01\n",
    "    # Initialize number of epochs\n",
    "    epoch = 3000\n",
    "\n",
    "    bgd.train_BGD(train_val_sets, RMSE, epoch)\n",
    "    # end of training\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted data based on trained weights:\n",
      "Predicted values: [[0.89802694]\n",
      " [0.91530157]\n",
      " [0.08749432]\n",
      " ...\n",
      " [0.08219407]\n",
      " [0.08922298]\n",
      " [0.10704098]]\n",
      "\n",
      "Target values: [[0.9]\n",
      " [0.9]\n",
      " [0.1]\n",
      " ...\n",
      " [0.1]\n",
      " [0.1]\n",
      " [0.1]]\n",
      "\n",
      "Testing loss: 0.08122365725270793\n",
      "\n",
      "\n",
      "Summary of training was exported.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "\n",
      "The script finished its execution after 1906.14 seconds\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Plot loss curve\n",
    "    xlabel = \"Epoch\"\n",
    "    ylabel = \"Average Cost\"\n",
    "    name = \"MGD_leaky_newsample\"\n",
    "    mgd.plot_loss_curve(xlabel, ylabel, name, save=True)\n",
    "\n",
    "    # Predict using testing samples\n",
    "    # Forward pass using the whole dataset\n",
    "    X_test_1o = test_set[..., :10]\n",
    "    y_test_1o = test_set[..., [-1]]\n",
    "\n",
    "    mgd.predict(X_test_1o, y_test_1o)\n",
    "\n",
    "    # Write results to disk\n",
    "    # Predicted\n",
    "    mgd.save_predict(X_test_1o, y_test_1o)\n",
    "\n",
    "    # Summary of results\n",
    "    # Get layers name with the same index as input vector\n",
    "    layers = [file for file in os.listdir(fp_prone) if os.path.isfile(os.path.join(fp_prone, file))][:-1]  #exclude \"readme.txt\" file\n",
    "    filename = \"runs_MGD_leaky.txt\"\n",
    "    mgd.save_summary(filename, layers)\n",
    "\n",
    "    # Plot target and predicted values\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.xlabel(\"Test samples\")\n",
    "    plt.ylabel(\"Cell Value\")\n",
    "\n",
    "    x, y = copy.deepcopy(X_test_1o), copy.deepcopy(y_test_1o)\n",
    "\n",
    "    # Draw target values\n",
    "    plt.plot(y, \"o\", color=\"b\", label=\"Target value\")\n",
    "\n",
    "    # Draw network output values\n",
    "    loss_testing = []\n",
    "    \n",
    "    for i in range(X_test_1o.shape[0]):\n",
    "        y[i] = mgd.forward(x[i])\n",
    "        rmse = np.sqrt(np.mean((y_test_1o[i] - y[i])**2)) \n",
    "        loss_testing.append(rmse)\n",
    "    \n",
    "    mean_error = np.mean(loss_testing)\n",
    "        \n",
    "    plt.plot(y, '.', color='r', alpha=0.6, label=\"Predicted value\")\n",
    "    plt.legend(loc=2)\n",
    "    \n",
    "    # Save plot\n",
    "    op = r\"D:\\MS Gme\\Thesis\\Final Parameters\\ANN\\sublime_run\\jupyter_notebook\\plots\"\n",
    "    fn = f\"{h_size}_{mgd.lrate}_{mgd.momentum}_{mgd.term_iter}_{mean_error}\"\n",
    "\n",
    "    if not os.path.isfile(os.path.join(op, fn)):\n",
    "        plt.savefig(os.path.join(op, f\"MGD_leaky_test_newsample{fn}.jpg\"))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"\\nThe script finished its execution after %.2f seconds\" % (time.process_time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating lsi using the best fit model...\n",
      "Finished predicting lsi for the whole study area.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Generate LSI using the optimized weights\n",
    "    print(\"\\nGenerating lsi using the best fit model...\")\n",
    "    fuzzy_path = r\"D:\\MS Gme\\Thesis\\Final Parameters\\Samples\\for_lsi\\Fuzzy\\fuzzy3\"\n",
    "\n",
    "    fuzzy = Dataset(fp_prone, fp_notprone)\n",
    "    lsi_ds = fuzzy.load_fuzzified_layers(fuzzy_path)\n",
    "    X_lsi = lsi_ds[0]\n",
    "    y_lsi = lsi_ds[1]\n",
    "\n",
    "    # Execute forward pass to the whole area per sample\n",
    "    for i in range(X_lsi.shape[0]):\n",
    "        y_lsi[i] = mgd.forward(X_lsi[i])\n",
    "\n",
    "    print(\"Finished predicting lsi for the whole study area.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exporting array to image...\n",
      "Landslide susceptility index was exported.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":    \n",
    "    # Reshape computed lsi to 2D array\n",
    "    lsi = y_lsi.reshape(6334, 3877)\n",
    "\n",
    "    # Export generated lsi\n",
    "    folder = \"lsi\"\n",
    "    out_path = os.getcwd()\n",
    "    op = os.path.join(out_path, folder)\n",
    "    if not os.path.exists(op):\n",
    "        os.mkdir(folder)\n",
    "\n",
    "    fn = \"MGD_leaky4.tif\"\n",
    "    ref_data = os.path.join(fuzzy_path, \"j_itogon_grid.tif\")\n",
    "    mgd.export_to_image(ref_data, os.path.join(op, fn), lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
